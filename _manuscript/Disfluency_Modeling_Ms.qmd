---
title: "Perceptual Disfluency and Recognition Memory: A Response Time Distributional Analysis"
# If blank, the running header is the title in 
shorttitle: "Modeling Perceptual Disfluency"
# Set names and affiliations.
# It is nice to specify everyone's orcid, if possible.
# There can be only one corresponding author, but declaring one is optional.
author:
  - name: Jason Geller
    corresponding: true
    orcid: 0000-0002-7459-4505
    email: jason.geller@bc.edu
    url: www.drjasongeller.com
    # Roles are optional. 
    # Select from the CRediT: Contributor Roles Taxonomy https://credit.niso.org/
    # conceptualization, data curation, formal Analysis, funding acquisition, investigation, 
    # methodology, project administration, resources, software, supervision, validation, 
    # visualization, writing, editing
    roles:
      - conceptualization
      - writing
      - data curation
      - editing
      - software
      - formal analysis
    affiliations:
      - id: id1
        name: "Boston College"
        department: Department of Psychology and Neuroscience
        address: Mcguinn Hall 405
        city: Chestnut Hill
        region: MA
        country: USA
        postal-code: 02467-9991

  - name: Pablo Gomez
    orcid: 0000-0003-4180-1560
    roles:
      - conceptualization
      - editing
      - formal analysis
    affiliations: 
      - id: id2
        name: "Skidmore College"
        department: Department of Psychology
  - name: Erin Buchanon
    orcid: 0000-0002-9689-4189
    roles: 
      - editing
      - formal analysis
    affiliations:
     - id: id3
       name: "Harrisburg University of Science and Technology"
       department: Department of Cognitive Analytics (Analytics)
author-note:
  status-changes: 
    affiliation-change: ~
    deceased: ~
  disclosures:
    # Example: This study was registered at X (Identifier Y).
    study-registration: "Experiment 1A and 2 were preregisted: https://osf.io/q3fjn; https://osf.io/kjq3t."
    # Acknowledge and cite data/materials to be shared.
    data-sharing: Data, code, and materials for this manuscript can be found at  https://osf.io/6sy7k/. 
    related-report: ~
    conflict-of-interest: The authors have no conflicts of interest to disclose.
    financial-support: This work was supported by research start-up funds to JG.
    authorship-agreements: ~
abstract: "Perceptual disfluency, induced by blurring or difficult-to-read typefaces, can sometimes enhance memory retention, but the underlying mechanisms remain unclear. Recent research suggests that this effect arises from interactions between early encoding and later decision-making processes. To investigate this, we manipulated blurring levels (clear, low blur, high blur) during encoding and assessed recognition performance in a surprise memory test.In Experiments 1a and 1b, response latencies from a lexical decision task were analyzed using ex-Gaussian distribution modeling and drift diffusion modeling. Results showed that blurring differentially influenced these parameters, with high blur affecting both early and late-stage processes, while low blur primarily influenced early-stage processes. Recognition test results further revealed that high-blur words were remembered better than both clear and low-blurred words.Experiment 2 employed a semantic categorization task with a word frequency manipulation to further examine the locus of the perceptual disfluency effect. Similar to Experiments 1a and 1b, high blur influenced both early and late-stage processes, while low blur primarily affected early-stage processes. Low-frequency words exhibited greater shifting and skewing in distributional parameters, yet only high-frequency, highly blurred words demonstrated an enhanced memory effect. These findings suggest that both early and late cognitive processes contribute to the mnemonic benefits associated with perceptual disfluency.Overall, this study demonstrates that distributional and computational analyses provide powerful tools for dissecting encoding mechanisms and their effects on memory, offering valuable insights into models of perceptual disfluency."

keywords: [disfluency, LDT, DDM, ex-Gaussian, distributional analyses, word recognition]
floatsintext: true
numbered-lines: true
bibliography: references.bib
suppress-title-page: false
link-citations: false
mask: false
masked-citations:
draft-date: false
lang: en
language: 
  citation-last-author-separator: "and"
  citation-masked-author: "Masked Citation"
  citation-masked-date: "n.d."
  citation-masked-title: "Masked Title"
  email: "drjasongeller@gmail.com"
  title-block-author-note: "Author Note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows:"
format:
  apaquarto-docx: default
  apaquarto-pdf:
    documentmode: doc
    include-in-header:
      - text: |
          \usepackage{fvextra} % Advanced verbatim environment for better wrapping
          \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
          \renewcommand{\baselinestretch}{1.2} % Adjust line spacing for readability
          \setlength{\parskip}{0.5em} % Paragraph spacing for better readability
          \usepackage{geometry} % Control margins
          \geometry{margin=1in} % Ensure text does not overflow page
          \usepackage{setspace} % Allows flexible line spacing
          \setstretch{1.2} % Slightly stretch lines for better readability

execute: 
  echo: false
  warning: false
  message: false
  fig-align: center
  tbl-align: center
  keep-with-next: true
  code-overflow: wrap
  cache: true
  out-width: 50%
---

```{r}
#| label: load packages


library(easystats)
library(tidyverse)
library(patchwork)
library(knitr)
library(cowplot)
library(ggeffects)
library(data.table)
library(ggrepel)
library(brms)
library(ggdist)
library(emmeans)
library(tidylog)
library(tidybayes)
library(hypr)
library(cowplot)
library(colorspace)
library(ragg)
library(cowplot)
library(ggtext)
library(ggdist)
library(flextable)
library(cmdstanr)


options(timeout = 3000)
```

```{r}

# inf does not display correctly

# Check if ER column value is numeric, if not, replace with Inf
process_ER_column <- function(x) ifelse(is.infinite(x), "Inf", x)
```

```{r}
#| label: ggplot theme
#| echo: false


# Set up your theme
bold <- element_text(face = "bold", color = "black", size = 16) #axis bold
theme_set(theme_minimal(base_size = 16, base_family = "Times New Roman"))
theme_update(
  panel.grid.major = element_line(color = "grey92", linewidth = .4),
  panel.grid.minor = element_blank(),
  axis.title.x = element_text(color = "grey30", margin = margin(t = 7)),
  axis.title.y = element_text(color = "grey30", margin = margin(r = 7)),
  axis.text = element_text(color = "grey50"),
  axis.ticks =  element_line(color = "grey92", linewidth = .4),
  axis.ticks.length = unit(.6, "lines"),
  legend.position = "top",
  plot.title = element_text(hjust = 0, color = "black", 
                            family = "Times New Roman",
                            size = 21, margin = margin(t = 10, b = 35)),
  plot.subtitle = element_text(hjust = 0, face = "bold", color = "grey30",
                               family = "Arial", 
                               size = 14, margin = margin(0, 0, 25, 0)),
  plot.title.position = "plot",
  plot.caption = element_text(color = "grey50", size = 10, hjust = 1,
                              family = "Times New Roman", 
                              lineheight = 1.05, margin = margin(30, 0, 0, 0)),
  plot.caption.position = "plot", 
  plot.margin = margin(rep(20, 4))
)

# Set Okabe-Ito colors as the default scale for all plots
# scale_colour_discrete <- scale_colour_okabe_ito
# scale_fill_discrete <- scale_fill_okabe_ito
```

## Introduction

We live in a world that, even for adults, is, "blooming and buzzing with confusion" [@james1890, p.488]. Despite this, we possess the remarkable ability to accomplish perceptual tasks, such as deciphering unclear and unsegmented handwritten cursive words, or actively participating in conversations amidst the chaos of a noisy bar. The ability to deal with a noisy, confusing world, has been a topic of research at the interface between education and cognitive psychology. Decades of work has demonstrated a relationship between encoding difficulty and long-term memory. While making learning something harder and not easier runs counter to people's beliefs, a host of memory and learning effects show that making encoding more effortful (and thus more errorful), under certain circumstances, can be beneficial for memory. This has popularly become known as the "desirable difficulties" principle (Bjork & Bjork, 2011). Well-documented examples of desirable difficulties include spacing out encoding across multiple sessions rather than massing study during a single session [see @carpenter2022], studying concepts in an interleaved fashion rather than a blocked fashion [@rohrer2007], and generating or retrieving information rather than simply re-reading or studying the information again [@roediger2006; @slamecka1978].

Another desirable difficulty example involves a very simple manipulation---changing the perceptual characteristics of to-be-learned material to make it more difficult to process. A growing literature has shown that manipulating the perceptual characteristics of the to-be-learned material at encoding can improve memory [e.g., @geller2018; @geller2021; @halamish2018; @rosner2015]. The resulting memory benefit has been called the perceptual disfluency effect [@geller2018]. Many studies have begun to examine how changes in physical characteristics of to be learned materials influence encoding and memory.

## The perceptual disfluency effect

The relationship between perceptual disfluency and long-term memory has a long and storied history. While it is not quite clear where the term perceptual disfluency effect originated from, the idea behind it goes back to the late 80s with the work of @nairne1988. Under the term *perceptual-interference effect*, Narine employed the technique of backward masking with hash marks ( e.g., ####) with a quick presentation time to make word encoding noisier during study. Because the word is presented and masked so quickly, considerable effort is needed to identify the word. Since then, different types of perceptual disfluency manipulations have shown to elicit a similar effect, such as high-level blurring [@rosner2015], word inversion [@sungkhasettee2011] , small text size [@halamish2018], handwritten cursive [@geller2018], and other unusual or difficult-to-read typefaces [@geller2021; @weissgerber2017; @weltman2014].

Given the simplicity and ease in which perceptual disfluency can be implemented, it is not surprising researchers began touting the educational implications of such a manipulation. Perceptual disfluency as a possible educational intervention started to garner more support with the publication of @diemand-yauman2011. Across two experiments, @diemand-yauman2011 showed placing learning materials in disfluent typefaces (e.g., Comic Sans, Bodoni MT, Haettenschweiler, Monotype Corsiva) improved memory in the lab (when learning about space aliens) and in a high school classroom where students learned about a variety of different content areas (i.e., AP English, Honors English, Honors Physics, Regular Physics, Honors U.S. History, and Honors Chemistry) from PowerPoints with information presented in difficult to read typefaces.

Unfortunately, the evidence for the perceptual disfluency effect is mixed. A recent case in point is the typeface Sans Forgetica, developed through a collaboration among marketers, psychologists, and graphic designers (see Earp. 2018). Originally launched with the strong claim that it enhances memory retention due to the backward-slanting letters and gaps within each letter which requires individuals to 'generate' the missing parts of each word. However, several subsequent studies have failed to replicate these claims, finding Sans Forgetica to be *[just as]* forgettable [@cushing2022; @geller2020; @huff2022; @roberts2023; @taylor2020; @wetzler2021]. Similar results have been found for various other perceptual disfluency interventions, such as small font sizes [@rhodes2008], difficult-to-hear stimuli [@rhodes2009], minor blurring [@yue2013], and alternative typefaces [@rummer2015].

Due to the mixed findings, researchers began exploring boundary conditions of the disfluency effect. The importance of testing the effects of disfluency in the presence of other variables is key to its usefulness as an educational intervention. @geller2018, for instance, found level of disfluency (more vs. less disfluent) mattered. Using easy-to-read and hard-to-read handwritten cursive words,@geller2018 showed there is a Goldilocks zone for perceptual disfluency effects. That is, stimuli cannot be too easy to read (i.e., print words) or too hard to read (i.e., hard-to-read cursive). Only when the stimuli were moderately difficult or just right (i.e., easy-to-read cursive), did memory improve. In another paper, @geller2021 demonstrated that memory benefits for disfluent stimuli are more robust when test expectancy is low. That is, a disfluency effect is only seen when participants are not told about an upcoming memory test. The authors reasoned that knowing about a memory test engaged a strategy where all stimuli get processed to a deeper level, regardless of how perceptually disfluent the stimulus is. This countervails any benefit disfluency has on memory. Additionally, a few studies have noted the importance of individual differences. For example, @eskenazi2021 showed better spellers remembered more words and meanings than poor spellers when placed in a disfluent font (Sans Forgetica).

Though perceptual disfluency can occur in certain scenarios, its usefulness in educational settings, where students anticipate tests, could be limited. However, @geller2021 contended that perceptual disfluency has practical implications, particularly due to our reliance on incidental memory in everyday decision-making. For this to be effective, though, predicting when and where disfluency will occur is crucial.

## Theoretical accounts of the disfluency effect

To achieve this aim, we require a better understanding of the mechanisms involved in eliciting the disfluency effect. Several theories have been proposed to explain this phenomenon. @geller2018 provided a review of two theories put forth to explain the disfluency effect. The metacognition account of disfluency [@alter2013; @pieger2016] posits that disfluency acts as a signal to engage in more control/regulatory processing [@pieger2016]. Within this account, disfluency arises *after* the stimulus has been identified. As a result, the type of disfluency experienced does not matter, just that the learner perceives something as disfluent and regulates their behavior properly.

The compensatory processing account [@mulligan1996] suggests that the disfluency effect is a result of increased top-down processing from lexical and semantic levels of representation. This framework is largely based on the popular interactive activation mode (IA model) of word recognition [@mcclelland1981]. In the IA model, visual input activates representations at three levels of representation: the feature, letter, and word levels. Activation in the IA model is both feed-forward and feedbackward. Thus, when there is perceptual noise (such as by a mask), there is increased top-down processing from higher, lexical or semantic, levels to aid in word identification. It is this increased top-down processing to lower levels that produces better memory.

More recently, @ptok2019 put forth a limited capacity and stage-specific model to explain conflict-encoding effects like the perceptual disfluency effect. Within their model, memory effects arising from encoding conflict rely on (1) the level of processing tapped by the task and (2) metacognitive processes that include monitoring and control. Across six experiments, @ptok2020 demonstrated better recognition memory for target words when shown with incongruent versus congruent semantic distractor words (i.e., category labels of size, animacy, or gender; e.g., "Chair - Alive" vs. "Chair - Inanimate"), but no memory benefit for incongruent versus congruent response distractor words (e.g., Lisa -"left"/ Lisa - "right"). While both tasks resulted in conflict evinced by longer reaction times (RTs) to targets preceded by incongruent primes, only when the encoding task focused attention on target meaning (i.e., semantic categorization) rather than response selection did a memory benefit arise. In a follow-up set of experiments, @ptok2020 replicated this pattern of findings, and in addition, provided physiological evidence from cognitive pupillometry (i.e., the study of eye pupil size and how it relates to cognitive processing) (see @mathot2018 for a review). They observed larger pupil size (which has been taken as an index of cognitive control; see [@vanderwel2018], for a review) for semantic incongruent and response incongruent primes, but only observed a memory benefit for semantic incongruent conditions. Interestingly, they also showed that these memory benefits can be mitigated by manipulating endogenous attention. @ptok2020 (Experiment 3) were able to eradicate the conflict encoding benefit by having participants sit in a chinrest and focus on the task. This is similar to what has been found in the perceptual disfluency literature. For example, having participants study words in anticipation for a test can eradicate the benefit of perceptual disfluency @geller2021. In addition, requiring participants to make judgments of learning (a metamemory judgment on a scale of 0-100 indicating how likely it is they will recall the word on a later memory test) after each studied word also eradicates the disfluency effect [@rosner2015; @besken2013]. Taken together, this highlights the critical role of both the kind of processing done on the to-be-remembered stimuli, and control processes in eliciting a disfluency effect.

All three of these theories propose potential loci for the perceptual disfluency effect. In the metacognitive account, the disfluency effect occurs at a later post-lexical stage, after word recognition has taken place. The compensatory processing account [@mulligan1996] links the perceptual disfluency effect directly to the word recognition process. That is, disfluent words receive more top-down processing from lexical or semantic levels during encoding. Lastly, the stage-specific model proposed by @ptok2019 associates perceptual disfluency effects with a specific stage of processing, namely the semantic level, but it also considers general attentional and cognitive control processes that are not solely tied to the word recognition process.

## Moving beyond the mean: modeling RT distributions

### Ex-Gaussian distribution

To test the different stages or loci involved in the perceptual disfluency effect, it is necessary to use methods that allow for a more fine-grained analysis of processes during encoding. In the perceptual disfluency literature (and learning and memory more broadly), it is common to use measures such as mean accuracy and RTs to assess differences between a disfluent and fluent condition [@geller2018; @geller2021; @rosner2015]. While this approach is often deemed as acceptable practice, there has been a call to go beyond traditional RT methods when making inferences [see @balota2011].

There are a several reasons for making the shift from traditional *[mean]*-RT analyses *[(which is what linear models do)]* to *approaches* that utilize the whole RT distribution. One reason is traditional approaches fail to capture the nuances inherent in RT distributions. Namely, RT distributions are unimodal and positively skewed. A standard analysis based on means can conceal effects that change only the shape of the tail of the distribution, only the location, or both the location and the shape of the distribution.

Another reason to transition away from traditional analyses is that RTs provide only a coarse measure of processing during encoding. RTs capture the total sum of various task-related factors, ranging from non-decisional components like stimulus encoding and motor responses to decisional components. This amalgamation does not allow one to parse out specific effects that might occur early or later in processing.

Lastly, from a statistical standpoint, RTs present significant challenges. Specifically, they often violate two crucial assumptions: they are not normally distributed and their variance is frequently heterogeneous. Such violations can lead to biased results when making statistical inferences, as pointed out by \[Wilcox, 1998\].

A perfect example of this comes from the Stroop task [@stroop1935] . The classic Stroop finding shows words presented in an incongruent color font (the word "red" printed in "blue" font) increases RTs compared to words in a baseline condition (e.g., XXXXX presented in a color font). The Stroop interference effect is something you can bet your house on. A more inconsistent finding is seeing facilitation (shortened RTs) when the word and color are congruent (i.e., "Green" presented in "Green") compared to a baseline condition.

One alternative for this conundrum is examine RT distributions using mathematical models that capture the nuances of the RT distribution and consider various statistical properties, such as the shape (\$\\mu\$) , spread ( $\sigma$) , and skewness ($\tau$) of the distribution. One popular choice is the ex-Gaussian distribution [@balota2011; @ratcliff1978]. As the name suggests, the ex-Gaussian distribution decomposes RTs into three parameters: mu (Œº) representing the mean of the Gaussian component, sigma (œÉ) representing the standard deviation of that Gaussian component, and tau (œÑ) representing the mean and standard deviation of an exponential component capturing the tail of the distribution. The algebraic mean of ex-Gaussian is a combination of $\mu$ + $\tau$ . Together the three parameters represent different aspects of the distribution's location and shape.

@heathcote1991 looked at this Stroop issue with an ex-Gaussian model and found both facilitation (from congruent trials) and interference (from incongruent trials) on $\mu$. For $\sigma$, the analysis showed interference, but no facilitation. For $\tau$, there was interference for both congruent and incongruent conditions. Comparing this to a mean RT analysis, they showed the standard interference Stroop effect, but no facilitation. Given that the algerbraic mean of the ex-Gaussian is $\mu$ + $\tau$, the failure to observe a facilitation effect in the standard mean analysis likely arose from facilitation on $\mu$ and interference on $\tau$ canceling each other out. A finding such as this would be impossible looking solely at mean RTs.

Exploring effects from a distributional perspective has provided a richer understanding of how different experimental manipulations affect word recognition. Experimental manipulations can produce several distinct patterns. One pattern involves a shift of the entire RT distribution to the right, without increasing the tail or skew. A pattern such as this would suggest a general effect and would manifest as an effect on $\mu$, but not $\tau$ . As an example, semantic priming effects--where responses are faster to targets when preceded by a semantically related prime compared to an unrelated prime--can be nicely explained by a simple shift in the RT distribution [@balota2008]. Alternatively, an experimental manipulation could produce a pattern where the RT distribution is skewed or stretched in the slower condition. This suggests that the manipulation only impacts a subset of trials, and is visible as an increase in $\tau$. An example of an effect that only impacts $\tau$ is the transposed letter effect in visual word recognition [@johnson2012]. The transposed letter (TL) effect involves misidentification of orthographically similar stimuli that with transposed internal like, like mistaking "JUGDE" for "JUDGE" [@perea2003]. Finally, you could observe a pattern wherein an experimental manipulation results in both changes in $\mu$ and $\tau$, which would shift and stretch the RT distribution. Recognizing low frequency words have been shown to not only shift the RT distribution, but also stretch the RT distributions [@andrews2001; @balota1999; @staub2010].

The ex-Gaussian model, while mostly descriptive in nature. has been used as a theoretical tool to map model parameters onto cognitive processes. For example, the $\mu$ and $\sigma$ parameters have been tied to early, non-analytic, processing. In the area of semantic priming, the selective effect on $\mu$ has been taken as evidence for an automatic spreading activation process (or head-start), according to which the activation of a node in a semantic network spreads automatically to interconnected nodes, preactivating a semantically related word [@dewit2015; @balota2008]. The exponential component ($\tau$) has been tied to later, more analytic, processing [@balota1999]. Specifically, increases in $\tau$ have been attributed to working memory and attentional processes [@Fitousi2020; @kane2003]. For instance, @johnson2012 tied $\tau$ differences for the TL effect to a post-lexical checking mechanism that arose from a failure to identify the stimulus on a select number of trials rather than a broader, lexical, effect occurring on every trial. When taken together, these findings suggest that ex-Gaussian parameters could map to early vs. late stages of cognitive processing. However, such mapping between distributional descriptives and and cognitive processes remains controversial and should be interpreted carefully [@matzke2009; @heathcote1991].

### Drift-diffusion model (DDM)

Contrary to the ex-Gaussian distribution discussed above, the drift diffusion model - DDM [see @ratcliff2016, for a comprehensive introduction] is a process-model and it's parameters can be linked to latent cognitive constructs [@gomez2013]. The DDM is a popular computational model commonly used in binary speeded decision tasks such as the lexical decision task (LDT). The DDM model assumes a decision is a cumulative process that begins at stimulus onset and ends once a noisy accumulation of evidence has reached a decision threshold.The DDM has led to important insights into cognition in a wide range of choice tasks, including perceptual-, memory-, and value-based decisions [@myers2022].

In the DDM, RTs are decomposed into several parameters that represent distinct cognitive processes. The most relevant to our purposes here are the drift rate ($v$) and non-decision time (ndt; $T_{er}$) parameters. Drift rate ($v$) represents the rate at which evidence is accumulated towards a decision boundary. In essence, it is a measure of how quickly information is processed to make a decision. A higher (more positive) $v$ indicates a steeper slope, meaning that evidence is accumulated more quickly, leading to faster decisions. Conversely, a lower $v$ indicates a shallower slope, meaning that evidence is accumulated more slowly. Drift rate is closely linked to the decision-making process itself and serves as an index of global processing demands imposed by factors such as task difficulty, memory load, or other concurrent cognitive demands‚Äîparticularly when these processes compete for the same cognitive resources [@boag2019]. Additionally, drift rates have been implicated as a key mechanism of reactive inhibitory control [@braver2012], where critical events (e.g., working memory updates or task switches) trigger inhibition of prepotent response drift rates [@boag2019; @boag2019a].

The $T_{er}$ parameter represents the time taken for processes other than the decision-making itself. This includes early sensory processing (like visual or auditory processing of the stimulus) and late motor processes (like executing the response).

The DDM has been shown to be a valuable tool for studying the effects of different experimental manipulations on cognitive processes in visual word recognition. For example, @gomez2014 demonstrated certain manipulations can deferentially affect specific parameters of the model. For instance manipulating the orientation of words (rotating them by 0, 90, or 180 degrees) affected the $T_{er}$ component, but not $v$ component. In contrast, word frequency (high-frequency words vs. low-frequency words) primarily influenced both the drift rate and non-decision time. These findings highlight the sensitivity of the DDM in identifying and differentiating the impact of various stimulus manipulations on different cognitive processes involved in decision-making.

## Goals of the present experiments

In the present experiments, we pursued two aims related to perceptual disfluency. The first aim was to examine the replicability of the perceptual disfluency effect. To optimize our chances for observing this effect, we utilized a disfluency manipulation known to enhance memory in the literature‚Äîblurring [@rosner2015]. We manipulated perceptual disfluency by blurring the words at three levels. Participants were presented with clear words (no blur), low blurred words (5% Gaussian blur) and high blurred words (15% Gaussian blur). High level blurring has been shown to enhance memory [@rosner2015]. As @geller2018 noted, not all manipulations are created equal. Perceptual manipulations affect processing in different ways. It is important to show just how these manipulations affect different stages of processing and what type of manipulations do and do not produce a disfluency effect. By examining different levels of perceptual disfluency, we provide a more nuanced account of encoding processes and how this affects memory.

The second, more pivotal aim, was to enrich the toolkit of researchers exploring conflict encoding like perceptual disfluency. Through the application of distribution techniques, like the ex-Gaussian analysis and DDM, our goal was to showcase how these techniques can grant us a deeper insight into the influence of encoding difficulty on memory. Overall, these endeavors will help us ascertain the conditions under which perceptual disfluency is beneficial for memory and when it is not.

### Predictions

Using the ex-Gaussian distribution and the DDM will provide us with a descriptive account of how disfluency manipulations affect encoding. Each theoretical account makes specific predictions about the loci of the perceptual disfluency affect. We can use these predictions to forecast how the components in each mathematical model might be influenced.

If the metacognitive account is correct [@alter2013], which assumes a post-lexical locus for the perceptual disfluency effect, we might predict a lengthening of the distribution tail on some of the trials for blurred words. This would manifest itself on the $\tau$ component. Specifically, a larger $\tau$ parameter estimate for high blurred and low blurred words compared to no blurred words. As it relates to memory performance, there should be no difference between high and low blurred words.

Another scenario entails a general slow down of processing---causing distributional shifting $\mu$, but not skewing $\tau$. According to the the compensatory processing account [@mulligan1996], we would expect increased shifting for high blurred words compared to low and no blurred words. This should result in a mnenmonic benefit for high blurred words, but not low blurred words.

Lastly, we might observe not only a shift of the entire distribution (an effect on $\mu$), but also change the shape of the distribution (an effect on $\tau$ ), indicating a combination of early and late processes. A similar pattern has been found with hard-to-read handwritten words [e.g., @perea2016; @vergara-mart√≠nez2021]. This extra post-lexical processing received by high blurred words is assumed to facilitate better recognition memory. This finding would be in line with the stage-specific account [@ptok2019]. The finding of better memory for high blurred words, but not low blurred words would be in line with the stage-specific account of conflict encoding [@ptok2019, @ptok2020]. Having a better sense of when and where disfluency effects arise is critical in determining its usefulness in the educational milieu.

In terms of the DDM parameters, we predicted high blurred words would affect both $v$ and $T_{er}$ parameters. Specifically, high blurred words would produce lower $v$ and high $T_{er}$ compared to clear and low blurred words. Additionally, we predicted that low blurred words would only affect $T_{er}$. @tbl-predictions summarizes each account of the perceptual disfluency and the predicted outcomes according to each mathematical model.

In addition to ex-Gaussian and DDM analyses, we also used quantile and delta plots to better understand the changes in the RT distribution across different conditions. These techniques are crucial as they shed light on how the impact of a particular manipulation varies across the RT distribution.

The process of visualization through quantile analysis can be broken down into four distinct steps:

1.  Sorting and plotting: For correct trials, RTs are arranged in ascending order within each condition. We then plot the average of the specified quantiles (e.g., .1, .2, .3, .4, .5, .9).

2.  Quantile averaging across participants: The individual quantiles for each participant are averaged, a concept reminiscent of Vincentiles.

3.  Between-condition quantile averaging: The average for each quantile is computed between the conditions.

4.  Difference Calculation: We determine the difference between the conditions, ensuring the sign of the difference remains unchanged.

Typically, there are four observable patterns. No observable difference occurs when the conditions do not show any noticeable distinction. Late differences emerge when increasing differences appear later in the sequence, suggesting that the conditions diverge over time. A complete shift indicates a consistent difference across all quantiles, signaling an overall shift in the distribution. Finally, early differences reveal distinctions early in the reaction time distribution, suggesting an initial divergence between conditions. maps these patterns onto existing theoretical models of disfluency.

```{r}
#| label: tbl-predictions
#| tbl-cap: "Mapping model predictions to theoretical constructs"

# Create the data frame using Unicode symbols
data <- data.frame(
    Account = rep(c("Meta-cognitive", "Compensatory-processing", "Stage-specific"), each = 2),
    Description = rep(c(
        "Perceptual disfluency affects meta-cognitive processes via increased system 2 processing", 
        "Perceptual disfluency affects the word recognition process", 
        "Disfluency effects rely on (1) the stage or level of processing tapped by the task and (2) monitoring and control processes"
    ), each = 2),
    Loci = rep(c("Post-lexical", "Lexical/semantic", "Lexical/semantic and Post-lexical"), each = 2),
    Contrast = c("High blur vs. Low blur/Clear", "Low blur vs. Clear",
                 "High blur vs. Low blur/Clear", "Low blur vs. Clear",
                 "High blur vs. Low blur/Clear", "Low blur vs. Clear"),
    ExGaussianPredictions = c(
        "Œº: No difference\nœÑ: Increase", "Œº: No difference\nœÑ: Increase",
        "Œº: Increase\nœÑ: No difference", "Œº: Increase\nœÑ: No difference",
        "Œº: Increase\nœÑ: Increase", "Œº: Increase\nœÑ: Increase"
    ),
    DDMPredictions = c(
        "v: No difference\nT‚Çë·µ£: Increase", "v: No difference\nT‚Çë·µ£: Increase",
        "v: Increase\nT‚Çë·µ£: No difference", "v: Increase\nT‚Çë·µ£: No difference",
        "v: Increase\nT‚Çë·µ£: Increase", "v: No Increase\nT‚Çë·µ£: Increase"
    ),
    QuantilePlots = c(
        "Late Difference",
        "Late Difference", 
        "Complete Shift", 
        "No Difference", 
        "Complete Shift + Late Differences", 
        "Early Difference"
    ), 
    RecognitionMemoryPredictions = c(
        "No difference", "No difference",
        "High blurred > Low blurred/Clear", "Low blurred > Clear",
        "High blurred > Low blurred/Clear", "Low blurred = Clear"
    ),
    stringsAsFactors = FALSE
) |>
    mutate(
        Account = ifelse(duplicated(Account), "", Account),
        Description = ifelse(duplicated(Description), "", Description),
        Loci = ifelse(duplicated(Loci), "", Loci)
    )

# Create flextable
ft <- flextable(data) |>
    set_header_labels(
        Account = "Account",
        Description = "Description",
        Loci = "Loci",
        Contrast = "Contrast",
        ExGaussianPredictions = "Ex-Gaussian Predictions",
        DDMPredictions = "Drift Diffusion Predictions",
        QuantilePlots = "Quantile Plots",
        RecognitionMemoryPredictions = "Recognition Memory Predictions"
    ) |>
    merge_v(j = c("Account", "Description", "Loci")) |>
    align(j = c("Account", "Description", "Loci", "Contrast"), align = "left", part = "body") |>
    align(j = c("ExGaussianPredictions", "DDMPredictions", "QuantilePlots", "RecognitionMemoryPredictions"), 
          align = "center", part = "body") |>
    bold(part = "header") |>
    autofit()

# Print the table
ft
```

## Experiment 1a: Context Reinstatement

In Experiment 1a, we collected RTs from a lexical decision task (LDT) during encoding followed by a surprise recognition memory test. Using a two-choice task like the LDT allowed us to examine how perceptual disfluency affects encoding processes using mathematical models. Based on previous research [@geller2021], there was no mention of the recognition test when participants signed up for the study to give us the best chance of observing a disfluency effect.

### Method

#### Transparency and Openness

This study complies with transparency and openness guidelines. The preregistered analysis plan for this experiment can be found here: https://osf.io/q3fjn. All raw and summary data, materials, and R scripts for pre-processing, analysis, and plotting for Experiments 1 can be found at https://osf.io/6sy7k/. All deviations and changes from the preregistration are noted herein.

### Participants

We preregistered a sample size of 216. All participants were recruited through the university subject pool (SONA). A design with a sample size of 216 can detect effect sizes of Œ¥ ‚â• 0.2 with a probability of at least 0.90, assuming a one-sided criterion for detection that allows for a maximum Type I error rate of Œ± = 0.05. Per our exclusion criteria, we retained participants that were native English speakers, were over the age of 17, had overall accuracy on the LDT greater than 80%, and did not complete the experiment more than once. Due to our exclusion criteria, we oversampled participants. Because of this we randomly chose 36 participants from each list to reach our target sample size.

### Apparatus and stimuli

The experiment was run using PsychoPy software and hosted on Pavlovia (www.pavlovia.org). You can see an example of the experiment by navigating to this website: <https://run.pavlovia.org/Jgeller112/ldt_dd_l1_jol_context>.

We used 84 words and 84 nonwords for the LDT. Words were obtained from the the LexOPS package in R [@taylor2020a]. All of our words were matched on a number of different lexical dimensions. All words were nouns, 4-6 letters in length, had a known proportion of between 90%-100%, had a low neighborhood density (OLD20 score between 1-2), high concreteness, imageability, and word frequency. Our nonwords were were created using the English Lexicon Project [@balota2007]. Stimuli can be found at our OSF project page cited above.

#### Blurring

Blurred stimuli were processed through the imager package in R [@imager] and a personal script (https://osf.io/gr5qv). Each image was processed through a high blur filter (Gaussian blur of 15) and low blur filter (Gaussian blur of 10). These pictures were then imported into PsychoPy as picture files. See @fig-blur for examples of what a clear, low blurred, and high blurred word would look like to the participant.

```{r}
#| label: fig-blur
#| fig-cap: Clear (left), low  blur (10% blur) (right), and high blur (15% blur) (center) examples 
# Combine images in one figure
include_graphics("figures/blurexp.png")
```

### Design

We created two lists: One list (84 words; 28 clear, 28 low blur, 28 high blur) served as a study (old) list for the LDT task while the other list served as a test (new) list (84 words; 28 clear, 28, LB, 28, HB) for our recognition memory test that occurred after the LDT. We counterbalanced each list so each word served as an old word and a new world and were presented in clear, low blurred, and high blurred across participants. This resulted in six counterbalanced lists. Lists were assigned to participants so that across participants each word occurs equally often in the six possible conditions: clear old, LB old, HB old, clear new, LB new, and HB new. For the LDT task, we generated a set of 84 legal nonwords that we obtained from the English Lexicon Project. These 84 nonwords were used across all 6 lists.

### Procedure

The experiment consisted of two phases: an encoding phase (LDT) and a test phase. During the encoding phase, a fixation cross appeared at the center of the screen for 500 ms. The fixation cross was immediately replaced by a letter string in the same location. To continue to the next trial, participants had to decide if the letter string presented on screen was a word or not by either pressing designated keys on the keyboard ("m" or "z") or by tapping on designated areas on the screen (word vs. nonword) if they were using a cell phone/tablet. After the encoding phase, participants were given a surprise old/new recognition memory test. During the test phase, a word appeared in the center of the screen that either had been presented during study ("old") or had not been presented during study ("new"). Old words occurred in their original typeface, and following the counterbalancing procedure, each of the new words was presented as clear, low blurred, or high blurred. All words were individually randomized for each participant during both the study and test phases and progress was self-paced. After the experiment, participants were debriefed. The entire experiment lasted approximately 15 minutes.

### Analysis Plan

All models were fit using The Stan modeling language [@grant2017] with the `brms` [@brms] package in in R (v.4.4.2; @R). For all reported models, we fit maximal random-effect structures justified by the design (Barr et al., 2013) *[REF should be bibtex]*. This included random intercepts for participants and items, and a random slope by blurring level for each varying random intercept. Contrast codes (effects coding) for each variable were created using the `hypr` package in R [@hypr]. We fit the models twice: Once with contrast codes for high blur vs. clear and for high blur vs. Low blur and once with the low blur vs. clear contrast.

In all experiments reported here, the statistical model was run with four chains of 5,000 Markov chain Monte Carlo iterations (For the DDM we fit a model with 2,000 chains to reduce computational time), with 1,000-iteration warm-ups for 4 chains (16,000 samples in total). Convergence and stability of the Bayesian sampling is quantified by the RÀÜ diagnostics below 1.01 and Effective Sample Size (ESS) greater than 1000 [@brms]. For both the RT data and accuracy data, we report our models with with weakly-informative priors for the population-level parameters. Using a weakly-informative prior as opposed to a default (which is an uniform prior where all effects are equally likely) allows for the calculation of evidence ratio (Bayes Factor) for one-sides tests. For the ex-Gaussian analysis we used a weak prior (i.e., N \~ (0, 100)). For the population-level effects in the accuracy and signal detection analyses, we used a Cauchy distribution with the mean of 0 and scale of 0.35 (cauchy \~ 0, 0.35)) recommended by [@kinoshita2023] for logistic regression.

For the marginal means and differences, we report the expected values under the posterior distribution and their 90% credible intervals (Cr. I.). For marginal mean differences, we also report the posterior probability that a difference Œ¥ is not zero. If a hypothesis states that Œ¥ \\\> 0, then it would be considered strong evidence for this hypothesis would be if zero is not included in the 90% Cr. I. of Œ¥ and the posterior P(Œ¥ \\\> 0) is close to one (by a reasonably clear margin). To extract the estimated marginal means from the posterior distribution of the fitted models we used a combination of `emmeans` R package [@emmeans-2] `bayestestr`[@bayestestR], and `brms` [@brms-2].

Model quality was thoroughly assessed via predictive prior and posterior checks, $\hat{R}$ and divergence diagnostics. In order to assess the evidence in favor or against our hypotheses, we used Evidence Ratio (ER, a generalization of Bayes factors allowing for directional hypotheses). An ER above 3 indicates moderate to substantial evidence for our hypothesis, below 0.3 indicates moderate to substantial evidence for the null hypothesis, and anything in between is inconclusive evidence [@BayesFactor].

#### Ex-Gaussian distribution

We used the ex-Gaussian distribution to model response times, with both the mean of the Gaussian component ùúá and the scale parameter of the exponential component ùõΩ (equaling the inverse of the rate parameter ùúÜ) being allowed to vary between conditions. In addition, to better visualize the distributional features of the latency data, we computed the delta plots for all variables.

#### Diffusion model

We used a hierarchical-Bayesian variant of the Wiener diffusion model [@vandekerckhove2011] with accuracy coding. This model accounts for the entire data (i.e., RT distributions of correct and error trials) with three latent parameters: (a) the drift rate, a measure of the efficiency of information processing in the decision process, (b) the boundary separation, a measure of response caution that controls the speed-accuracy trade-off (this was fixed to .5), and (c) the non-decision time on-coding parametrization -- to each dataset.

#### Recognition memory

For our recognition memory data, we fit a Bayesian generalized linear multilevel model with a Bernoulli distribution with a probit link. In its simplest from, SDT models are regressions with a probit link. To estimate the SDT parameter of interest (\$d\^\$), we fit a Bayesian hierarchical generalized linear model with a binomial distribution probit link function to participant responses (their actual response (old vs. new)) as a function of actual item status (old vs. new) and blurring level. Traditional SDT analyses have proven to be an informative and efficient approach to analyzing binary accuracy data. However, considering the deficiency in precision and power in traditional analyses compared to mixed effects analyses, it is worth considering a Bayesian generalized linear mixed effect approach to SDT [see @zloteanu2024] for a nice tutorial on Bayesian SDT models.

### Results and Discussion

All models presented no divergences, and all chains mixed well and produced comparable estimates ($\hat{R}$ \< 1.01).

#### Accuracy

```{r}
#The data file is cleaned (participants >=.8, no duplicate participants, no participants < 17. )
# get data from osf
blur_acc <- read.csv("https://osf.io/xv5bd/download") |>
    dplyr::filter(lex=="m")


blur_acc_new<- blur_acc |>
  dplyr::filter(rt >= .2 & rt <= 2.5)
```

The analysis of accuracy is is based on `r dim(blur_acc)[1]` data points. After removing fast and slow RTs we were left with `r dim(blur_acc_new)[1]` data point points `r 1-dim(blur_acc_new)[1]/dim(blur_acc)[1]` %

```{r}
#| echo: false

## Contrasts
#hypothesis
blurC <- hypr(HB~C, HB~LB, levels=c("C", "HB", "LB"))

#set contrasts in df 
blur_acc_new$blur <- as.factor(blur_acc_new$blur)

contrasts(blur_acc_new$blur) <-contr.hypothesis(blurC)
```

```{r}
#| eval: false
#| echo: false

#weak prior
prior_exp1 <- c(set_prior("cauchy(0,.35)", class = "b"))

#fit model
fit_acc_weak <- brm(corr ~ blur + (1 + blur | participant) + (1 + blur | string),
  data = blur_acc_new,
  warmup = 1000,
  iter = 5000,
  chains = 4,
  init = 0,
  family = bernoulli(),
  cores = 4,
  prior = prior_exp1,
  control = list(adapt_delta = 0.9),
  backend = "cmdstanr",
  save_pars = save_pars(all = T),
  sample_prior = T,
  threads = threading(4),
  file = "fit_acc_context"
)
```

```{r}
#| label: expt 1a contrast code accuracy
#| echo: false


## Contrasts
#hypothesis
blurC <-hypr(LB~C, levels=c("C", "HB", "LB"))

#set contrasts in df 
blur_acc_new$blur <- as.factor(blur_acc_new$blur)

contrasts(blur_acc_new$blur) <-contr.hypothesis(blurC)

```

```{r}
#| eval: false
#| echo: false
#weak prior
prior_exp1 <- c(set_prior("cauchy(0,.35)", class = "b"))

#fit model
fit_acc_weak_lb <- brm(corr ~ blur + (1 + blur | participant) + (1 + blur | string),
  data = blur_acc_new,
  warmup = 1000,
  iter = 5000,
  chains = 4,
  init = 0,
  family = bernoulli(),
  cores = 4,
  prior = prior_exp1,
  control = list(adapt_delta = 0.9),
  backend = "cmdstanr",
  save_pars = save_pars(all = T),
  sample_prior = T,
  threads = threading(4),
  file = "fit_acc_context_lbc"
)

```

```{r}
# get file from osf
acc_c<-read_rds("https://osf.io/xwdzn/download")

# get lowblur-c comparison
acc_lb <- read_rds("https://osf.io/wt3ry/download")
```

```{r}

acc_means <- emmeans(acc_c, specs="blur", type="response")

```

```{r}
#| label: tbl-acc1a
#| tbl-cap: Summary of posterior fixed effect estimates for accuracy hypotheses in Experiment 1a 

a <- hypothesis(acc_c, "blur1 <  0") 

# Ensure rounding applies only to numeric columns
a$hypothesis <- a$hypothesis |>
  mutate(across(where(is.numeric), ~ round(.x, 3)))

b <- hypothesis(acc_c, "blur2 <  0")

# Ensure rounding applies only to numeric columns
b$hypothesis <- b$hypothesis |>
  mutate(across(where(is.numeric), ~ round(.x, 3)))

c<- hypothesis(acc_lb, "blur1 =  0")

# Ensure rounding applies only to numeric columns
c$hypothesis <- c$hypothesis |>
  mutate(across(where(is.numeric), ~ round(.x, 3)))

tab <- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis) |>
    mutate(Evid.Ratio=as.numeric(Evid.Ratio))|>
  rename("Mean" = "Estimate") |>
  select(-Star)

tab[, -1] <- t(apply(tab[, -1], 1, round, digits = 3))


acc_exp1a <- tab |> 
    mutate(Hypothesis = c("High Blur - Clear < 0", "High Blur - Low Blur < 0", "Low Blur - Clear =  0 ")) |> 
    mutate(
        mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
        `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")) |>
    rename("SE" = "Est.Error", "ER" = "Evid.Ratio") |>
    select(Hypothesis, Mean, SE, `90% CrI`,ER, Post.Prob)|> 
  flextable() |>
  autofit() |> 
  theme_apa()

acc_exp1a

```

Model estimates can be found in @tbl-acc1a. Clear words were better identified ($M$ = 0.985) compared to high blur words ($M$ = 0.962), $b$ = `r a$hypothesis$Estimate`, 90% Cr.I\[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`. Low blurred words were better identified ($M$ = 0.986) than high blurred words, $b$ = `r b$hypothesis$Estimate`, 90% Cr.I\[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`. However, the evidence was weak for there being no significant difference in the identification accuracy between clear and low blurred words, $b$ = `r c$hypothesis$Estimate`, 90% Cr.I\[`r c$hypothesis$CI.Lower`, `r c$hypothesis$CI.Upper`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`.

#### RTs: Ex-Gaussian

```{r}
#load data from osf
rts <- read_csv("https://osf.io/xv5bd/download")

```

```{r}

blur_rt<- rts |>
  group_by(participant) |>
   dplyr::filter(corr==1, lex=="m")#only include nonwords

blur_rt_new <- blur_rt |> 
  dplyr::filter(rt >= .2 & rt <= 2.5)|> # remove high and low RTs
    mutate(rt_ms=rt*1000)

```

The analysis of RTs (correct trials and words) is based on `r dim(blur_rt_new)[1]` data points, after removing fast and slow RTs (`r 1-dim(blur_rt_new)[1]/dim(blur_rt)[1]` %)

```{r}
## Contrasts
#hypothesis
blurC <-hypr(HB~C, HB~LB, levels=c("C", "HB", "LB"))

#set contrasts in df 
blur_rt$blur <- as.factor(blur_rt$blur)

contrasts(blur_rt$blur) <-contr.hypothesis(blurC)

```

```{r}
#| eval: false

bform_exg1 <- bf(
rt ~  0 + blur + (1 + blur |p| participant) + (1 + blur|i| string),
sigma ~ 0 + blur + (1 + blur |p|participant) + (1 + blur |i| string),
beta ~ 0 + blur + (1 + blur |p|participant) + (1 + blur |i| string))
```

```{r}
#| eval: false

prior_exp1 <- c(set_prior("normal(0,100)", class = "b", coef=""))
                

fit_exg1 <- brm(
bform_exg1, data = blur_rt,
warmup = 1000,
                    iter = 5000,
                    chains = 4,
                    prior = prior_exp1,
                    family = exgaussian(),
                    init = 0,
                    cores = 4, 
sample_prior = T, 
save_pars = save_pars(all=T),
control = list(adapt_delta = 0.8), 
backend="cmdstanr", 
threads = threading(4))

setwd(here::here("Context", "RT_BRM_Model"))

save(fit_exg1, file = "blmm_rt_context_05-25-23.RData")

```

```{r}
#load rdata for model 
#load_github_data("https://osf.io/uxc2f/download")

#setwd(here::here("Expt1", "BRM_ACC_RT"))

#here::here("Expt1", "BRM_ACC_RT", #"blmm_rt_context_05-25-23.RData"))

# no intercept model - easier to fit priors on all levels of factor 
fit_c <- read_rds("https://osf.io/82nre/download")

```

```{r}
#| label: tbl-exgauss1a
#| tbl-cap: "Summary of posterior fixed effect estimates for Ex-Gaussian parameter hypotheses in Experiment 1a"

# Compute hypothesis tests
a <- hypothesis(fit_c, "blurHB - blurC > 0", dpar="mu") 
b <- hypothesis(fit_c, "blurHB - blurLB > 0", dpar="mu")
c <- hypothesis(fit_c, "blurLB - blurC > 0", dpar="mu")
d <- hypothesis(fit_c, "sigma_blurHB - sigma_blurC > 0", dpar="sigma")
e <- hypothesis(fit_c, "sigma_blurHB - sigma_blurLB > 0", dpar="sigma")
f <- hypothesis(fit_c, "sigma_blurLB - sigma_blurC = 0", dpar="sigma")
g <- hypothesis(fit_c, "beta_blurHB - beta_blurC > 0", dpar="beta")
h <- hypothesis(fit_c, "beta_blurHB - beta_blurLB > 0", dpar="beta")
i <- hypothesis(fit_c, "beta_blurLB - beta_blurC = 0", dpar="c")

# Ensure rounding applies only to numeric columns
a$hypothesis <- a$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
b$hypothesis <- b$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
c$hypothesis <- c$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
d$hypothesis <- d$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
e$hypothesis <- e$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
f$hypothesis <- f$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
g$hypothesis <- g$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
h$hypothesis <- h$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
i$hypothesis <- i$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))

tab <- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis, g$hypothesis, h$hypothesis, i$hypothesis) |> 
    mutate(Evid.Ratio=as.numeric(Evid.Ratio))|>
   rename("Mean" = "Estimate") |>
  select(-Star)

tab[, -1] <- t(apply(tab[, -1], 1, round, digits = 3))

tab |> 
  mutate(Parameter=c("mu","mu", "mu",  "sigma", "sigma", "sigma", "beta", "beta", "beta"))|>
  mutate(Hypothesis = c("High Blur - Clear > 0", "High Blur - Low Blur > 0", "Low Blur - Clear >  0 ", "High Blur - Clear > 0", "High Blur - Low Blur > 0", "Low Blur - Clear =  0","High Blur - Clear > 0", "High Blur - Low Blur > 0", "Low Blur - Clear = 0  "))|> 
  mutate(
        mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
        `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")) |>
    rename("SE" = "Est.Error", "ER" = "Evid.Ratio") |>
    select(Hypothesis, Parameter,Mean, SE, `90% CrI`,ER, Post.Prob)|>
  flextable() |>
  autofit() |>
  theme_apa()

```

A visualization of how blurring affected processing during word recognition can be seen in the quantile and delta plots in @fig-deltaquant. Beginning with the Œº parameter, there was greater shifting for high blurred words compared to clear words, *b* = `r round(a$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(a$hypothesis$CI.Lower, 3)`, `r round(a$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`, and low blur words, *b* = `r round(b$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(b$hypothesis$CI.Lower, 3)`, `r round(b$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`.

Analyses of the œÉ and œÑ parameters yielded a similar pattern. Variance was higher for high blurred words compared to clear words, *b* = `r round(d$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(d$hypothesis$CI.Lower, 3)`, `r round(d$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(d$hypothesis$Evid.Ratio)`, and low blurred words, *b* = `r round(e$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(e$hypothesis$CI.Lower, 3)`, `r round(e$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(e$hypothesis$Evid.Ratio)`.

Finally, there was greater skewing for high blurred words compared to clear words, *b* = `r round(g$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(g$hypothesis$CI.Lower, 3)`, `r round(g$hypothesis$CI.Upper, 3)`\], ER = `r round(g$hypothesis$Evid.Ratio, 3)`, and low blurred words, *b* = `r round(h$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(h$hypothesis$CI.Lower, 3)`, `r round(h$hypothesis$CI.Upper, 3)`\], ER = `r round(h$hypothesis$Evid.Ratio, 3)`.

Low blurred words compared to clear words only differed on the Œº parameter, *b* = `r round(c$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(c$hypothesis$CI.Lower, 3)`, `r round(c$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`, with greater shifting for low blurred words. For $\tau$ and $\sigma$, the 95% Cr.I crossed zero and the ER was greater than 100.

```{r}

#Delta plots (one per subject) 
quibble <- function(x, q = seq(.1, .9, .2)) {
  tibble(x = quantile(x, q), q = q)
}

data.quantiles <- rts |>
  dplyr::filter(rt >= .2 | rt <= 2.5) |> 
  dplyr::group_by(participant,blur,corr) |>
  dplyr::filter(lex=="m")|>
  dplyr::summarise(RT = list(quibble(rt, seq(.1, .9, .2)))) |> 
  tidyr::unnest(RT)


data.delta <- data.quantiles |>
  dplyr::filter(corr==1) |>
  dplyr::select(-corr) |>
  dplyr::group_by(participant, blur, q) |>
  dplyr::summarize(RT=mean(x))


#Delta plots (based on vincentiles)
vincentiles <- data.quantiles |>
  dplyr::filter(corr==1) |>
  dplyr::select(-corr) |>
  dplyr::group_by(blur,q) |>
  dplyr::summarize(RT=mean(x)) 

v=vincentiles |>
  dplyr::group_by(blur,q) |>
  dplyr::summarise(MRT=mean(RT))

v <- v |>
  mutate(blur=ifelse(blur=="HB", "High blur", ifelse(blur=="LB", "Low blur", "Clear")))


v$blur<- factor(v$blur, level=c("High blur", "Low blur", "Clear"))



p <- ggplot(v, aes(x = q, y = MRT*1000, colour = blur, group=blur))+
  geom_line(size = 1) +
  geom_point(size = 3)  +  
  scale_y_continuous(breaks=seq(500,1600,100)) +
  theme(legend.title=element_blank())+
    coord_cartesian(ylim = c(500, 1600)) +
  scale_x_continuous(breaks=seq(.1,.9, .2))+
  geom_label_repel(data=v, aes(x=q, y=MRT*1000, label=round(MRT*1000,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10) + 
  labs(x = "Quantiles", y = "Response latencies in ms", tag = "A")  +      # Optional tag for labeling
    theme_minimal(base_size=14) 

  

```

```{r}

 v_chb <- v |>
    dplyr::filter(blur=="Clear" | blur=="High blur") |>
    dplyr::group_by(q)|>
     mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> select(-q) |>
   tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
    mutate(diff=`High blur`*1000-Clear*1000)
 
 
p1 <- ggplot(v_chb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme(legend.position = "none") +
    theme_minimal(base_size=14)+
scale_y_continuous(breaks=seq(110,440,50)) +
    coord_cartesian(ylim = c(110, 440)) +
  scale_x_continuous(breaks=seq(600,1300, 100))+
   geom_label_repel(data=v_chb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10)+
  labs( title = "Delta Plots: Clear - High Blur", x = "", y = "Group differences", tag = "B")

```

```{r}


 v_clb <- v |>
    dplyr::filter(blur=="Clear" | blur=="Low blur") |>
    dplyr::group_by(q)|>
     mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> 
   select(-q) |>
   tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
    mutate(diff=`Low blur`*1000-Clear*1000)
 


p2 <- ggplot(v_clb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme_minimal(base_size=14) + 
  theme(legend.position = "none") + 
scale_y_continuous(breaks=seq(-10, 70, 10)) +
    coord_cartesian(ylim = c(-10, 70)) +
  scale_x_continuous(breaks=seq(600,1300, 100))+
    geom_label_repel(data=v_clb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10) + 
  labs( title = "Delta Plots: Low Blur - Clear", x = "Mean RT per quantile", y = "")


```

```{r}

v_hlb <- v |>
  dplyr::filter(blur=="High blur" | blur=="Low blur") |>
  dplyr::group_by(q)|>
  mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> 
   select(-q) |>
  tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
  mutate(diff=`High blur`*1000-`Low blur`*1000)


p3 <- ggplot(v_hlb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme_minimal(base_size = 14) + 
  theme(legend.position = "none") + 
  scale_x_continuous(breaks=seq(600,1350, 200))+
    geom_label_repel(data=v_hlb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10)+ 
  labs( title = "Delta Plots: High Blur - Low Blur", x = "", y = "")

```

```{r}
#| label: fig-deltaquant
#| fig-cap: "Quantile plots for each condition (A) and Delta plots (B) depicting the magnitude of the effect for hypotheses of interest over time in Experiment 1a. Each dot represents the mean RT at the .1, .3, .5, .7 and .9 quantiles."
#| fig-width: 12
#| fig-height: 14

delta_comb = (p1+p2+p3)

combined_plot <- p / delta_comb

combined_plot
 # Automatically labels rows as A, B


ggsave(filename='./figures/figure_kde.png',width=10,height=12)
```

#### RTs: Diffusion Model

```{r}

blur_rt_diff<- rts |>
  group_by(participant) |>
  dplyr::filter(rt >= .2 & rt <= 2.5)|>
  dplyr::filter(lex=="m")

```

```{r}
#| eval: false

formula <- bf(rt | dec(corr) ~ 0 + blur + 
                (1 + blur|p|participant) + (1+blur|i|string),  
              ndt ~ 0 + blur + (1 + blur|p|participant) + (1+blur|i|string),
              bias = .5)



bprior <- prior(normal(0, 1), class = b) +
  prior(normal(0, 1), class = b, dpar = ndt)+
  prior(normal(0, 1), class = sd) +
  prior(normal(0, 1), class = sd, dpar = ndt) + 
  prior("normal(0, 0.3)", class = "sd", group = "participant")+ 
  prior("normal(0, 0.3)", class = "sd", group = "string")


```

```{r}
#| eval: false
#| 

make_stancode(formula, 
              family = wiener(link_bs = "identity", 
                              link_ndt = "identity",
                              link_bias = "identity"),
              data = blur_rt_diff, 
              prior = bprior)

tmp_dat <- make_standata(formula, 
                         family = wiener(link_bs = "identity", 
                              link_ndt = "identity",
                              link_bias = "identity"),
                            data = blur_rt_diff, prior = bprior)
str(tmp_dat, 1, give.attr = FALSE)

initfun <- function() {
  list(
    b = rnorm(tmp_dat$K),
    bs=.5, 
    b_ndt = runif(tmp_dat$K_ndt, 0.1, 0.15),
    sd_1 = runif(tmp_dat$M_1, 0.5, 1),
    sd_2 = runif(tmp_dat$M_2, 0.5, 1),
    z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),
                 tmp_dat$M_1, tmp_dat$N_1),
    z_2 = matrix(rnorm(tmp_dat$M_2*tmp_dat$N_2, 0, 0.01),
                 tmp_dat$M_2, tmp_dat$N_2),
    L_1 = diag(tmp_dat$M_1),
    L_2 = diag(tmp_dat$M_2)
  )
}

```

```{r}
#| eval: false

fit_wiener1 <- brm(formula, 
                  data = blur_rt_diff,
                  family = wiener(link_bs = "identity", 
                                  link_ndt = "identity",
                                  link_bias = "identity"),
                  prior = bprior, init=initfun,
                  iter = 2000, warmup = 500, 
                  chains = 4, cores = 4,
                  file="weiner_diff_1", 
                  backend = "cmdstanr", threads = threading(4), 
                  control = list(max_treedepth = 15))


```

```{r}
#diff object on osf
fit_wiener <- read_rds("https://osf.io/hqauz/download")


```

```{r}
#| label: tbl-diffexpt1a
#| tbl-cap: "Summary of posterior fixed effect estimates for DDM parameter hypotheses in Experiment 1a"

# Compute hypothesis tests
a <- hypothesis(fit_wiener, "blurHB - blurC < 0", dpar="mu")
b <- hypothesis(fit_wiener, "blurHB - blurLB < 0", dpar="mu")
c <- hypothesis(fit_wiener, "blurLB - blurC = 0", dpar="mu")
d <- hypothesis(fit_wiener, "ndt_blurHB - ndt_blurC > 0", dpar="ndt")
e <- hypothesis(fit_wiener, "ndt_blurHB - ndt_blurLB > 0", dpar="ndt")
f <- hypothesis(fit_wiener, "ndt_blurLB - ndt_blurC > 0", dpar="ndt")

# Ensure rounding applies only to numeric columns
a$hypothesis <- a$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
b$hypothesis <- b$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
c$hypothesis <- c$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
d$hypothesis <- d$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
e$hypothesis <- e$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
f$hypothesis <- f$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))

tab <- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis) |> 
    mutate(Evid.Ratio=as.numeric(Evid.Ratio))|>
   rename("Mean" = "Estimate") |>
  select(-Star)

tab[, -1] <- t(apply(tab[, -1], 1, round, digits = 3))

tab |> 
  mutate(Parameter=c("v","v", "v",  "T_er", "T_er", "T_er")) |>
  mutate(Hypothesis = c("High Blur - Clear < 0", "High Blur - Low Blur < 0", "Low Blur - Clear =  0 ", "High Blur - Clear > 0", "High Blur - Low Blur > 0", "Low Blur - Clear =  0"))|> 
   mutate(
        mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
        `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")) |>
    rename("SE" = "Est.Error", "ER" = "Evid.Ratio") |>
    select(Hypothesis, Parameter, Mean, SE, `90% CrI`,ER, Post.Prob)|>  
  flextable() |>
  autofit() |> 
  theme_apa()
```

A summary of the diffusion model results can be found in @tbl-diffexpt1a. High-blurred words had a lower drift rate than clear words, *b* = `r a$hypothesis$Estimate`, 90% Cr.I \[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`, and low-blurred words, *b* = `r b$hypothesis$Estimate`, 90% Cr.I \[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`. There was no difference in drift rate between low-blurred words and clear words, *b* = `r c$hypothesis$Estimate`, 90% Cr.I \[`r c$hypothesis$CI.Lower`, `r c$hypothesis$CI.Upper`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`.

Non-decision time was higher for high-blurred words compared to clear words, *b* = `r d$hypothesis$Estimate`, 90% Cr.I \[`r d$hypothesis$CI.Lower`, `r d$hypothesis$CI.Upper`\], ER = `r process_ER_column(d$hypothesis$Evid.Ratio)`, and low-blurred words, *b* = `r e$hypothesis$Estimate`, 90% Cr.I \[`r e$hypothesis$CI.Lower`, `r e$hypothesis$CI.Upper`\], ER = `r process_ER_column(e$hypothesis$Evid.Ratio)`. Low-blurred words had a higher non-decision time than clear words, *b* = `r f$hypothesis$Estimate`, 90% Cr.I \[`r f$hypothesis$CI.Lower`, `r f$hypothesis$CI.Upper`\], ER = `r process_ER_column(f$hypothesis$Evid.Ratio)`.

#### Recognition Memory

```{r}

mem_c <- read_csv("https://osf.io/xjvc7/download")
```

```{r}
## Contrasts
#hypothesis
blurC <-hypr(HB~C, HB~LB, levels=c("C", "HB", "LB"))

#set contrasts in df 
mem_c$blur <- as.factor(mem_c$blur)

contrasts(mem_c$blur) <-contr.hypothesis(blurC)
```

```{r}
#| eval: false
#| 

prior_exp2 <- c(set_prior("cauchy(0,.35)", class = "b"))

fit_mem_c <- brm(sayold ~ 0 + isold * blur + (1 + isold * blur | participant) + (1 + isold * blur | string),
  data = mem_c,
  warmup = 1000,
  iter = 5000,
  chains = 4,
  init = 0,
  family = bernoulli(link = "probit"),
  cores = 4,
  control = list(adapt_delta = 0.9),
  prior = prior_exp2,
  sample_prior = T,
  file = "blmm_sdt_c_nointercept",
  save_pars = save_pars(all = T),
  backend = "cmdstanr",
  threads = threading(4)
)


```

```{r}

fit_mem1 <- read_rds("https://osf.io/xsvgt/download")


sd_lb <- read_rds("https://osf.io/4qp38/download")

```

```{r}
#HB > C
a <- hypothesis(fit_mem1 , "isold1:blur1 > 0")
b <- hypothesis(fit_mem1 , "isold1:blur2 > 0")
c <- hypothesis(sd_lb , "isold1:blur1 = 0")

a$hypothesis <- a$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
b$hypothesis <- b$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
c$hypothesis <- c$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))

```

##### Sensitivity

@fig-dprimeexp1a highlights High-blurred words were better remembered than clear words, $\beta$ = `r a$hypothesis$Estimate`, 90% Cr.I \[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`, and low-blurred words, $\beta$ = `r b$hypothesis$Estimate`, 90% Cr.I \[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`. There was no difference in sensitivity between clear words and low-blurred words, $\beta$ = `r c$hypothesis$Estimate`, 90% Cr.I \[`r c$hypothesis$CI.Lower`, `r c$hypothesis$CI.Upper`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`.

##### Exploratory

###### Bias

```{r}
a <- hypothesis(fit_mem1 , "blur2 < 0")
b <- hypothesis(sd_lb , "blur1 < 0")
c<-  hypothesis(fit_mem1 , "blur1= 0")


a$hypothesis <- a$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
b$hypothesis <- b$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
c$hypothesis <- c$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))

```

Low blurred words had a bias towards more "old" responses compared to clear words, $\beta$ = `r b$hypothesis$Estimate`, 90% Cr.I\[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`, and high blurred words, $\beta$ = `r a$hypothesis$Estimate`, 90% Cr.I\[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`. There was no difference in bias between high blurred words and clear words, $\beta$ = `r c$hypothesis$Estimate`, 95% Cr.I\[`r c$hypothesis$CI.Lower`, `r c$hypothesis$CI.Upper`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`.

```{r}
# (Negative) criteria
emm_m1_c1 <- emmeans(fit_mem1, ~blur) |> 
  parameters::parameters(centrality = "mean")
# Differences in (negative) criteria
emm_m1_c2 <- emmeans(fit_mem1, ~blur) |> 
  contrast("pairwise") |> 
  parameters::parameters(centrality = "mean")

# Dprimes for three groups
emm_m1_d1 <- emmeans(fit_mem1, ~isold + blur) |> 
  contrast("revpairwise", by = "blur") |> 
  parameters::parameters(centrality = "mean")
# Differences between groups
emm_m1_d2 <- emmeans(fit_mem1, ~isold + blur) |> 
  contrast(interaction = c("revpairwise", "pairwise")) |> 
  parameters::parameters(centrality = "mean")

```

```{r}
#| label: fig-dprimeexp1a
#| fig-cap: Estimated posterior distributions for d-prime and criterion, and differences, with 66% (thick) and 95% CIs (thin lines)
#| fig-width: 12
#| fig-height: 8


emm_m1_c1 <- emmeans(fit_mem1, ~blur) 

  
emm_m1_c2 <- emmeans(fit_mem1, ~blur) |> 
  contrast("pairwise")

# Dprimes for three groups
emm_m1_d1 <- emmeans(fit_mem1, ~isold + blur) |> 
  contrast("revpairwise", by = "blur")
# Differences between groups
emm_m1_d2 <- emmeans(fit_mem1, ~isold + blur) |> 
  contrast(interaction = c("revpairwise", "pairwise")) 


tmp <- bind_rows(
  bind_rows(
    gather_emmeans_draws(emm_m1_d1) |> 
      group_by(blur) |> 
      select(-contrast),
    gather_emmeans_draws(emm_m1_d2) |> 
      rename(
        blur = blur_pairwise
      ) |> 
      group_by(blur) |> 
      select(-isold_revpairwise)
  ),
  bind_rows(
    gather_emmeans_draws(emm_m1_c1),
    gather_emmeans_draws(emm_m1_c2) |> 
      rename(
        blur = contrast
      )
  ),
  .id = "Parameter"
) |> 
  mutate(Parameter = factor(Parameter, labels = c("d-prime", "Criterion"))) |> 
  mutate(
    t = if_else(str_detect(blur, " - "), "Differences", "Group means") |> 
      fct_inorder(),
    blur = fct_inorder(blur)
  )

tmp |>   
  ungroup() |> 
  mutate(.value = if_else(Parameter == "Criterion", .value * -1, .value)) |> 
  mutate(Parameter = fct_rev(Parameter)) |> 
  mutate(blur=case_when(
  blur=="C"~ "Clear", 
  blur=="C - HB" ~ "Clear - High blur", 
  blur=="HB" ~ "High blur", 
  blur=="LB" ~ "Low blur", 
  blur=="C - LB" ~ "Clear - Low blur", 
  TRUE ~ "High blur - Low blur"
  )) |> 
  ggplot(aes(blur, .value)) +
  labs(
    x = "Blurring Level (or difference)",
    y = "Parameter value", 
  ) +
  stat_halfeye(colour="black") + 
    facet_grid(Parameter~t, scales = "free") + 
    geom_hline(yintercept = 0, linewidth = .25) + 
  theme_minimal(base_size=14)

ggsave("expt1a-dprime.png", width=19, height=8, dpi=300)


```

Experiment 1a successfully replicated the pattern of results found in @rosner2015. Specifically, we found high blurred words had lower accuracy than clear and low blurred words, but had better memory.

Adding to this, we utilized cognitive and mathematical modeling to gain further insights into the mechanisms underlying the perceptual disfluency effect. Descriptively, high blurred words induced a more pronounced shift in the RT distribution (Œº) and exhibited a higher degree of skew (œÑ) compared to clear and low blurred words. However, low blurred words did not differ compared to clear words on $\mu$ or $\beta$. These patterns can be clearly seen in our quantile delta plots in Fig. 3.

We also fit the RTs and accuracy data to a diffusion model, which allowed us to make stronger inferences as it relates to stages of processing. High blurred words impacted both an early, non-decision, component evinced by higher $T_{er}$ and a later more analytic, component evinced by a lower $v$ than clear or low blurred words. On the other hand, low blurred words only affected $T_{er}$.

We present evidence that different levels of disfluency can influence distinct stages of encoding, potentially contributing to the presence or absence of a mnemonic effect for perceptually blurred stimuli. Unlike most studies that commonly employ a single level of disfluency, our study incorporated two levels of disfluency. The results indicate that a subtle manipulation such as low blur primarily affects early processing stages, whereas a more pronounced perceptual manipulation (i.e., high blur) impacts both early and late processing stages. Regarding recognition memory, high blurred stimuli were better recognized compared to low blurred and clear words. This suggests that in order to observe a perceptual disfluency effect, the perceptual manipulation must be sufficiently disfluent to do so.

Given the important theoretical implications of these findings, Experiment 1b served as a conceptual replication. Due to the bias observed in the recognition memory test (i.e., low blurred words were responded to more liberally), we will not present old and new items as blurred, instead all of the words will be presented in a clear, different, font at test.

## Experiment 1b

### Method

#### Transparency and Openness

This study was not pregregistered.All raw and summary data, materials, and R scripts for pre-processing, analysis, and plotting for Experiment 1b can be found at https://osf.io/6sy7k/

#### Participants

We used the same sample size as Experiment 1a (*N* = 216). All participants were recruited through the university subject pool at Rutgers University (SONA). We used a similar exclusion criteria to Experiment 1a. Because of this, we oversampled we randomly chose 36 participants from each list to reach our target sample size.

#### Apparatus, stimuli, design, procedure, and analysis

Similar to Experiment 1a, the experiment was run using PsychoPy [@peirce2019] and hosted on Pavlovia (www.pavlovia.org). You can see an example of the experiment by navigating to this website: https://run.pavlovia.org/Jgeller112/ldt_dd_l1_jol.

We used the same stimuli from Experiment 1a. The main difference between Experiment 1a and 1b was all items were presented in a clear, Arial font. To make it more similar to Experiment 1a each set of words presented as clear, low blur, and high blur at study were yoked to a set of new words that were counterbalanced across lists. Therefore, instead of there being one false alarm rate there were 3, one for each blurring level. This ensured each word was compared to studied clear, studied high blurred, and studied low blurred words.

We fit the same statistical models as Experiment 1a.

### Results

#### Accuracy

```{r}

# get data from osf
blur_acc <- read_csv("https://osf.io/excgd/download") |>
    dplyr::filter(lex=="m")


blur_acc_new<- blur_acc |>
  dplyr::filter(rt >= .2 & rt <= 2.5)

```

The analysis of accuracy is is based on `r dim(blur_acc)[1]` data points. After removing fast and slow RTs we were left with `r dim(blur_acc_new)[1]` data point (`r 1-dim(blur_acc_new)[1]/dim(blur_acc)[1]` %)

```{r}

## Contrasts
#hypothesis
blurC <-hypr(HB~C, HB~LB, levels=c("C", "HB", "LB"))

#set contrasts in df 
blur_acc$blur <- as.factor(blur_acc$blur)

contrasts(blur_acc$blur) <-contr.hypothesis(blurC)


```

```{r}
#| eval: false

#weak prior
prior_exp1 <- c(set_prior("cauchy(0,.35)", class = "b"))

#fit model
fit_acc_weak <- brm(corr ~ blur + (1+blur|participant) + (1+blur|string), data=blur_acc_new, 
warmup = 1000,
                    iter = 5000,
                    chains = 4, 
                    init=0, 
                    family = bernoulli(),
     cores = 4,
prior = prior_exp1, 
control = list(adapt_delta = 0.9), 
backend="cmdstanr", 
save_pars = save_pars(all=T),
sample_prior = T, 
threads = threading(4), 
file="fit_acc_weak_nocontext")

```

```{r}

# get file from osf
tmp <- tempdir()
download.file("https://osf.io/ne36z/download", 
              file.path(tmp, "acc_blmm_expnocontext.RData"))
load(file.path(tmp, "acc_blmm_expnocontext.RData"))

fit_acc_lbc <- read_rds("https://osf.io/yhz4c/download")

```

```{r}

acc_means <- emmeans(fit_acc_noc, specs="blur", type="response") |>
  as.data.frame()

```

```{r}
#| label: tbl-acc1b
#| tbl-cap: Summary of posterior fixed effect estimates for accuracy hypotheses in Experiment 1b 
#| 

a <- hypothesis(fit_acc_noc, "blur1 <  0") 

# Ensure rounding applies only to numeric columns
a$hypothesis <- a$hypothesis |>
  mutate(across(where(is.numeric), ~ round(.x, 3)))

b <- hypothesis(fit_acc_noc, "blur2 <  0")

# Ensure rounding applies only to numeric columns
b$hypothesis <- b$hypothesis |>
  mutate(across(where(is.numeric), ~ round(.x, 3)))

c<- hypothesis(fit_acc_lbc, "blur1 =  0")

# Ensure rounding applies only to numeric columns
c$hypothesis <- c$hypothesis |>
  mutate(across(where(is.numeric), ~ round(.x, 3)))

tab <- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis) |>
    mutate(Evid.Ratio=as.numeric(Evid.Ratio))|>
   rename("Mean" = "Estimate") |>
  select(-Star)

tab[, -1] <- t(apply(tab[, -1], 1, round, digits = 3))


acc_exp1b <- tab |> 
    mutate(Hypothesis = c("High Blur - Clear < 0", "High Blur - Low Blur < 0", "Low Blur - Clear =  0 ")) |> 
    mutate(
        mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
        `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")) |>
    rename("SE" = "Est.Error", "ER" = "Evid.Ratio") |>
    select(Hypothesis, Mean, SE, `90% CrI`,ER, Post.Prob)|> 
  flextable() |>
  autofit() |> 
  theme_apa()

acc_exp1b


```

A summary of posterior estimates are located in @tbl-acc1b. Clear words were better identified ($M$ = .987) compared to high blur words ($M$ = .962), $b$ = `r a$hypothesis$Estimate`, 95% Cr.I\[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r a$hypothesis$Evid.Ratio`. Low blurred words were better identified ($M$ = .987) than high blurred words, $b$ = `r b$hypothesis$Estimate`, 95% Cr.I\[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r b$hypothesis$Evid.Ratio`. However, the evidence was weak for there being no significant difference in the identification accuracy between clear and low blurred words, b = `r c$hypothesis$Estimate`, 95% Cr.I\[`r c$hypothesis$CI.Lower`, `r c$hypothesis$CI.Upper`\], ER = `r c$hypothesis$Evid.Ratio`.

#### RTs: Ex-Gaussian

```{r}
#load data from osf
rts <- read_csv("https://osf.io/excgd/download")
```

```{r}

blur_rt<- rts |>
  group_by(participant) |>
   dplyr::filter(corr==1, lex=="m")#only include nonwords

blur_rt_new <- blur_rt |> 
  dplyr::filter(rt >= .2 & rt <= 2.5) |>
  mutate(rt_ms=rt*1000)


```

The analysis of RTs (correct trials and word stimuli) is based on `r dim(blur_rt_new)[1]` data points, after removing fast and slow RTs (`r 1-dim(blur_rt_new)[1]/dim(blur_rt)[1]` %)

```{r}
#hypothesis
blurC <-hypr(HB~C, HB~LB, levels=c("C", "HB", "LB"))

#set contrasts in df 
blur_rt$blur <- as.factor(blur_rt$blur)

contrasts(blur_rt$blur) <-contr.hypothesis(blurC)

```

```{r}
#| eval: false

bform_exg1 <- bf(
rt ~ 0+ blur + (1 + blur |p| participant) + (1 + blur|i| string),
sigma ~ 0+ blur + (1 + blur |p|participant) + (1 + blur |i| string),
beta ~ 0 + blur + (1 + blur |p|participant) + (1 + blur |i| string))
```

```{r}
#| eval: false

prior_exp1 <- c(set_prior("normal(0,10)", class = "b"), 
                 set_prior("normal(0,10)", class = "b", dpar="sigma"), 
                 set_prior("normal(0,10)", class = "b", dpar="beta"))
                

fit_exg1 <- brm(
bform_exg1, data = blur_rt,
warmup = 1000,
                    iter = 5000,
                    chains = 4,
                    prior = prior_exp1,
                    family = exgaussian(),
                    init = 0,
                    cores = 4, 
sample_prior = T, 
save_pars = save_pars(all=T),
control = list(adapt_delta = 0.8), 
backend="cmdstanr", 
threads = threading(4))


```

```{r}
#load rdata for model 
#load_github_data("https://osf.io/uxc2f/download")


fit_exg1 <- read_rds("https://osf.io/egqyt/download")


```

```{r}
#| label: tbl-exgauss1b
#| tbl-cap: "Summary of posterior fixed effect estimates for Ex-Guassian parmater hypotheses in Experiment 1b"

# Compute hypothesis tests
a <- hypothesis(fit_exg1, "blurHB - blurC > 0", dpar="mu") 
b <- hypothesis(fit_exg1, "blurHB - blurLB > 0", dpar="mu")
c <- hypothesis(fit_exg1, "blurLB - blurC > 0", dpar="mu")
d <- hypothesis(fit_exg1, "sigma_blurHB - sigma_blurC > 0", dpar="sigma")
e <- hypothesis(fit_exg1, "sigma_blurHB - sigma_blurLB > 0", dpar="sigma")
f <- hypothesis(fit_exg1, "sigma_blurLB - sigma_blurC = 0", dpar="sigma")
g <- hypothesis(fit_exg1, "beta_blurHB - beta_blurC > 0", dpar="beta")
h <- hypothesis(fit_exg1, "beta_blurHB - beta_blurLB > 0", dpar="beta")
i <- hypothesis(fit_exg1, "beta_blurLB - beta_blurC = 0", dpar="beta")

# Ensure rounding applies only to numeric columns
a$hypothesis <- a$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
b$hypothesis <- b$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
c$hypothesis <- c$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
d$hypothesis <- d$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
e$hypothesis <- e$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
f$hypothesis <- f$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
g$hypothesis <- g$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
h$hypothesis <- h$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
i$hypothesis <- i$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))

tab <- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis, g$hypothesis, h$hypothesis, i$hypothesis) |> 
    mutate(Evid.Ratio=as.numeric(Evid.Ratio))|>
   rename("Mean" = "Estimate") |>
  select(-Star)

tab[, -1] <- t(apply(tab[, -1], 1, round, digits = 3))

tab |> 
  mutate(Parameter=c("mu","mu", "mu",  "sigma", "sigma", "sigma", "beta", "beta", "beta"))|>
  mutate(Hypothesis = c("High Blur - Clear > 0", "High Blur - Low Blur > 0", "Low Blur - Clear >  0 ", "High Blur - Clear > 0", "High Blur - Low Blur > 0", "Low Blur - Clear =  0","High Blur - Clear > 0", "High Blur - Low Blur > 0", "Low Blur - Clear = 0  "))|> 
  mutate(
        mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
        `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")) |>
    rename("SE" = "Est.Error", "ER" = "Evid.Ratio") |>
    select(Hypothesis, Parameter, Mean, SE, `90% CrI`,ER, Post.Prob)|> 
   flextable() |>
  autofit() |>
  theme_apa()

```

A visualization of how blurring affected processing during word recognition can be seen in the quantile and delta plots in @fig-deltaquant1b. Beginning with the Œº parameter, there was greater shifting for high blurred words compared to clear words, *b* = `r round(a$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(a$hypothesis$CI.Lower, 3)`, `r round(a$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`, and low blur words, *b* = `r round(b$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(b$hypothesis$CI.Lower, 3)`, `r round(b$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`.

Analyses of the œÉ and œÑ parameters yielded a similar pattern. Variance was higher for high blurred words compared to clear words, *b* = `r round(d$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(d$hypothesis$CI.Lower, 3)`, `r round(d$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(d$hypothesis$Evid.Ratio)`, and low blurred words, *b* = `r round(e$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(e$hypothesis$CI.Lower, 3)`, `r round(e$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(e$hypothesis$Evid.Ratio)`.

Finally, there was greater skewing for high blurred words compared to clear words, *b* = `r round(g$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(g$hypothesis$CI.Lower, 3)`, `r round(g$hypothesis$CI.Upper, 3)`\], ER = `r round(g$hypothesis$Evid.Ratio, 3)`, and low blurred words, *b* = `r round(h$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(h$hypothesis$CI.Lower, 3)`, `r round(h$hypothesis$CI.Upper, 3)`\], ER = `r round(h$hypothesis$Evid.Ratio, 3)`.

Low blurred words compared to clear words only differed on the Œº parameter, *b* = `r round(c$hypothesis$Estimate, 3)`, 90% Cr.I \[`r round(c$hypothesis$CI.Lower, 3)`, `r round(c$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`, with greater shifting for low blurred words. For $\tau$ and $\sigma$, the 95% Cr.I crossed zero and the ER was greater than 100.

#### DDM

```{r}

blur_rt_diff<- rts |>
  group_by(participant) |>
  dplyr::filter(rt >= .2 & rt <= 2.5)|>
  dplyr::filter(lex=="m")

```

```{r}
formula <- bf(rt | dec(corr) ~ 0 + blur + 
                (1 + blur|p|participant) + (1+blur|i|string),  
              ndt ~ 0 + blur + (1 + blur|p|participant) + (1+blur|i|string),
              bias =.5)

bprior <- prior(normal(0, 1), class = b) +
  prior(normal(0, 1), class = b, dpar = ndt)+
  prior(normal(0, 1), class = sd) +
  prior(normal(0, 1), class = sd, dpar = ndt) + 
  prior("normal(0, 0.3)", class = "sd", group = "participant")+ 
  prior("normal(0, 0.3)", class = "sd", group = "string")


```

```{r}
#| eval: false

make_stancode(formula, 
              family = wiener(link_bs = "identity", 
                              link_ndt = "identity",
                              link_bias = "identity"),
              data = blur_rt_diff, 
              prior = bprior)

tmp_dat <- make_standata(formula, 
                         family = wiener(link_bs = "identity", 
                              link_ndt = "identity",
                              link_bias = "identity"),
                            data = blur_rt_diff, prior = bprior)
str(tmp_dat, 1, give.attr = FALSE)

initfun <- function() {
  list(
    b = rnorm(tmp_dat$K),
    bs=.5, 
    b_ndt = runif(tmp_dat$K_ndt, 0.1, 0.15),
    sd_1 = runif(tmp_dat$M_1, 0.5, 1),
    sd_2 = runif(tmp_dat$M_2, 0.5, 1),
    z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),
                 tmp_dat$M_1, tmp_dat$N_1),
    z_2 = matrix(rnorm(tmp_dat$M_2*tmp_dat$N_2, 0, 0.01),
                 tmp_dat$M_2, tmp_dat$N_2),
    L_1 = diag(tmp_dat$M_1),
    L_2 = diag(tmp_dat$M_2)
  )
}

```

```{r}
#| eval: false
#| 

fit_wiener1 <- brm(formula, 
                  data = blur_rt_diff,
                  family = wiener(link_bs = "identity", 
                                  link_ndt = "identity",
                                  link_bias = "identity"),
                  prior = bprior, init=initfun,
                  iter = 2000, warmup = 500, 
                  chains = 4, cores = 4,
                  file="weiner_diff_1", 
                  backend = "cmdstanr", threads = threading(4), 
                  control = list(max_treedepth = 15))


```

```{r}

fit_wiener <- read_rds("https://osf.io/3j98t/download")

```

```{r}
#| label: tbl-diffexpt1b
#| tbl-cap: "Summary of posterior fixed effect estimates for DDM parameter hypotheses in Experiment 1b"

# Compute hypothesis tests
a <- hypothesis(fit_wiener, "blurHB - blurC < 0", dpar="mu")
b <- hypothesis(fit_wiener, "blurHB - blurLB < 0", dpar="mu")
c <- hypothesis(fit_wiener, "blurLB - blurC = 0", dpar="mu")
d <- hypothesis(fit_wiener, "ndt_blurHB - ndt_blurC > 0", dpar="ndt")
e <- hypothesis(fit_wiener, "ndt_blurHB - ndt_blurLB > 0", dpar="ndt")
f <- hypothesis(fit_wiener, "ndt_blurLB - ndt_blurC > 0", dpar="ndt")

# Ensure rounding applies only to numeric columns
a$hypothesis <- a$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
b$hypothesis <- b$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
c$hypothesis <- c$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
d$hypothesis <- d$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
e$hypothesis <- e$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
f$hypothesis <- f$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))

tab <- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis) |> 
    mutate(Evid.Ratio=as.numeric(Evid.Ratio))|>
   rename("Mean" = "Estimate") |>
  select(-Star)

tab[, -1] <- t(apply(tab[, -1], 1, round, digits = 3))

tab |> 
  mutate(Parameter=c("v","v", "v",  "T_er", "T_er", "T_er")) |>
  mutate(Hypothesis = c("High Blur - Clear > 0", "High Blur - Low Blur > 0", "Low Blur - Clear =  0 ", "High Blur - Clear > 0", "High Blur - Low Blur > 0", "Low Blur - Clear =  0"))|> 
   mutate(
        mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
        `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")) |>
    rename("SE" = "Est.Error", "ER" = "Evid.Ratio") |>
    select(Hypothesis, Parameter, Mean, SE, `90% CrI`,ER, Post.Prob)|>  
  flextable() |>
  autofit() |> 
  theme_apa()

```

A summary of the diffusion model results can be found in @tbl-diffexpt1b. High-blurred words had a lower drift rate than clear words, *b* = `r a$hypothesis$Estimate`, 90% Cr.I \[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`, and low-blurred words, *b* = `r b$hypothesis$Estimate`, 90% Cr.I \[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`. There was no difference in drift rate between low-blurred words and clear words, *b* = `r c$hypothesis$Estimate`, 90% Cr.I \[`r c$hypothesis$CI.Lower`, `r c$hypothesis$CI.Upper`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`.

Non-decision time was higher for high-blurred words compared to clear words, *b* = `r d$hypothesis$Estimate`, 90% Cr.I \[`r d$hypothesis$CI.Lower`, `r d$hypothesis$CI.Upper`\], ER = `r process_ER_column(d$hypothesis$Evid.Ratio)`, and low-blurred words, *b* = `r e$hypothesis$Estimate`, 90% Cr.I \[`r e$hypothesis$CI.Lower`, `r e$hypothesis$CI.Upper`\], ER = `r process_ER_column(e$hypothesis$Evid.Ratio)`. Low-blurred words had a higher non-decision time than clear words, *b* = `r f$hypothesis$Estimate`, 90% Cr.I \[`r f$hypothesis$CI.Lower`, `r f$hypothesis$CI.Upper`\], ER = `r process_ER_column(f$hypothesis$Evid.Ratio)`.

```{r}

#Delta plots (one per subject) 
quibble <- function(x, q = seq(.1, .9, .2)) {
  tibble(x = quantile(x, q), q = q)
}

data.quantiles <- rts |>
  dplyr::filter(rt >= .2 | rt <= 2.5) |> 
  dplyr::group_by(participant,blur,corr) |>
  dplyr::filter(lex=="m")|>
  dplyr::summarise(RT = list(quibble(rt, seq(.1, .9, .2)))) |> 
  tidyr::unnest(RT)


data.delta <- data.quantiles |>
  dplyr::filter(corr==1) |>
  dplyr::select(-corr) |>
  dplyr::group_by(participant, blur, q) |>
  dplyr::summarize(RT=mean(x))


#Delta plots (based on vincentiles)
vincentiles <- data.quantiles |>
  dplyr::filter(corr==1) |>
  dplyr::select(-corr) |>
  dplyr::group_by(blur,q) |>
  dplyr::summarize(RT=mean(x)) 

v=vincentiles |>
  dplyr::group_by(blur,q) |>
  dplyr::summarise(MRT=mean(RT))

v <- v |>
  mutate(blur=ifelse(blur=="HB", "High blur", ifelse(blur=="LB", "Low blur", "Clear")))


v$blur<- factor(v$blur, level=c("High blur", "Low blur", "Clear"))



p <- ggplot(v, aes(x = q, y = MRT*1000, colour = blur, group=blur))+
  geom_line(size = 1) +
  geom_point(size = 3) +
  scale_y_continuous(breaks=seq(500,1600,100)) +
  theme(legend.title=element_blank())+
    coord_cartesian(ylim = c(500, 1600)) +
  scale_x_continuous(breaks=seq(.1,.9, .2))+
  geom_label_repel(data=v, aes(x=q, y=MRT*1000, label=round(MRT*1000,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10) + 
  labs(x = "Quantiles", y = "Response latencies in ms", tag = "A")  +      # Optional tag for labeling
    theme_minimal(base_size=14) 

  

```

```{r}

 v_chb <- v |>
    dplyr::filter(blur=="Clear" | blur=="High blur") |>
    dplyr::group_by(q)|>
     mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> select(-q) |>
   tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
    mutate(diff=`High blur`*1000-Clear*1000)
 
 
p1 <- ggplot(v_chb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme(legend.position = "none") +
    theme_minimal(base_size=14)+
scale_y_continuous(breaks=seq(110,440,50)) +
    coord_cartesian(ylim = c(110, 440)) +
  scale_x_continuous(breaks=seq(600,1300, 100))+
   geom_label_repel(data=v_chb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10)+
  labs( title = "Delta Plots: Clear - High Blur", x = "", y = "Group differences", tag = "B")

```

```{r}


 v_clb <- v |>
    dplyr::filter(blur=="Clear" | blur=="Low blur") |>
    dplyr::group_by(q)|>
     mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> 
   select(-q) |>
   tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
    mutate(diff=`Low blur`*1000-Clear*1000)
 


p2 <- ggplot(v_clb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme_minimal(base_size=14) + 
  theme(legend.position = "none") + 
scale_y_continuous(breaks=seq(-10, 70, 10)) +
    coord_cartesian(ylim = c(-10, 70)) +
  scale_x_continuous(breaks=seq(600,1300, 100))+
    geom_label_repel(data=v_clb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10) + 
  labs( title = "Delta Plots: Low Blur - Clear", x = "Mean RT per quantile", y = "")


```

```{r}

v_hlb <- v |>
  dplyr::filter(blur=="High blur" | blur=="Low blur") |>
  dplyr::group_by(q)|>
  mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> 
   select(-q) |>
  tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
  mutate(diff=`High blur`*1000-`Low blur`*1000)


p3 <- ggplot(v_hlb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme_minimal(base_size = 14) + 
  theme(legend.position = "none") + 
  scale_x_continuous(breaks=seq(600,1350, 200))+
    geom_label_repel(data=v_hlb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10)+ 
  labs( title = "Delta Plots: High Blur - Low Blur", x = "", y = "")

```

```{r}
#| label: fig-deltaquant1b
#| fig-cap: "Quantile plots for each condition (A) and Delta plots (B) depicting the magnitude of the effect for hypotheses of interest over time in Experiment 1b. Each dot represents the mean RT at the .1, .3, .5, .7 and .9 quantiles."
#| fig-width: 12
#| fig-height: 14

delta_comb = (p1+p2+p3)

combined_plot <- p / delta_comb

combined_plot
 # Automatically labels rows as A, B


ggsave(filename='./figures/figure_kde2.png',width=12,height=12)
```

#### Recognition Memory

```{r}

mem_nc <- read_csv("https://osf.io/jw2gx/download")

```

```{r}
## Contrasts
#hypothesis
blurC <-hypr(HB~C, HB~LB, levels=c("C", "HB", "LB"))

#set contrasts in df 
mem_c$blur <- as.factor(mem_c$blur)

contrasts(mem_c$blur) <-contr.hypothesis(blurC)
```

```{r}
#| eval: false
#| 

prior_exp2 <- c(set_prior("cauchy(0,.35)", class = "b"))

fit_mem_nc <- brm(sayold ~ isold*blur + (1+isold*blur|participant) + (1+isold*blur|string), data=mem_nc, 
warmup = 1000,
                    iter = 5000,
                    chains = 4, 
                    init=0, 
                    family = bernoulli(link = "probit"),
                    cores = 4, 
control = list(adapt_delta = 0.9),
prior=prior_exp2, 
sample_prior = T, 
file="blmm_sdt_c_nointercept",
save_pars = save_pars(all=T),
backend="cmdstanr",

threads = threading(4))


```

```{r}

fit_mem_noc <- read_rds("https://osf.io/2pgnm/download")

#get the lowblur vs. c conrtast
fit_mem_lbc <- read_rds("https://osf.io/tucn9/download")


```

```{r}
#HB > C
a <- hypothesis(fit_mem_noc , "isold1:blur1 > 0")
b <- hypothesis(fit_mem_noc , "isold1:blur2 > 0")
c <- hypothesis(fit_mem_lbc , "isold1:blur1 = 0")

a$hypothesis <- a$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
b$hypothesis <- b$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
c$hypothesis <- c$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))

```

##### Sensitivity

@fig-dprimeexp1b highlights High-blurred words were better remembered than clear words, $\beta$ = `r a$hypothesis$Estimate`, 90% Cr.I \[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`, and low-blurred words, $\beta$ = `r b$hypothesis$Estimate`, 90% Cr.I \[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`. There was no difference in sensitivity between clear words and low-blurred words, $\beta$ = `r c$hypothesis$Estimate`, 90% Cr.I \[`r c$hypothesis$CI.Lower`, `r c$hypothesis$CI.Upper`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`.

```{r}
# (Negative) criteria
emm_m1_c1 <- emmeans(fit_mem_noc, ~blur) |> 
  parameters::parameters(centrality = "mean")
# Differences in (negative) criteria
emm_m1_c2 <- emmeans(fit_mem_noc, ~blur) |> 
  contrast("pairwise") |> 
  parameters::parameters(centrality = "mean")

# Dprimes for three groups
emm_m1_d1 <- emmeans(fit_mem_noc, ~isold + blur) |> 
  contrast("revpairwise", by = "blur") |> 
  parameters::parameters(centrality = "mean")
# Differences between groups
emm_m1_d2 <- emmeans(fit_mem_noc, ~isold + blur) |> 
  contrast(interaction = c("revpairwise", "pairwise")) |> 
  parameters::parameters(centrality = "mean")

```

```{r}
#| label: fig-dprimeexp1b
#| fig-cap: Estimated posterior distributions for d-prime and criterion, and differences, with 66% (thick) and 95% CIs (thin lines)
#| fig-width: 12
#| fig-height: 8


emm_m1_c1 <- emmeans(fit_mem_noc, ~blur) 

  
emm_m1_c2 <- emmeans(fit_mem_noc, ~blur) |> 
  contrast("pairwise")

# Dprimes for three groups
emm_m1_d1 <- emmeans(fit_mem_noc, ~isold + blur) |> 
  contrast("revpairwise", by = "blur")
# Differences between groups
emm_m1_d2 <- emmeans(fit_mem_noc, ~isold + blur) |> 
  contrast(interaction = c("revpairwise", "pairwise")) 


tmp <- bind_rows(
  bind_rows(
    gather_emmeans_draws(emm_m1_d1) |> 
      group_by(blur) |> 
      select(-contrast),
    gather_emmeans_draws(emm_m1_d2) |> 
      rename(
        blur = blur_pairwise
      ) |> 
      group_by(blur) |> 
      select(-isold_revpairwise)
  ),
  bind_rows(
    gather_emmeans_draws(emm_m1_c1),
    gather_emmeans_draws(emm_m1_c2) |> 
      rename(
        blur = contrast
      )
  ),
  .id = "Parameter"
) |> 
  mutate(Parameter = factor(Parameter, labels = c("d-prime", "Criterion"))) |> 
  mutate(
    t = if_else(str_detect(blur, " - "), "Differences", "Group means") |> 
      fct_inorder(),
    blur = fct_inorder(blur)
  )

tmp |>   
  ungroup() |> 
  mutate(.value = if_else(Parameter == "Criterion", .value * -1, .value)) |> 
  mutate(Parameter = fct_rev(Parameter)) |> 
  mutate(blur=case_when(
  blur=="C"~ "Clear", 
  blur=="C - HB" ~ "Clear - High blur", 
  blur=="HB" ~ "High blur", 
  blur=="LB" ~ "Low blur", 
  blur=="C - LB" ~ "Clear - Low blur", 
  TRUE ~ "High blur - Low blur"
  )) |> 
  ggplot(aes(blur, .value)) +
  labs(
    x = "Blurring Level (or difference)",
    y = "Parameter value", 
  ) +
  stat_halfeye(colour="black", .width = c(0.66, 0.90)) + 
    facet_grid(Parameter~t, scales = "free") + 
    geom_hline(yintercept = 0, linewidth = .25) + 
  theme_minimal(base_size=14)

ggsave("expt1b-dprime.png", width=19, height=8, dpi=300)

```

## Experiment 2

In Experiments 1a and 1b, we employed mathematical and computational techniques to study the impact of blurring on encoding and recognition memory. High blurred words influenced both early and late stages evident by increased distributional shifting and skewing, lower $v$, and higher $T_{er}$. Low blurred words (compared to clear words), on the other hand, only impacted an early phase, indicated by increased distributional shifting and higher $T_{er}$. In terms of recognition memory, sensitivity was higher for high blurred words than clear and low blurred words. This implies two facets to the disfluency effect: an early, automatic/non-analytic component, and a subsequent, analytic component. The locus of this later component remains ambiguous. The mnemonic benefit for recognizing high blurred words might arise from enhanced top-down (lexical or semantic processing) processing which offsets the challenge of reading blurred text. Alternatively, the benefit might stem from increased attention or control processes operating along side processes needed to recognize the word.

One way to test more directly the accounts of perceptual disfluency would be to identify an aspect of higher level information that plays a role in word perception and to examine its impact on the disfluency effect. Several models of word recognition assume the speed and ease of word identification varies as a function of word frequency [@coltheart2001; @mcclelland1981]. Looking at RT distributions for word frequency effects has shown both an early and late locus, showing larger distribution shifts and more skewing for low frequency words [@andrews2001; @balota1999; @plourde1997; @staub2010; @yap2007; but see @gomez2014 for a DDM account]. In regards to memory, low frequency words are generally better recognized than high frequency word in recognition memory[@glanzer1985]. The recognition advantage for less frequent words has been ascribed to the additional cognitive effort or attenton required to process them [@diana2006; but see @pazzaglia2014]. This has been called the elevated attention hypothesis [@malmberg2003].

In tasks like semantic categorization and pronunciation, the interaction between word frequency and stimulus degradation (in this case, perceptual disfluency) leads to an over-additive effect [@yap2007]. By the logic of additive factors [@sternberg1969], if factors do interact, they are believed to be associated with similar processing stages. The interplay between perceptual disfluency and word frequency originates from perceptual disfluency hindering initial processing and word identification. This leads to a magnification of the word frequency effect. Perceptual disfluencies like handwritten cursive [@barnhart2010; @perea2016] and research on letter rotation in words [@fern√°ndez-l√≥pez2022] have shown a magnification of the word frequency effect.

In Experiment 2, we manipulated word frequency (high vs. low frequency words) and perceptual blur (i.e., clear, low, and high) within a semantic categorization task. Mirroring Experiments 1a and 1b, the categorization task preceded a surprise memory recognition test. Our goal here was to evaluate the compensatory processing and stage-specific theories as both theories offer predictions about memory performance.

In Experiment 2, we opted to forgo using the DDM. Instead we focus on ex-Gaussian parameters during encoding. Both the compensatory processing and stage specific accounts predict an interaction of word frequency and blurring on $\mu$ and $\tau$ where the word frequency effect is largest for high blurred words. Where both accounts differ is in terms of memory performance.

The compensatory processing account predicts items receiving the most top-down processing during encoding should be better remembered. On a recognition test, this account would predict items low in frequency should show a disfluency effect due to low frequency items receiving more top-down processing during encoding.

The stage specific account, on the other hand, is influenced by extra attentional or control processes taking place during and after word recognition. Here, memory performance relies not only on the type of processing during encoding but also on limited-capacity resources such as cognitive control.

Low frequency words, like high blurred words, routinely attract attention during encoding [see evidence from pupillometry; @kuchinke2007]. Thus the benefits of perceptual disfluency may be less effective for these items. High frequency items on the other hand should be more likely to benefit from a manipulation that enhances attention to, and encoding of, the item. The presence of low frequency words could make perceptual disfluency redundant in this regard. In this regard, we might not see a perceptual disfluency effect for low frequency items. @ptok2019 argued that the memory benefits for conflict encoding phenomena are limited to tasks that are relatively fluent, automatic, and encoding poor. Any additional demands placed on the participants imposed by the task could reduce the disfluency effect. As evidence for this, @ptok2020 showed manipulating endogenous attention (by using a chinrest) eliminated the memory benefit from semantic interference during encoding. Similarly, @geller2021 manipulated attention through a test expectancy manipulation (i.e., being told about an upcoming memory test or not) which presumably oriented participants to study all words for the upcoming memory test, regardless of disfluency. This resulted in the elimination of the disfluency effect. Lastly, @westerman1997 (Experiment 3), showed, with a masking manipulation, that changing encoding instructions from reading the target word (more automatic) to spelling the target word eliminated the perceptual disfluency effect. In Experiment 2, we examine how word frequency interacts with perceptual disfluency and how this affects memory.

### Method

### Transparenency and Openness

This study was preregistered https://osf.io/kjq3t. All raw and summary data, materials, and R scripts for pre-processing, analysis, and plotting for Experiment 2 can be found at our OSF page: https://osf.io/6sy7k/.

### Participants

We preregistered a sample size of 432, which is twice the size of Experiments 1a and 1b. This sample size was chosen based on the interaction effect we aimed to test. All participants were recruited through SONA and Prolific. Participants recruited via Prolific were compensated \$12 per hour. On Prolific, we applied built-in filters to include only monolingual, native English-speaking Americans currently residing in the United States, with normal or corrected-to-normal vision. Participants recruited via SONA were given course credit for their participation.

### Materials

Non-animal and animal words were adapted from @fern√°ndez-l√≥pez2022. To make the experiment more feasible for online participants and to evenly split our conditions, we winnowed their non-animal words and presented 90 (1/2 HF and 1/2 LF) non-animal words and 45 animal words during study. This kept the 2:1 ratio used in previous experiments (e.g. [@fern√°ndez-l√≥pez2022; @perea2018]. At test, 90 non-animal words we did not use during the semantic categorization task were used as new words for the recognition test. We created six counterbalanced lists to ensure that each non-animal word was presented as both old and new and as clear, high blurred, and low blurred across participants. Similar to non-words from Experiments 1a and 1b, we excluded animal words from analysis.

The number of letters of the animal words (M = 5.3; range: 3-9) was similar to that of the non-animal words (high-frequency words: M = 5.3, range: 3-8; low-frequency words: M = 5.3, range: 3-9). The animal words had an ample range of word-frequency in the SUBTLEX database (M = 11.84 per million; range: 0.61-192.84).

### Procedure

We used the same procedure as Experiments 1b. The main difference is that instead of making a word/non-word decision, participants made a semantic categorization judgement (i.e., animal/not animal). You can view the task here: https://run.pavlovia.org/Jgeller112/hf_lf_sem_1.


## References

::: {#refs}
:::
