---
title: "Perceptual Disfluency and Recognition Memory: A Response Time Distributional Analysis"
# If blank, the running header is the title in 
shorttitle: "Modeling Perceptual Disfluency"
# Set names and affiliations.
# It is nice to specify everyone's orcid, if possible.
# There can be only one corresponding author, but declaring one is optional.
author:
  - name: Jason Geller
    corresponding: true
    orcid: 0000-0002-7459-4505
    email: jason.geller@bc.edu
    url: www.drjasongeller.com
    # Roles are optional. 
    # Select from the CRediT: Contributor Roles Taxonomy https://credit.niso.org/
    # conceptualization, data curation, formal Analysis, funding acquisition, investigation, 
    # methodology, project administration, resources, software, supervision, validation, 
    # visualization, writing, editing
    roles:
      - conceptualization
      - writing
      - data curation
      - editing
      - software
      - formal analysis
    affiliations:
      - id: id1
        name: "Boston College"
        department: Department of Psychology and Neuroscience
        address: Mcguinn Hall 405
        city: Chestnut Hill
        region: MA
        country: USA
        postal-code: 02467-9991

  - name: Pablo Gomez
    orcid: 0000-0003-4180-1560
    roles:
      - conceptualization
      - editing
      - formal analysis
    affiliations: 
      - id: id2
        name: "Skidmore College"
        department: Department of Psychology
  - name: Erin Buchanon
    orcid: 0000-0002-9689-4189
    roles: 
      - editing
      - validation
      - formal analysis
    affiliations:
     - id: id3
       name: "Harrisburg University of Science and Technology"
       department: Department of Cognitive Analytics (Analytics)
  - name: Dominique Makowski 
    orcid: 0000-0001-5375-9967
    roles: 
      - editing
      - validation
      - formal analysis
    affiliations:
     - id: id4
       name: "University of Sussex"
       department: School of Psychology
author-note:
  status-changes: 
    affiliation-change: ~
    deceased: ~
  disclosures:
    # Example: This study was registered at X (Identifier Y).
    study-registration: "Experiment 1A and 2 were preregisted: https://osf.io/q3fjn; https://osf.io/kjq3t."
    # Acknowledge and cite data/materials to be shared.
    data-sharing: Data, code, and materials for this manuscript can be found at  https://osf.io/6sy7k/. 
    related-report: ~
    conflict-of-interest: The authors have no conflicts of interest to disclose.
    financial-support: This work was supported by research start-up funds to JG.
    authorship-agreements: ~
abstract: "Perceptual disfluency, induced by blurring or difficult-to-read typefaces, can sometimes enhance memory retention, but the underlying mechanisms remain unclear. To investigate this, we manipulated blurring levels (clear, low blur, high blur) during encoding and assessed recognition performance in a surprise memory test. In Experiments 1A and 1B, response latencies from a lexical decision task were analyzed using ex-Gaussian distribution modeling and drift diffusion modeling. Results showed that blurring differentially influenced parameters oe the model, with high blur affecting both early and late-stage processes, while low blur primarily influenced early-stage processes. Recognition test results further revealed that high blur words were remembered better than both clear and low blurred words.Experiment 2 employed a semantic categorization task with a word frequency manipulation to further examine the locus of the perceptual disfluency effect. Similar to Experiments 1A and 1B, high blur influenced both early and late-stage processes, while low blur primarily affected early-stage processes. Low-frequency words exhibited greater shifting and skewing in distributional parameters, yet only high-frequency, highly blurred words demonstrated an enhanced memory effect. These findings suggest that both early and late cognitive processes contribute to the mnemonic benefits associated with perceptual disfluency.Overall, this study demonstrates that distributional and computational analyses provide powerful tools for dissecting encoding mechanisms and their effects on memory, offering valuable insights into models of perceptual disfluency."

keywords: [disfluency, LDT, DDM, ex-Gaussian, distributional analyses, word recognition]
floatsintext: true
numbered-lines: true
bibliography: references.bib
suppress-title-page: false
link-citations: false
mask: false
masked-citations:
draft-date: false
lang: en
language: 
  citation-last-author-separator: "and"
  citation-masked-author: "Masked Citation"
  citation-masked-date: "n.d."
  citation-masked-title: "Masked Title"
  email: "drjasongeller@gmail.com"
  title-block-author-note: "Author Note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows:"
format:
  apaquarto-typst: default
  
execute: 
  echo: false
  warning: false
  message: false
  fig-align: center
  tbl-align: center
  keep-with-next: true
  code-overflow: wrap
  cache: true
  out-width: 50%
---

```{r}

required_packages <- c(
  "brms",
  "cmdstanr",
  "colorspace",
  "cowplot",
  "data.table",
  "easystats",
  "emmeans",
  "flextable",
  "ggeffects",
  "ggdist",
  "ggrepel",
  "ggtext",
  "hypr",
  "knitr",
  "officer",
  "patchwork",
  "ragg",
  "tidybayes",
  "tidylog",
  "tinytable", 
  "tidyverse"
)

install_missing <- setdiff(required_packages, rownames(installed.packages()))
if (length(install_missing) > 0) install.packages(install_missing)
```

```{r}
#| label: load packages

library(easystats)
library(tidyverse)
library(patchwork)
library(knitr)
library(data.table)
library(ggrepel)
library(brms)
library(ggdist)
library(emmeans)
library(tidylog)
library(tidybayes)
library(hypr)
library(colorspace)
library(ragg)
library(cowplot)
library(ggtext)
library(ggdist)
library(flextable)
library(officer)
library(cmdstanr)
library(tinytable)


options(timeout = 5000)
```

```{r}

# inf does not display correctly

# Check if ER column value is numeric, if not, replace with Inf
process_ER_column <- function(x) ifelse(is.infinite(x), "Inf", x)
```

```{r}
#| label: ggplot theme
#| echo: false


# Set up your theme
bold <- element_text(face = "bold", color = "black", size = 16) #axis bold
theme_set(theme_minimal(base_size = 16, base_family = "Times New Roman"))
theme_update(
  panel.grid.major = element_line(color = "grey92", linewidth = .4),
  panel.grid.minor = element_blank(),
  axis.title.x = element_text(color = "grey30", margin = margin(t = 7)),
  axis.title.y = element_text(color = "grey30", margin = margin(r = 7)),
  axis.text = element_text(color = "grey50"),
  axis.ticks =  element_line(color = "grey92", linewidth = .4),
  axis.ticks.length = unit(.6, "lines"),
  legend.position = "top",
  plot.title = element_text(hjust = 0, color = "black", 
                            family = "Times New Roman",
                            size = 21, margin = margin(t = 10, b = 35)),
  plot.subtitle = element_text(hjust = 0, face = "bold", color = "grey30",
                               family = "Arial", 
                               size = 14, margin = margin(0, 0, 25, 0)),
  plot.title.position = "plot",
  plot.caption = element_text(color = "grey50", size = 10, hjust = 1,
                              family = "Times New Roman", 
                              lineheight = 1.05, margin = margin(30, 0, 0, 0)),
  plot.caption.position = "plot", 
  plot.margin = margin(rep(20, 4))
)
```

```{r}
#| label: ex-guass brms family file


source(here::here("exguass_reparm.R"))

# load ex-gauss family with more traditonal mu, sigma, tau
```

We live in a world that, even for adults, is, "blooming and buzzing with confusion" [@james1890, p.488]. Despite this, we possess the remarkable ability to accomplish perceptual tasks, such as deciphering unclear and unsegmented handwritten cursive words, or actively participating in conversations amidst the chaos of a noisy bar. The ability to deal with a noisy, confusing world, has been a topic of research at the interface between education and cognitive psychology. Decades of work has demonstrated a relationship between encoding difficulty and long-term memory. While making learning something harder and not easier runs counter to people's beliefs, a host of memory and learning effects show that making encoding more effortful (and thus more errorful), under certain circumstances, can be beneficial for memory. This has popularly become known as the "desirable difficulties" principle (Bjork & Bjork, 2011). Well-documented examples of desirable difficulties include spacing out encoding across multiple sessions rather than massing study during a single session [see @carpenter2022], studying concepts in an interleaved fashion rather than a blocked fashion [@rohrer2007], and generating or retrieving information rather than simply re-reading or studying the information again [@roediger2006; @slamecka1978].

Another desirable difficulty example involves a very simple manipulation: changing the perceptual characteristics of to-be-learned material to make it more difficult to process. A growing literature has shown that manipulating the perceptual characteristics of the to-be-learned material at encoding can improve memory [e.g., @geller2018; @geller2021; @halamish2018; @rosner2015]. The resulting memory benefit has been called the perceptual disfluency effect [see @geller2018 for a discussion].

## The perceptual disfluency effect

The relationship between perceptual disfluency and long-term memory has a long and storied history. While it is not quite clear where the term perceptual disfluency effect originated from, the idea behind it goes back to the late 80s with the work of @nairne1988. Under the term *perceptual-interference effect*, Nairne employed the technique of backward masking with hash marks ( e.g., ####) with a quick presentation time to make word encoding noisier during study. Because the word is presented and masked so quickly, considerable effort is needed to identify the word. Since then, different types of perceptual disfluency manipulations have shown to elicit a similar effect, such as high-level blurring [@rosner2015], word inversion [@sungkhasettee2011] , small text size [@halamish2018], handwritten cursive [@geller2018], and other unusual or difficult-to-read typefaces [@geller2021; @weissgerber2017; @weltman2014].

Given the simplicity and ease in which perceptual disfluency can be implemented, it is not surprising researchers began touting the educational implications of such a manipulation. Perceptual disfluency as a possible educational intervention started to garner more support with the publication of @diemand-yauman2011. Across two experiments, @diemand-yauman2011 showed that placing learning materials in disfluent typefaces (e.g., Comic Sans, Bodoni MT, Haettenschweiler, Monotype Corsiva) improved memory in the lab (when learning about space aliens) and in a high school classroom where students learned about a variety of different content areas (i.e., AP English, Honors English, Honors Physics, Regular Physics, Honors U.S. History, and Honors Chemistry) from PowerPoints with information presented in difficult to read typefaces.

Unfortunately, the evidence for the perceptual disfluency effect is not always as clear as in the example above. A recent case in point is the typeface Sans Forgetica, developed through a collaboration among marketers, psychologists, and graphic designers [see @earp2018]. Originally launched with the strong claim that typeface enhances memory retention due to the backward-slanting letters and gaps within each letter which forces individuals to 'generate' the missing parts of each word. However, several subsequent studies have failed to replicate these claims, finding Sans Forgetica to be *just as* forgettable [@cushing2022; @geller2020; @huff2022; @roberts2023; @taylor2020; @wetzler2021]. Similar negative results have been found for various other perceptual disfluency interventions, such as small font sizes [@rhodes2008], difficult-to-hear stimuli [@rhodes2009], minor blurring [@yue2013], and alternative typefaces [@rummer2015].

Due to the mixed findings, researchers began exploring boundary conditions of the disfluency effect. The importance of testing the effects of disfluency in the presence of other variables is key to its usefulness as an educational intervention. @geller2018, for instance, found level of disfluency (more vs. less disfluent) mattered. Using easy-to-read and hard-to-read handwritten cursive words,@geller2018 showed there is a *Goldilocks* zone for perceptual disfluency effects. That is, stimuli cannot be too easy to read (i.e., print words) or too hard to read (i.e., hard-to-read cursive). Only when the stimuli were moderately difficult or just right (i.e., easy-to-read cursive), did memory improve. In another paper, @geller2021 demonstrated that memory benefits for disfluent stimuli are more robust when test expectancy is low. That is, a disfluency effect is only seen when participants are not told about an upcoming memory test. The authors reasoned that knowing about a memory test engaged a strategy where all stimuli get processed to a deeper level, regardless of how perceptually disfluent the stimulus is. This countervails any benefit disfluency has on memory. Additionally, a few studies have noted the importance of individual differences. For example, @eskenazi2021 showed better spellers remembered more words and meanings than poor spellers when placed in a disfluent font (Sans Forgetica).

Although perceptual disfluency can arise in certain contexts, its usefulness in educational settings—where students are prepared for tests—may be limited. However, @geller2021 argued that perceptual disfluency holds practical value because people often rely on incidental memory in everyday decision-making. For disfluency to be leveraged effectively, though, it is essential to predict when and where it will occur.

## Theoretical accounts of the disfluency effect

To achieve the aim of utilizing perceptual disflueny in applied settings, a better understanding of the mechanisms involved in eliciting the disfluency effect. Several theories have been proposed to explain this phenomenon. @geller2018 provided a review of two theories put forth to explain the disfluency effect. The metacognition account of disfluency [@alter2013; @pieger2016] posits that disfluency acts as a signal to engage in more control/regulatory processing [@pieger2016]. Within this account, disfluency arises *after* the stimulus has been identified. As a result, the type of disfluency experienced does not matter, just that the learner perceives something as disfluent and regulates their behavior properly.

The compensatory processing account [@mulligan1996] suggests that the disfluency effect is a result of increased top-down processing from lexical and semantic levels of representation. This framework is largely based on the popular interactive activation mode (IA model) of word recognition [@mcclelland1981]. In the IA model, visual input activates representations at three levels of representation: the feature, letter, and word levels. Activation in the IA model is both feed-forward and feed-backward. Thus, when there is perceptual noise (such as the one induced by a mask), there is increased top-down processing from higher, lexical or semantic, levels to aid in word identification. It is this increased top-down processing to lower levels that produces better memory.

More recently, @ptok2019 put forth a limited capacity and stage-specific model to explain conflict-encoding effects like the perceptual disfluency effect. Within their model, memory effects arising from encoding conflict rely on (1) the level of processing tapped by the task and (2) metacognitive processes that include monitoring and control. Across six experiments, @ptok2020 demonstrated better recognition memory for target words when shown with incongruent versus congruent semantic distractor words (i.e., category labels of size, animacy, or gender; e.g., "Chair - Alive" vs. "Chair - Inanimate"), but no memory benefit for incongruent versus congruent response distractor words (e.g., Lisa -"left"/ Lisa - "right"). While both tasks resulted in conflict evinced by longer reaction times (RTs) to targets preceded by incongruent primes, only when the encoding task focused attention on target meaning (i.e., semantic categorization) rather than response selection did a memory benefit arise. In a follow-up set of experiments, @ptok2020 replicated this pattern of findings, and in addition, provided physiological evidence from cognitive pupillometry (i.e., the study of eye pupil size and how it relates to cognitive processing), which has been shown to be an indicator of mental effort and congitive control [see @mathot2018 and @vanderwel2018 for reviews]. In their study they observed larger pupil size for semantic incongruent and response incongruent primes, but only observed a memory benefit for semantic incongruent conditions. Interestingly, they also showed that these memory benefits can be mitigated by manipulating endogenous attention. @ptok2020 (Experiment 3) were able to eradicate the conflict encoding benefit by having participants sit in a chinrest and focus on the task. This is similar to what has been found in the perceptual disfluency literature. For example, having participants study words in anticipation for a test can eradicate the benefit of perceptual disfluency @geller2021. In addition, requiring participants to make judgments of learning (i.e., a metamemory judgment on a scale of 0-100 indicating how likely it is they will recall the word on a later memory test) after each studied word also eradicates the disfluency effect [@rosner2015; @besken2013]. Taken together, this highlights the critical role of both the kind of processing done on the to-be-remembered stimuli, and control processes in eliciting a disfluency effect.

All three of these theories propose potential loci for the perceptual disfluency effect. In the metacognitive account, the disfluency effect occurs at a later post-lexical stage, after word recognition has taken place. The compensatory processing account [@mulligan1996] links the perceptual disfluency effect directly to the word recognition process. That is, disfluent words receive more top-down processing from lexical or semantic levels during encoding. Lastly, the stage-specific model proposed by @ptok2019 associates perceptual disfluency effects with a specific stage of processing, namely the semantic level, but it also considers general attentional and cognitive control processes that are not solely tied to the word recognition process.

## Moving beyond the mean: modeling RT distributions

### Ex-Gaussian distribution

To test the different stages or loci involved in the perceptual disfluency effect, it is necessary to use methods that allow for a more fine-grained analysis of processes during encoding. In the perceptual disfluency literature (and learning and memory more broadly), it is common to use measures such as mean accuracy and RTs to assess differences between a disfluent and fluent condition [@geller2018; @geller2021; @rosner2015]. While this approach is often deemed as acceptable practice, there has been a call to go beyond traditional RT methods when making inferences [see @balota2011].

There are a several reasons for making the shift from traditional *mean*-RT analyses *(which is what linear models do)* to *approaches* that utilize the whole RT distribution. One reason is traditional approaches fail to capture the nuances inherent in RT distributions. Namely, RT distributions are unimodal and positively skewed. A standard analysis based on means can conceal effects that change only the shape of the tail of the distribution, only the location, or both the location and the shape of the distribution.

Another reason to transition away from traditional analyses is that RTs provide only a coarse measure of processing during encoding. RTs capture the total sum of various task-related factors, ranging from non-decisional components like stimulus encoding and motor responses to decisional components. This amalgamation does not allow one to parse out specific effects that might occur early or later in processing.

Lastly, from a statistical standpoint, RTs present significant challenges. Specifically, they often violate two crucial assumptions: they are not normally distributed and their variance is frequently heterogeneous. Such violations can lead to biased results when making statistical inferences, as pointed out by [@wilcox1998].

A perfect example of this comes from the Stroop task [@stroop1935] . The classic Stroop finding shows words presented in an incongruent color font (the word "red" printed in "blue" font) increases RTs compared to words in a baseline condition (e.g., XXXXX presented in a color font). The Stroop interference effect is something you can bet your house on. A more inconsistent finding is seeing facilitation (shortened RTs) when the word and color are congruent (i.e., "Green" presented in "Green") compared to a baseline condition.

One alternative for this conundrum is to examine RT distributions using mathematical models that capture the nuances of the RT distribution and consider various statistical properties, such as the location (\$\\mu\$) , spread ($\sigma$), and skewness ($\tau$) of the distribution. One popular choice is the ex-Gaussian distribution [@balota2011; @ratcliff1978]. As the name suggests, the ex-Gaussian distribution decomposes RTs into three parameters: mu (μ) representing the mean of the Gaussian component, sigma (σ) representing the standard deviation of that Gaussian component, and tau (τ) representing the mean and standard deviation of an exponential component capturing the tail of the distribution. The algebraic mean of ex-Gaussian is a combination of $\mu$ + $\tau$ and the sd is $\sqrt{\sigma^2 + \tau^2}$ . Together the three parameters represent different aspects of the distribution's location and shape.

@heathcote1991 explored the facilitation-inhibition asymmetry in the Stroop task with an ex-Gaussian model and found both facilitation (from congruent trials) and interference (from incongruent trials) on $\mu$. For $\sigma$, the analysis showed interference, but no facilitation. For $\tau$, there was interference for both congruent and incongruent conditions. Comparing this to a mean RT analysis, they showed the standard interference Stroop effect, but no facilitation. Given that the algerbraic mean of the ex-Gaussian is $\mu$ + $\tau$, the failure to observe a facilitation effect in the standard mean analysis likely arose from facilitation on $\mu$ and interference on $\tau$ canceling each other out. A finding such as this would be impossible looking solely at mean RTs.

Exploring effects from a distributional perspective has provided a richer understanding of how different experimental manipulations affect word recognition. Experimental manipulations can produce several distinct patterns. One pattern involves a shift of the entire RT distribution to the right, without increasing the tail or skew. A pattern such as this would suggest a general effect and would manifest as an effect on $\mu$, but not $\tau$ . As an example, semantic priming effects--where responses are faster to targets when preceded by a semantically related prime compared to an unrelated prime--can be nicely explained by a simple shift in the RT distribution [@balota2008]. Alternatively, an experimental manipulation could produce a pattern where the RT distribution is skewed or stretched in the slower condition. This suggests that the manipulation only impacts a subset of trials, and is visible as an increase in $\tau$. An example of an effect that only impacts $\tau$ is the transposed letter effect in visual word recognition [@johnson2012]. The transposed letter (TL) effect involves misidentification of orthographically similar stimuli that with transposed internal like, like mistaking "JUGDE" for "JUDGE" [@perea2003]. Finally, you could observe a pattern wherein an experimental manipulation results in both changes in $\mu$ and $\tau$, which would shift and stretch the RT distribution. Recognizing low frequency words have been shown to not only shift the RT distribution, but also stretch the RT distributions [@andrews2001; @balota1999; @staub2010].

The ex-Gaussian model, while mostly descriptive in nature. has been used as a theoretical tool to map model parameters onto cognitive processes. For example, the $\mu$ and $\sigma$ parameters have been tied to early, non-analytic, processing. In the area of semantic priming, the selective effect on $\mu$ has been taken as evidence for an automatic spreading activation process (or head-start), according to which the activation of a node in a semantic network spreads automatically to interconnected nodes, preactivating a semantically related word [@dewit2015; @balota2008]. The exponential component ($\tau$) has been tied to later, more analytic, processing [@balota1999]. Specifically, increases in $\tau$ have been attributed to working memory and attentional processes [@Fitousi2020; @kane2003]. For instance, @johnson2012 tied $\tau$ differences for the TL effect to a post-lexical checking mechanism that arose from a failure to identify the stimulus on a select number of trials rather than a broader, lexical, effect occurring on every trial. When taken together, these findings suggest that ex-Gaussian parameters could map to early vs. late stages of cognitive processing. However, such mapping between distributional descriptives and and cognitive processes remains controversial and should be interpreted carefully [@matzke2009; @heathcote1991].

### Drift-diffusion model (DDM)

Unlike the ex-Gaussian distribution discussed above, which makes little theortical commitments regarding process, the drift diffusion model - DDM [see @ratcliff2016, for a comprehensive introduction] is a process-model and it's parameters can be linked to latent cognitive constructs [@gomez2013]. The DDM is a popular computational model commonly used in binary speeded decision tasks such as the lexical decision task (LDT). The DDM model assumes a decision is a cumulative process that begins at stimulus onset and ends once a noisy accumulation of evidence has reached a decision threshold.The DDM has led to important insights into cognition in a wide range of choice tasks, including perceptual-, memory-, and value-based decisions [@myers2022].

In the DDM, RTs are decomposed into several parameters that represent distinct cognitive processes. The most relevant to our purposes here are the drift rate ($v$) and non-decision time (ndt; $T_{er}$) parameters. Drift rate ($v$) represents the rate at which evidence is accumulated towards a decision boundary. In essence, it is a measure of how quickly information is processed to make a decision. A higher (more positive) $v$ indicates a steeper slope, meaning that evidence is accumulated more quickly, leading to faster decisions. Conversely, a lower $v$ indicates a shallower slope, meaning that evidence is accumulated more slowly. Drift rate is closely linked to the decision-making process itself and serves as an index of global processing demands imposed by factors such as task difficulty, memory load, or other concurrent cognitive demands—particularly when these processes compete for the same cognitive resources [@boag2019]. Additionally, drift rates have been implicated as a key mechanism of reactive inhibitory control [@braver2012], where critical events (e.g., working memory updates or task switches) trigger inhibition of prepotent response drift rates [@boag2019; @boag2019a].

The $T_{er}$ parameter represents the time taken for processes other than the decision-making itself. This includes early sensory processing (like visual or auditory processing of the stimulus) and late motor processes (like executing the response).

The DDM has been shown to be a valuable tool for studying the effects of different experimental manipulations on cognitive processes in visual word recognition. For example, @gomez2014 demonstrated certain manipulations can deferentially affect specific parameters of the model. For instance manipulating the orientation of words (rotating them by 0, 90, or 180 degrees) affected the $T_{er}$ component, but not $v$ component. In contrast, word frequency (high-frequency words vs. low-frequency words) primarily influenced both the drift rate and non-decision time. These findings highlight the sensitivity of the DDM in identifying and differentiating the impact of various stimulus manipulations on different cognitive processes involved in decision-making.

## Goals of the present experiments

In the present experiments, we pursued two aims related to perceptual disfluency. The first aim was to examine the replicability of the perceptual disfluency effect. To optimize our chances for observing this effect, we utilized a disfluency manipulation known in the literature to enhance memory: blurring [@rosner2015]. We manipulated perceptual disfluency by blurring the words at three levels. Participants were presented with clear words (no blur), low blurred words (5% Gaussian blur) and high blurred words (15% Gaussian blur). High level blurring has been shown to enhance memory [@rosner2015]. As @geller2018 noted, not all manipulations are created equal. Perceptual manipulations affect processing in different ways. It is important to show just how these manipulations affect different stages of processing and what type of manipulations do and do not produce a disfluency effect. By examining different levels of perceptual disfluency, we provide a more nuanced account of encoding processes and how this affects memory.

The second, more pivotal aim was to enrich the methodological toolkit available to researchers investigating conflict encoding, such as perceptual disfluency. By applying distributional techniques—specifically ex-Gaussian analysis and the diffusion decision model (DDM)—our goal was to demonstrate how these approaches can provide deeper insight into how encoding difficulty influences memory. It is important to note that other distributional techniques for analyzing response times exist (e.g., the linear ballistic accumulator model, gamma distributions, etc.). However, we chose the ex-Gaussian and DDM approaches due to their popularity and extensive use in the word recognition literature, where they have received the most empirical attention [@balota2008]. Ultimately, these efforts aim to clarify the conditions under which perceptual disfluency enhances memory—and when it does not.

To promote transparency and reproducibility, this paper was written in R [@R] using Quarto [@Allaire_Quarto_2024], an open-source publishing system that allows for dynamic and static documents. This allows figures, tables, and text to be programmatically included directly in the manuscript, ensuring that all results are seamlessly integrated into the document.To increase computational reproducibility we use the rix [@rix] package which harnesses the power of the nix [@nix] ecosystem to help with computational reproducibility. Not only does this give us a snapshot of the packages used to create the current manuscript, but it also takes a snapshot of system dependencies used at run-time. This way reproducers can easily re-use the exact same environment by installing the nix package manager and using the included default.nix file to set up the right environment. The README file in the GitHub repository contains detailed information on how to set this up to reproduce the contents of the current manuscript. We have also included a video tutorial. We hope this will make it easier for researchers to apply this code to their own research.

### Predictions

Using the ex-Gaussian distribution and the DDM will provide us with a descriptive account of how disfluency manipulations affect encoding. Each theoretical account makes specific predictions about the loci of the perceptual disfluency affect. We can use these predictions to forecast how the components in each mathematical model might be influenced.

If the metacognitive account is correct [@alter2013], which assumes a post-lexical locus for the perceptual disfluency effect, we might predict a lengthening of the distribution tail on some of the trials for blurred words. This would manifest itself on the $\tau$ component. Specifically, a larger $\tau$ parameter estimate for high blurred and low blurred words compared to no blurred words. As it relates to memory performance, there should be no difference between high and low blurred words.

Another scenario entails a general slow down of processing---causing distributional shifting $\mu$, but not skewing $\tau$. According to the the compensatory processing account [@mulligan1996], we would expect increased shifting for high blurred words compared to low and no blurred words. This should result in a mnenmonic benefit for high blurred words, but not low blurred words.

Lastly, we might observe not only a shift of the entire distribution (an effect on $\mu$), but also change the shape of the distribution (an effect on $\tau$ ), indicating a combination of early and late processes. A similar pattern has been found with hard-to-read handwritten words [e.g., @perea2016; @vergara-martínez2021]. This extra post-lexical processing received by high blurred words is assumed to facilitate better recognition memory. This finding would be in line with the stage-specific account [@ptok2019]. The finding of better memory for high blurred words, but not low blurred words would be in line with the stage-specific account of conflict encoding [@ptok2019, @ptok2020]. Having a better sense of when and where disfluency effects arise is critical in determining its usefulness in the educational milieu.

In terms of the DDM parameters, we predicted high blurred words would affect both $v$ and $T_{er}$ parameters. Specifically, high blurred words would produce lower $v$ and high $T_{er}$ compared to clear and low blurred words. Additionally, we predicted that low blurred words would only affect $T_{er}$. @tbl-predictions summarizes each account of the perceptual disfluency and the predicted outcomes according to each mathematical model. Importantly, some of the principles espoused by the three accounts are postulated verbally, and they can be realized in models in many different ways. We have made a good-faith effort to translate these verbal descriptions into formal models, but we recognize that reasonable researchers might make different modeling choices—a reality that reflects the unavoidable subjectivity in scientific inference [see @mcelreath2020statistical.

In addition to ex-Gaussian and DDM analyses, we will provide a graphical description of changes to the RT distribution using quantile and delta plots [see @angele2023]. The process of visualization through quantile analysis can be broken down into four distinct steps:

1.  Sorting and plotting: For correct trials, RTs are arranged in ascending order within each condition. We then plot the average of the specified quantiles (e.g., .1, .2, .3, .4, .5, .9).

2.  Quantile averaging across participants: The individual quantiles for each participant are averaged, a concept reminiscent of Vincentiles.

3.  Between-condition quantile averaging: The average for each quantile is computed between the conditions.

4.  Difference calculation: We determine the difference between the conditions, ensuring the sign of the difference remains unchanged.

Typically, there are four observable patterns in the graphical depiction. No observable difference occurs when the conditions do not show any noticeable distinction. Late differences emerge when increasing differences appear later in the sequence, suggesting that the conditions diverge over time. A complete shift indicates a consistent difference across all quantiles, signaling an overall shift in the distribution. Finally, early differences reveal distinctions early in the reaction time distribution, suggesting an initial divergence between conditions. maps these patterns onto existing theoretical models of disfluency.

```{r}
#| label: tbl-predictions
#| tbl-cap: "Mapping model predictions to theoretical constructs"

data <- data.frame(
  Account = rep(c("Meta-cognitive", "Compensatory-processing", "Stage-specific"), each = 2),
  Description = rep(c(
    "Perceptual disfluency affects meta-cognitive processes via increased system 2 processing", 
    "Perceptual disfluency affects the word recognition process", 
    "Disfluency effects rely on (1) the stage or level of processing tapped by the task and (2) monitoring and control processes"
  ), each = 2),
  Loci = rep(c("Post-lexical", "Lexical/semantic", "Lexical/semantic and Post-lexical"), each = 2),
  Contrast = c("High Blur vs. Low Blur/Clear", "Low Blur vs. Clear",
               "High Blur vs. Low Blur/Clear", "Low Blur vs. Clear",
               "High Blur vs. Low Blur/Clear", "Low Blur vs. Clear"),
  ExGaussianPredictions = c(
    "μ: ×\nτ: ↑", "μ: ×\nτ: ↑",                # Meta-cognitive
    "μ: ↑\nτ: ×", "μ: x\nτ: ×",                # Compensatory-processing (updated)
    "μ: ↑\nτ: ↑", "μ: ↑\nτ: ×"                 # Stage-specific
  ),
  DDMPredictions = c(
    "Ter: ×\nv: ↑", "Ter: ×\nv: ↑",           # Meta-cognitive
    "Ter: ↑\nv: ×", "Ter: ↑\nv: ×",           # Compensatory-processing (updated)
    "Ter: ↑\nv: ↑", "Ter: ×\nv: ×"            # Stage-specific
  ),
  QuantilePlots = c(
    "Late Difference",
    "Late Difference", 
    "Complete Shift", 
    "No Difference", 
    "Complete Shift + Late Differences", 
    "No Difference"
  ), 
  RecognitionMemoryPredictions = c(
  "High > Low/Clear", "Low > Clear",        # Meta-cognitive
  "High > Low/Clear", "Low  =  Clear",   # Compensatory-processing (updated)
  "High > Low/Clear", "Low  =  Clear"                   # Stage-specific
),
  stringsAsFactors = FALSE
) |> 
  dplyr::mutate(
    Account = ifelse(duplicated(Account), "", Account),
    Description = ifelse(duplicated(Description), "", Description),
    Loci = ifelse(duplicated(Loci), "", Loci)
  )

ft <- flextable(data) |>
  set_header_labels(
    Account = "Account",
    Description = "Description",
    Loci = "Loci",
    Contrast = "Contrast",
    ExGaussianPredictions = "Ex-Gaussian Predictions",
    DDMPredictions = "Drift Diffusion Predictions",
    QuantilePlots = "Quantile Plots",
    RecognitionMemoryPredictions = "Recognition Memory Predictions"
  ) |>
  merge_v(j = c("Account", "Description", "Loci")) |>
  align(j = c("Account", "Description", "Loci", "Contrast"), align = "left", part = "body") |>
  align(j = c("ExGaussianPredictions", "DDMPredictions", "QuantilePlots", "RecognitionMemoryPredictions"), 
        align = "center", part = "body") |>
  width(j = "Account", width = 1) |>
  width(j = "Description", width = 2) |>
  width(j = "Loci", width = 1) |>
  width(j = "Contrast", width = 1) |>
  width(j = "ExGaussianPredictions", width = 1.5) |>
  width(j = "DDMPredictions", width = 1.5) |>
  width(j = "QuantilePlots", width = 1.5) |>
  width(j = "RecognitionMemoryPredictions", width = 1.5) |>
  bold(part = "header")

ft

```



# Experiment 1A: Context Reinstatement

In Experiment 1A, we collected RTs from a lexical decision task (LDT) during encoding followed by a surprise recognition memory test. Using a two-choice task like the LDT allowed us to examine how perceptual disfluency affects encoding processes using mathematical models. Based on previous research [@geller2021], there was no mention of the recognition test when participants signed up for the study to give us the best chance of observing a disfluency effect.

## Method

### Transparency and Openness

This study complies with transparency and openness guidelines. The preregistered analysis plan for this experiment can be found here: https://osf.io/q3fjn. All raw and summary data, materials, and R scripts for pre-processing, analysis, and plotting for Experiments 1 can be found at https://osf.io/6sy7k/. All deviations and changes from the preregistration are noted herein.

### Participants

We preregistered a sample size of 216. All participants were recruited through the university subject pool (SONA). A design with a sample size of 216 can detect effect sizes of $\delta \geq 0.2$ with a probability of at least 0.90, assuming a one-sided criterion for detection that allows for a maximum Type I error rate of $\alpha = 0.05$. Per our exclusion criteria, we retained participants that were native English speakers, were over the age of 17, had overall accuracy on the LDT greater than 80%, and did not complete the experiment more than once. Due to our exclusion criteria, we oversampled participants. To ensure equal number of participants in each list we randomly chose 36 participants from each list to reach our target sample size.

### Apparatus and stimuli

The experiment was run using PsychoPy software and hosted on Pavlovia (www.pavlovia.org). You can see an example of the experiment by navigating to this website: <https://run.pavlovia.org/Jgeller112/ldt_dd_l1_jol_context>.

We used 84 words and 84 nonwords for the LDT. Words were obtained from the LexOPS package in R [@taylor2020a]. All of our words were matched on a number of different lexical dimensions. All words were nouns, 4-6 letters in length, had a known proportion of between 90%-100%, had a low neighborhood density (OLD20 score between 1-2), high concreteness, imageability, and word frequency. Our nonwords were created using the English Lexicon Project [@balota2007]. Stimuli can be found at our OSF project page cited above.

#### Blurring

Blurred stimuli were processed through the imager package in R [@imager] and a personal script (https://osf.io/gr5qv). Each image was processed through a high blur filter (Gaussian blur of 15) and low blur filter (Gaussian blur of 10). These pictures were then imported into PsychoPy as picture files. See @fig-blur for examples of what a clear, low blurred, and high blurred word would look like to the participant.

```{r}
#| label: fig-blur
#| fig-cap: Clear (left), low  blur (10% blur) (right), and high blur (15% blur) (center) examples 
# Combine images in one figure
include_graphics("figures/blur.jpg")
```

### Design

We created two lists: One list (84 words; 28 clear, 28 low blur, and 28 high blur) served as a study (old) list for the LDT task while the other list served as a test (new) list (84 words; 28 clear, 28 low blur, and 28 high blur) for our recognition memory test that occurred after the LDT. We counterbalanced each list so each word served as an old word and a new world and were presented in clear, low blurred, and high blurred across participants. This resulted in six counterbalanced lists. Lists were assigned to participants so that across participants each word occurs equally often in the six possible conditions: clear old, low blur old, high blur old, clear new, low blur new, and high blur new. For the LDT task, we generated a set of 84 legal nonwords that we obtained from the English Lexicon Project. These 84 nonwords were used across all 6 lists.

### Procedure

The experiment consisted of two phases: an encoding phase (LDT) and a test phase. During the encoding phase, a fixation cross appeared at the center of the screen for 500 ms. The fixation cross was immediately replaced by a letter string in the same location. To continue to the next trial, participants had to decide if the letter string presented on screen was a word or not by either pressing designated keys on the keyboard ("m" or "z") or by tapping on designated areas on the screen (word vs. nonword) if they were using a cell phone/tablet. After the encoding phase, participants were given a surprise old/new recognition memory test. During the test phase, a word appeared in the center of the screen that either had been presented during study ("old") or had not been presented during study ("new"). Old words occurred in their original typeface, and following the counterbalancing procedure, each of the new words was presented as clear, low blurred, or high blurred. All words were individually randomized for each participant during both the study and test phases and progress was self-paced. After the experiment, participants were debriefed. The entire experiment lasted approximately 15 minutes.

### Data Analysis Plan

All models were fit in R (v.4.5.0) [@R] using the Stan modeling language [@grant2017] via the `brms` package [@brms]. [^1]We used maximal random-effects structures justified by the design [@barr2013].

[^1]: To promote transparency and reproducibility, this paper was written using Quarto [@Allaire_Quarto_2024]. To increase computational reproducibility we use the rix [@rix] package which harnesses the power of the nix [@nix] ecosystem to help with computational reproducibility. We used the following R packages: brms v. 2.21.0 (Bürkner 2017, 2018, 2021), cmdstanr v. 0.8.1 (Gabry et al. 2024), colorspace v. 2.1.1 (Zeileis, Hornik, and Murrell 2009; Stauffer et al. 2009; Zeileis et al. 2020), cowplot v. 1.1.3 (Wilke 2024), data.table v. 1.17.0 (T. Barrett et al. 2025), easystats v. 0.7.4 (Lüdecke et al. 2022), emmeans v. 1.10.4 (Lenth 2024), flextable v. 0.9.6 (Gohel and Skintzos 2024), ggdist v. 3.3.2 (Kay 2024b, 2024a), ggeffects v. 1.7.0 (Lüdecke 2018), ggokabeito v. 0.1.0 (M. Barrett 2021), ggrepel v. 0.9.6 (Slowikowski 2024), ggtext v. 0.1.2 (Wilke and Wiernik 2022), here v. 1.0.1 (Müller 2020), hypr v. 0.2.8 (Schad et al. 2019; Rabe et al. 2020), knitr v. 1.50 (Xie 2014, 2015, 2025), modelbased v. 0.10.0 (Makowski et al. 2020), parameters v. 0.24.2 (Lüdecke et al. 2020), rmarkdown v. 2.29 (Xie, Allaire, and Grolemund 2018; Xie, Dervieux, and Riederer 2020; Allaire et al. 2024), tidybayes v. 3.0.7 (Kay 2024c), tidylog v. 1.1.0 (Elbers 2024), tidyverse v. 2.0.0 (Wickham et al. 2019).

We ran four chains of 5,000 MCMC iterations (1,000 warm-up), totaling 16,000 post-warm-up samples (except for the diffusion model, which used 2,000 iterations to reduce computation time). Model quality was checked via prior/posterior predictive checks, $\hat{R}$, and ESS [@vehtari2021]. Convergence was assessed using $\hat{R}$ (target ≤ 1.01) and effective sample size (ESS ≥ 1000) [@brms]. Default (non-informative) priors were used for most parameters. Weakly informative priors were used for population-level parameters to enable Bayes factor (Evidence Ratio(ER)) calculations for two sided-hypotheses against a point null. Full prior specifications are available in the Quarto source file on OSF:

We report posterior means and 90% credible intervals (CrIs) for one-sided hypotheses (preregistered differences), and 95% CrIs for two-sided hypotheses (against zero). Estimated marginal means were extracted using a combination of `emmeans` [@emmeans], `bayestestR` [@bayestestR], and `brms` [@brms].

Additionally, we report the posterior probability (`Post.Probability`) that an effect lies in a particular direction and ER, which is a generalization of the Bayes factor for directional hypotheses. An ER \> 3 indicates moderate to strong evidence for the hypothesis; ER \< 0.3 indicates support for the alternative; and ER values between 0.3 and 3 are considered inconclusive. ERs were also used to assess point-null hypotheses ($\delta = 0$). Hypotheses were considered supported if zero was excluded from the CrI, The posterior probability approached 1, and ER was \> 3.

For all models (excluding the DDMs), we applied ANOVA-style (effects) coding using contrast variables. For the Blur factor in Experiments 1A, 1B, and 2, we defined two orthogonal contrasts to capture the primary comparisons of interest. Contrast 1 compared High Blur against the average of Clear and Low Blur, coding High Blur as 0.5 and both Clear and Low Blur as –0.5. Contrast 2 isolated the difference between Low Blur and Clear, with Low Blur coded as 0.5, Clear as –0.5, and High Blur as 0. In Experiment 2, we also included a Frequency factor, with High Frequency coded as 0.5 and Low Frequency as –0.5.

Although these contrasts deviate from our preregistered comparisons, we believe they offer a more targeted test of our hypotheses. For transparency, we provide all pairwise comparisons in the accompanying visualizations.

#### Accuracy

Accuracy (coded as correct (1) vs. incorrect (0)) was modeled using a Bayesian logistic regression with a Bernoulli distribution.

####  Ex-Gaussian model

We modeled response times with an ex-Gaussian distribution[^2], allowing the Gaussian mean/location ($\mu$), the Guassian standard deviation ($\sigma$) and the exponential scale ($\beta = 1/\lambda$) to vary by condition. Please note that when we refer to $\beta$ we are referring to $\tau$. To visualize distributional shifts, we generated quantile and delta plots [@balota2008; @dejong1994].

[^2]: The parameterization used by brms does not match the standard formulation of the ex-Gaussian distribution. A custom script was implemented to recover the traditional parameters.

#### Diffusion model

We fit a hierarchical Bayesian Wiener diffusion model [@vandekerckhove2011] with accuracy coding, estimating drift rate (v), boundary separation (response caution; fixed at 0.5), and non-decision time (\$T\_{er}\$).

#### Recognition memory

Following recent trends [@zloteanu2024] recognition memory data were analyzed using a Bayesian generalized linear multilevel model (GLMM) (Bernoulli distribution with a probit link). Here the response of the participant ("say old" vs. "say new") are modeled as function of item status (" is old" vs. " is new") and condition.

Bayesian GLMMs provide a more precise and flexible approach than traditional SDT analyses. Following Signal Detection Theory [SDT; @green1966], participant responses can be classified as hits, correct rejections, misses, or false alarms, depending on the item status ("old" vs. "new"). In the probit regression framework, the interaction between item status and a predictor of interest corresponds directly tp *d′*, while the main effects reflect response criterion [@decarlo1998; for a detailed discussion of Bayesian SDT modeling see @zloteanu2024]. Note that the model parameterization reflects *–c* (i.e., reversed sign) and this is what is reported in the paper. For visualization purposes we use the conventional parameterization, where positive values indicate more conservative responding and negative values indicate a more liberal bias.

## Results

All models presented no divergences, and all chains mixed well and produced comparable estimates ($\hat{R}$ \< 1.01 and ESS \> 1000).

### Accuracy

```{r}

blur_acc <- read_csv("https://osf.io/xv5bd/download")

```

```{r}
#The data file is cleaned (participants >=.8, no duplicate participants, no participants < 17. )
# get data from osf
blur_acc <- blur_acc  |>
    dplyr::filter(lex=="m")


blur_acc_new<- blur_acc |>
  dplyr::filter(rt >= .2 & rt <= 2.5)
```

The analysis of accuracy is is based on `r dim(blur_acc)[1]` data points. After removing fast and slow RTs we were left with `r dim(blur_acc_new)[1]` data point points (`r round(1-dim(blur_acc_new)[1]/dim(blur_acc)[1], 3)`).

```{r}
#| echo: false

## Contrasts
#hypothesis

# Create custom contrast matrix manually
contrast_matrix <- matrix(
  c(-.5,  -.5,   # C
     .5,  0,   # HB
     -.5, .5),  # LB
  ncol = 2,
  byrow = TRUE
)


blur_acc_new$blur <- as.factor(blur_acc_new$blur)

# Assign to factor
contrasts(blur_acc_new$blur) <- contrast_matrix


```

```{r}
#| eval: false
#| echo: false

#weak prior
prior_exp1 <- c(set_prior("cauchy(0,.35)", class = "b"))

#fit model
fit_acc_weak <- brm(corr ~ blur + (1 + blur | participant) + (1 + blur | string),
  data = blur_acc_new,
  warmup = 1000,
  iter = 5000,
  chains = 4,
  init = 0,
  family = bernoulli(),
  cores = 4,
  prior = prior_exp1,
  control = list(adapt_delta = 0.9),
  backend = "cmdstanr",
  save_pars = save_pars(all = T),
  sample_prior = T,
  seed=666,
  threads = threading(4),
  file = "fit_acc_context"
)
```

```{r}
# get file from osf
acc_c<-read_rds("https://osf.io/xkgqj/download")
```

```{r}
#| label: tbl-acc1A
#| tbl-cap: "Summary of posterior distributions for fixed effects in the Bayesian accuracy model (Experiment 1A)."
#| apa-note:  "95% CrI for equivalency tests. Posterior probability indicates the proportion of the posterior distribution that falls on one side of zero (either positive or negative), representing the probability that the effect is greater than or less than zero."

# Run hypotheses
a <- hypothesis(acc_c, "blur1 < 0")  # High Blur < (Low Blur + Clear)/2
b <- hypothesis(acc_c, "blur2 = 0")  # Low Blur < Clear

# Round numeric outputs
round_hypothesis <- function(hyp) {
  hyp$hypothesis <- hyp$hypothesis |> mutate(across(where(is.numeric), round, 3))
  hyp
}

a <- round_hypothesis(a)
b <- round_hypothesis(b)

# Combine and format table
tab <- bind_rows(a$hypothesis, b$hypothesis) |>
  mutate(
    Hypothesis = c(
      "High Blur < (Low Blur + Clear)",
      "Low Blur < Clear"
    ),
    `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")
  ) |>
  rename(
    Mean = Estimate,
    SE = Est.Error,
    ER = Evid.Ratio,
    `Posterior Prob` = Post.Prob
  ) |>
  select(Hypothesis, Mean, SE, `90% CrI`, ER, `Posterior Prob`)

# Create the flextable
acc_summary <- flextable(tab) |>
  theme_apa()

acc_summary


```

Model estimates can be found in @tbl-acc1A. High blur words had lower accuracy compared to clear and low blurred words, , $b$ = `r a$hypothesis$Estimate`, 90% CrI\[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`. However, the evidence was weak for there being no significant difference in the identification accuracy between clear and low blurred words, $b$ = `r b$hypothesis$Estimate`, 90% CrI\[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`.

### RTs: Ex-Gaussian

```{r}
#load data from osf
rts <- read_csv("https://osf.io/xv5bd/download")

```

```{r}

blur_rt<- rts |>
  group_by(participant) |>
   dplyr::filter(corr==1, lex=="m")#only include nonwords

blur_rt_new <- blur_rt |> 
  dplyr::filter(rt >= .2 & rt <= 2.5)|> # remove high and low RTs
    mutate(rt_ms=rt*1000)

```

The analysis of RTs (correct trials and words) is based on `r dim(blur_rt_new)[1]` data points, after removing fast and slow RTs (`r round(1-dim(blur_rt_new)[1]/dim(blur_rt)[1], 3)` .

```{r}
#| eval: false
bform_exg1 <- bf(
rt ~  blur + (1 + blur |p| participant) + (1 + blur|i| string),
sigma ~ blur + (1 + blur |p|participant) + (1 + blur |i| string),
beta ~ blur + (1 + blur |p|participant) + (1 + blur |i| string))
```

```{r}
#| eval: false
prior_exp1 <- c(prior("normal(0,10)", class = "b", coef = ""))

fit_exg1A <- brm(
  bform_exg1, data = blur_rt_new,
  warmup = 1000,
  iter = 5000,
  prior = prior_exp1,
  family = three_param_exgauss,
  stanvars = three_param_exg_stanvars,
  init = 0,
  cores = 4, 
  file="mod_exgauss_1B", 
  chains=4, 
  seed=666, 
  sample_prior = T, 
  save_pars = save_pars(all=T),
  control = list(adapt_delta = 0.8), 
  backend="cmdstanr", 
  threads = threading(2))

```

```{r}
#load rdata for model 
# no intercept model - easier to fit priors on all levels of factor
download.file("https://osf.io/m4nv5/download", destfile = "data/fit_sc.rds", mode = "wb")
fit_sc <- readRDS("data/fit_sc.rds")

```

```{r}
#| label: tbl-exgauss1A
#| tbl-cap: "Summary of posterior distributions for fixed effects in the ex-Gaussian model (Experiment 1A)."
#| apa-note:  "95% CrI for equivalency tests. Posterior probability indicates the proportion of the posterior distribution that falls on one side of zero (either positive or negative), representing the probability that the effect is greater than or less than zero. Sigma and Beta parameters are on the log (logit) scale."

# Define hypothesis tests
a <- hypothesis(fit_sc, "blur1 > 0")
b <- hypothesis(fit_sc, "blur2 > 0")

c <- hypothesis(fit_sc, "sigma_blur1 > 0")
d <- hypothesis(fit_sc, "sigma_blur2 = 0")

e <- hypothesis(fit_sc, "beta_blur1 > 0")
f <- hypothesis(fit_sc, "beta_blur2 = 0")
 
# Round all numeric columns in each result
round_hypothesis <- function(hyp) {
  hyp$hypothesis <- hyp$hypothesis |> mutate(across(where(is.numeric), round, 3))
  hyp
}

a <- round_hypothesis(a)
b <- round_hypothesis(b)
c <- round_hypothesis(c)
d <- round_hypothesis(d)
e <- round_hypothesis(e)
f <- round_hypothesis(f)

# Combine into one table
tab <- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis) |> 
  mutate(
    Parameter = c("Mu", "Mu", "Sigma", "Sigma", "Beta", "Beta"),
    Hypothesis = c(
      "High Blur > (Low Blur + Clear)",
      "Low Blur > Clear",
      "High Blur > (Low Blur + Clear)",
      "Low Blur = Clear",
      "High Blur > (Low Blur + Clear)",
      "Low Blur = Clear"
    ),
    `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")
  ) |>
  rename(
    Mean = Estimate,
    SE = Est.Error,
    ER = Evid.Ratio,
    `Posterior Prob` = Post.Prob
  ) |>
  select(Hypothesis, Parameter, Mean, SE, `90% CrI`, ER, `Posterior Prob`)

# Create flextable
flextable(tab) |>
  theme_apa()

```

A visualization of how blurring affected processing during word recognition can be seen in the quantile and delta plots in @fig-deltaquant. Beginning with the μ parameter, there was greater shifting for high blurred words compared to clear and low blurred words, *b* = `r round(a$hypothesis$Estimate, 3)`, 90% CrI \[`r round(a$hypothesis$CI.Lower, 3)`, `r round(a$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`. Low blurred compared to clear words showed greater shifting, *b* = `r round(b$hypothesis$Estimate, 3)`, 90% CrI \[`r round(b$hypothesis$CI.Lower, 3)`, `r round(b$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`.

Analyses of the σ and τ parameters yielded a similar pattern. Variance was higher for high blurred words compared to clear and low blurred words, *b* = `r round(c$hypothesis$Estimate, 3)`, 90% CrI \[`r round(c$hypothesis$CI.Lower, 3)`, `r round(c$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`. There was strong evidence for no difference between low blurred and clear words, *b* = `r round(d$hypothesis$Estimate, 3)`, 90% CrI \[`r round(d$hypothesis$CI.Lower, 3)`, `r round(d$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(d$hypothesis$Evid.Ratio)`.

Finally, there was greater skewing for high blurred words compared to clear and low blurred words, *b* = `r round(e$hypothesis$Estimate, 3)`, 90% CrI \[`r round(e$hypothesis$CI.Lower, 3)`, `r round(e$hypothesis$CI.Upper, 3)`\], ER = `r round(e$hypothesis$Evid.Ratio, 3)`. There was strong evidence for no difference between low blurred and clear words, *b* = `r round(f$hypothesis$Estimate, 3)`, 90% CrI \[`r round(f$hypothesis$CI.Lower, 3)`, `r round(f$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(f$hypothesis$Evid.Ratio)`.

```{r}

#Delta plots (one per subject) 
quibble <- function(x, q = seq(.1, .9, .2)) {
  tibble(x = quantile(x, q), q = q)
}

data.quantiles <- rts |>
  dplyr::filter(rt >= .2 | rt <= 2.5) |> 
  dplyr::group_by(participant,blur,corr) |>
  dplyr::filter(lex=="m")|>
  dplyr::summarise(RT = list(quibble(rt, seq(.1, .9, .2)))) |> 
  tidyr::unnest(RT)


data.delta <- data.quantiles |>
  dplyr::filter(corr==1) |>
  dplyr::select(-corr) |>
  dplyr::group_by(participant, blur, q) |>
  dplyr::summarize(RT=mean(x))


#Delta plots (based on vincentiles)
vincentiles <- data.quantiles |>
  dplyr::filter(corr==1) |>
  dplyr::select(-corr) |>
  dplyr::group_by(blur,q) |>
  dplyr::summarize(RT=mean(x)) 

v=vincentiles |>
  dplyr::group_by(blur,q) |>
  dplyr::summarise(MRT=mean(RT))

v <- v |>
  mutate(blur=ifelse(blur=="HB", "High blur", ifelse(blur=="LB", "Low blur", "Clear")))


v$blur<- factor(v$blur, level=c("High blur", "Low blur", "Clear"))


p <- ggplot(v, aes(x = q, y = MRT * 1000, colour = blur, group = blur)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  scale_y_continuous(breaks = seq(500, 1600, 100)) +
  theme(legend.title = element_blank()) +
  coord_cartesian(ylim = c(500, 1600)) +
  scale_x_continuous(breaks = seq(.1, .9, .2)) +
  geom_label_repel(data = v, aes(x = q, y = MRT * 1000, label = round(MRT * 1000, 0)), color = "black", min.segment.length = 0, seed = 42, box.padding = 0.5, size = 10) +
  labs(x = "Quantiles", y = "Response latencies in ms", tag = "A", colour = "Blur Condition") + # Optional tag for labeling
  theme_minimal(base_size = 16) +
  ggokabeito::scale_colour_okabe_ito()
```

```{r}

 v_chb <- v |>
    dplyr::filter(blur=="Clear" | blur=="High blur") |>
    dplyr::group_by(q)|>
     mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> select(-q) |>
   tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
    mutate(diff=`High blur`*1000-Clear*1000)
 
 
p1 <- ggplot(v_chb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme(legend.position = "none") +
    theme_minimal(base_size=16)+
scale_y_continuous(breaks=seq(-10,440,50)) +
  scale_x_continuous(breaks=seq(600,1300, 100))+
  coord_cartesian(xlim=c(600 ,1300), ylim = c(-10, 440)) +
   geom_label_repel(data=v_chb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10)+
  labs( title = "Delta Plots: Clear - High Blur", x = "", y = "Group differences", tag = "B")

```

```{r}

 v_clb <- v |>
    dplyr::filter(blur=="Clear" | blur=="Low blur") |>
    dplyr::group_by(q)|>
     mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> 
   select(-q) |>
   tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
    mutate(diff=`Low blur`*1000-Clear*1000)
 


p2 <- ggplot(v_clb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme_minimal(base_size=16) + 
  theme(legend.position = "none") + 
scale_y_continuous(breaks=seq(-10,440,50)) +
  scale_x_continuous(breaks=seq(600,1300, 100))+
    coord_cartesian(xlim=c(600 ,1300), ylim = c(-10, 440)) +
    geom_label_repel(data=v_clb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10) + 
  labs( title = "Delta Plots: Low Blur - Clear", x = "Mean RT per quantile", y = "")


```

```{r}

v_hlb <- v |>
  dplyr::filter(blur=="High blur" | blur=="Low blur") |>
  dplyr::group_by(q)|>
  mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> 
   select(-q) |>
  tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
  mutate(diff=`High blur`*1000-`Low blur`*1000)


p3 <- ggplot(v_hlb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme_minimal(base_size = 16) + 
  theme(legend.position = "none") + 
  scale_y_continuous(breaks=seq(-10,440,50)) +
    scale_x_continuous(breaks=seq(600,1300, 100))+
    coord_cartesian(xlim=c(600 ,1300), ylim = c(-10, 440)) +
    geom_label_repel(data=v_hlb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10)+ 
  labs( title = "Delta Plots: High Blur - Low Blur", x = "", y = "")

```

```{r}
#| label: fig-deltaquant
#| fig-cap: "Quantile plots for each condition (A) and Delta plots (B) depicting the magnitude of the effect for hypotheses of interest over time in Experiment 1A. Each dot represents the mean RT at the .1, .3, .5, .7 and .9 quantiles."
#| fig-width: 15
#| fig-height: 10

delta_comb = (p1+p2+p3)

combined_plot <- p / delta_comb

combined_plot
 # Automatically labels rows as A, B

ggsave(filename='./figures/figure_kde.png',width=10,height=12)
```

### RTs: DDM

We suppressed intercepts when modeling DDM parameters [see [@diffusio2017] and calculated all comparisons manually.

```{r}

blur_rt_diff<- rts |>
  group_by(participant) |>
  dplyr::filter(rt >= .2 & rt <= 2.5)|>
  dplyr::filter(lex=="m")

```

```{r}
#| eval: false

formula <- bf(rt | dec(corr) ~ 0 + blur + 
                (1 + blur|p|participant) + (1+blur|i|string),  
              ndt ~ 0 + blur + (1 + blur|p|participant) + (1+blur|i|string),
              bias = .5)


bprior <- prior(normal(0, 1), class = b) +
  prior(normal(0, 1), class = b, dpar = ndt)+
  prior(normal(0, 1), class = sd) +
  prior(normal(0, 1), class = sd, dpar = ndt) + 
  prior("normal(0, 0.3)", class = "sd", group = "participant")+ 
  prior("normal(0, 0.3)", class = "sd", group = "string")


```

```{r}
#| eval: false
#| 

make_stancode(formula, 
              family = wiener(link_bs = "identity", 
                              link_ndt = "identity",
                              link_bias = "identity"),
              data = blur_rt_diff, 
              prior = bprior)

tmp_dat <- make_standata(formula, 
                         family = wiener(link_bs = "identity", 
                              link_ndt = "identity",
                              link_bias = "identity"),
                            data = blur_rt_diff, prior = bprior)
str(tmp_dat, 1, give.attr = FALSE)

initfun <- function() {
  list(
    b = rnorm(tmp_dat$K),
    bs=.5, 
    b_ndt = runif(tmp_dat$K_ndt, 0.1, 0.15),
    sd_1 = runif(tmp_dat$M_1, 0.5, 1),
    sd_2 = runif(tmp_dat$M_2, 0.5, 1),
    z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),
                 tmp_dat$M_1, tmp_dat$N_1),
    z_2 = matrix(rnorm(tmp_dat$M_2*tmp_dat$N_2, 0, 0.01),
                 tmp_dat$M_2, tmp_dat$N_2),
    L_1 = diag(tmp_dat$M_1),
    L_2 = diag(tmp_dat$M_2)
  )
}

```

```{r}
#| eval: false

fit_wiener1 <- brm(formula, 
                  data = blur_rt_diff,
                  family = wiener(link_bs = "identity", 
                                  link_ndt = "identity",
                                  link_bias = "identity"),
                  prior = bprior, init=initfun,
                  iter = 2000, warmup = 500, 
                  chains = 4, cores = 4,
                  file="weiner_diff_1", 
                  backend = "cmdstanr", threads = threading(4), 
                  control = list(max_treedepth = 15))


```

```{r}
#diff object on osf
fit_wiener <- read_rds("https://osf.io/hqauz/download")
```

```{r}
#| label: tbl-diffexpt1A
#| tbl-cap: "Summary of posterior distributions for fixed effects in the DDM (Experiment 1A)"
#| apa-note: "95% CrI for equivalency tests"


# Define contrasts using averages and differences
a <- hypothesis(fit_wiener, "blurHB - ((blurLB + blurC / 2)) < 0")
b <- hypothesis(fit_wiener, "blurLB - blurC =  0")

c <- hypothesis(fit_wiener, "ndt_blurHB - ((ndt_blurLB + ndt_blurC)/2) > 0", dpar = "ndt")
d <- hypothesis(fit_wiener, "ndt_blurLB - ndt_blurC >  0", dpar = "ndt")

# Round numeric columns
round_hypothesis <- function(hyp) {
  hyp$hypothesis <- hyp$hypothesis |> mutate(across(where(is.numeric), round, 3))
  hyp
}

a <- round_hypothesis(a)
b <- round_hypothesis(b)
c <- round_hypothesis(c)
d <- round_hypothesis(d)

# Combine into a single tidy table
tab <- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis) |>
  mutate(
    Parameter = c("v", "v", "T_er", "T_er"),
    Hypothesis = c(
      "High Blur > (Low Blur + Clear)",
      "Low Blur  = Clear",
      "High Blur > (Low Blur + Clear)",
      "Low Blur = Clear"
    ),
    `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")
  ) |>
  rename(
    Mean = Estimate,
    SE = Est.Error,
    ER = Evid.Ratio,
    `Posterior Prob` = Post.Prob
  ) |>
  select(Hypothesis, Parameter, Mean, SE, `90% CrI`, ER, `Posterior Prob`)

# Generate flextable
flextable(tab) |>
  theme_apa()

```

A summary of the diffusion model results can be found in @tbl-diffexpt1A. There was strong evidence that high blurred words were associated with a lower drift rate than both clear and low blurred words, *b* = `r a$hypothesis$Estimate`, 90% CrI \[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`. In contrast, the evidence supported the absence of a drift rate difference between low blurred and clear words, *b* = `r b$hypothesis$Estimate`, 90% CrI \[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`.

There was also substantial evidence that non-decision time was greater for high blurred words compared to both clear and low blurred words, *b* = `r c$hypothesis$Estimate`, 90% CrI \[`r c$hypothesis$CI.Lower`, `r c$hypothesis$CI.Upper`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`. Additionally, the posterior distribution indicated that low blurred words had a longer non-decision time than clear words, *b* = `r d$hypothesis$Estimate`, 90% CrI \[`r d$hypothesis$CI.Lower`, `r d$hypothesis$CI.Upper`\], ER = `r process_ER_column(d$hypothesis$Evid.Ratio)`.

### Recognition Memory

```{r}

mem_c <- read_csv("https://osf.io/xjvc7/download")
```

```{r}
## Contrasts
#hypothesis

contrast_matrix <- matrix(
  c(-0.5,  0,    # C
     0.5,  0.5,  # HB
     0,   -0.5), # LB
  ncol = 2,
  byrow = TRUE
)

colnames(contrast_matrix) <- c("blur1", "blur2")
rownames(contrast_matrix) <- c("C", "HB", "LB")

# Apply to factor
mem_c$blur <- factor(mem_c$blur, levels = c("C", "HB", "LB"))
contrasts(mem_c$blur) <- contrast_matrix
```

```{r}
#| eval: false 
#| 
#priors for sdt model
sdt_priors <- c(set_prior("cauchy(0,0.35)", class = "b"))
```

```{r}
#| eval: false
#| 

fit_mem_c <- brm(sayold ~ isold * blur + (1 + isold * blur | participant) + (1 + isold * blur | string),
  data = mem_c,
  warmup = 1000,
  iter = 5000,
  chains = 4,
  init = 0,
  family = bernoulli(link = "probit"),
  cores = 4,
  control = list(adapt_delta = 0.9),
  prior = sdt_priors,
  sample_prior = T,
  seed=666, 
  file = "blmm_sdt_expt1",
  save_pars = save_pars(all = T),
  backend = "cmdstanr",
  threads = threading(2)
)

```

```{r}

fit_mem1 <- read_rds("https://osf.io/dbfwa/download")


```

```{r}
#HB > C
a <- hypothesis(fit_mem1 , "isold1:blur1  > 0")
b <- hypothesis(fit_mem1 , "isold1:blur2 > 0")

a$hypothesis <- a$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
b$hypothesis <- b$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))

```

#### Sensitivity

@fig-dprimeexp1A highlights $D'$ and $c$ means and comparisons across all groups. high blurred words were better remembered than clear and low blurred words words, $\beta$ = `r a$hypothesis$Estimate`, 90% CrI \[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`. The evidence for no difference in sensitivity between clear words and low blurred words was strong, $\beta$ = `r b$hypothesis$Estimate`, 90% CrI \[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`.

#### Exploratory

#### Bias

```{r}
a <- hypothesis(fit_mem1 , "blur1 < 0")
b <- hypothesis(fit_mem1, "blur2 < 0")

a$hypothesis <- a$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
b$hypothesis <- b$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))

```

Low blurred words had a bias towards more "old" responses compared to clear words, $\beta$ = `r b$hypothesis$Estimate`, 90% CrI\[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`, and high blurred words, $\beta$ = `r a$hypothesis$Estimate`, 90% CrI\[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`. High blurred words showed a more liberal bias compared to clear and low blurred words, $\beta$ = `r b$hypothesis$Estimate`, 90% CrI\[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`.

```{r}
# (Negative) criteria
emm_m1_c1 <- emmeans(fit_mem1, ~blur) |> 
  parameters::parameters(centrality = "mean")
# Differences in (negative) criteria
emm_m1_c2 <- emmeans(fit_mem1, ~blur) |> 
  contrast("pairwise") |> 
  parameters::parameters(centrality = "mean")

# Dprimes for three groups
emm_m1_d1 <- emmeans(fit_mem1, ~isold + blur) |> 
  contrast("revpairwise", by = "blur") |> 
  parameters::parameters(centrality = "mean")
# Differences between groups
emm_m1_d2 <- emmeans(fit_mem1, ~isold + blur) |> 
  contrast(interaction = c("revpairwise", "pairwise")) |> 
  parameters::parameters(centrality = "mean")

```

```{r}
#| label: fig-dprimeexp1A
#| fig-cap: Estimated posterior distributions for d-prime and criterion, and differences, with 95% CrIs (thin lines)
#| fig-width: 12
#| fig-height: 8

emm_m1_c1 <- emmeans(fit_mem1, ~blur) 

emm_m1_c2 <- emmeans(fit_mem1, ~blur) |> 
  contrast("pairwise")

# Dprimes for three groups
emm_m1_d1 <- emmeans(fit_mem1, ~isold + blur) |> 
  contrast("revpairwise", by = "blur")
# Differences between groups
emm_m1_d2 <- emmeans(fit_mem1, ~isold + blur) |> 
  contrast(interaction = c("revpairwise", "pairwise")) 

tmp <- bind_rows(
  bind_rows(
    gather_emmeans_draws(emm_m1_d1) |> 
      group_by(blur) |> 
      select(-contrast),
    gather_emmeans_draws(emm_m1_d2) |> 
      rename(
        blur = blur_pairwise
      ) |> 
      group_by(blur) |> 
      select(-isold_revpairwise)
  ),
  bind_rows(
    gather_emmeans_draws(emm_m1_c1),
    gather_emmeans_draws(emm_m1_c2) |> 
      rename(
        blur = contrast
      )
  ),
  .id = "Parameter"
) |> 
  mutate(Parameter = factor(Parameter, labels = c("d-prime", "Criterion"))) |> 
  mutate(
    t = if_else(str_detect(blur, " - "), "Differences", "Group means") |> 
      fct_inorder(),
    blur = fct_inorder(blur)
  )

tmp |>   
  ungroup() |> 
  mutate(.value = if_else(Parameter == "Criterion", .value * -1, .value)) |> 
  mutate(Parameter = fct_rev(Parameter)) |> 
  mutate(blur=case_when(
  blur=="C"~ "Clear", 
  blur=="C - HB" ~ "Clear - High Blur", 
  blur=="HB" ~ "High Blur", 
  blur=="LB" ~ "Low Blur", 
  blur=="C - LB" ~ "Clear - Low Blur", 
  TRUE ~ "High Blur - Low Blur"
  )) |> 
  ggplot(aes(blur, .value)) +
  labs(
    x = "Blurring Level (or difference)",
    y = "Parameter value", 
  ) +
  stat_halfeye(colour="black", 
               .width = c(0.95), point_interval = "mean_qi") + 
    facet_grid(Parameter~t, scales = "free") + 
    geom_hline(yintercept = 0, linewidth = .25) + 
  theme_minimal(base_size=16) + 
  theme(
    strip.background = element_rect(fill = "black", color = "black"),
    strip.text = element_text(color = "white", face = "bold")
  )

ggsave("expt1A-dprime.png", width=19, height=8, dpi=300)


```

## Discussion

Experiment 1A successfully replicated the pattern of results found in @rosner2015. Specifically, we found high blurred words had lower accuracy than clear and low blurred words, but had better memory.

Adding to this, we utilized cognitive and mathematical modeling to gain further insights into the mechanisms underlying the perceptual disfluency effect. Descriptively, high blurred words induced a more pronounced shift in the RT distribution ($\mu$) and exhibited a higher degree of skew ($\beta$) compared to clear and low blurred words. However, low blurred words did not differ compared to clear words on $\mu$ or $\beta$. These patterns can be clearly seen in our quantile delta plots in Fig. 3.

We also fit the RTs and accuracy data to a diffusion model, which allowed us to make stronger inferences as it relates to stages of processing. High blurred words impacted both an early, non-decision, component evinced by higher $T_{er}$ and a later more analytic, component evinced by a lower $v$ than clear or low blurred words. On the other hand, low blurred words only affected $T_{er}$.

We present evidence that different levels of disfluency can influence distinct stages of encoding, potentially contributing to the presence or absence of a mnemonic effect for perceptually blurred stimuli. Unlike most studies that commonly employ a single level of disfluency, our study incorporated two levels of disfluency. The results indicate that a subtle manipulation such as low blur primarily affects early processing stages, whereas a more pronounced perceptual manipulation (i.e., high blur) impacts both early and late processing stages. Regarding recognition memory, high blurred stimuli were better recognized compared to low blurred and clear words. This suggests that in order to observe a perceptual disfluency effect, the perceptual manipulation must be sufficiently disfluent to do so and tap later stages of encoding.

Given the important theoretical implications of these findings, Experiment 1B served as a conceptual replication. Due to the bias observed in the recognition memory test (i.e., low blurred words were responded to more liberally), we will not present old and new items as blurred, instead all of the words will be presented in a clear, different, font at test.

# Experiment 1B

## Method

### Transparency and Openness

This study was not preregistered. All raw and summary data, materials, and R scripts for pre-processing, analysis, and plotting for Experiment 1B can be found at https://osf.io/6sy7k/.

### Participants

We used the same sample size procedure as in Experiment 1A (*N* = 216). All participants were recruited through the Rutgers University subject pool (SONA system). The exclusion criteria were the same as in Experiment 1A.

### Apparatus, Stimuli, Design, Procedure, and Analysis

Similar to Experiment 1A, the experiment was run using PsychoPy [@peirce2019] and hosted on Pavlovia (www.pavlovia.org). You can see an example of the experiment by navigating to this website: https://run.pavlovia.org/Jgeller112/ldt_dd_l1_jol.

We used the same stimuli from Experiment 1A. The main difference between Experiment 1A and 1B was all items were presented in a clear, Arial font. To make it more similar to Experiment 1A each set of words presented as clear, low blur, and high blur at study were yoked to a set of new words that were counterbalanced across lists. Therefore, instead of there being one false alarm rate there were 3, one for each blurring level. This ensured each word was compared to studied clear, studied high blurred, and studied low blurred words.

The same model specifications and analyses used in Experiment 1A were used in Experiment 1B.

## Results

### Accuracy

```{r}

# get data from osf
blur_acc <- read_csv("https://osf.io/excgd/download") |>
    dplyr::filter(lex=="m")


blur_acc_new<- blur_acc |>
  dplyr::filter(rt >= .2 & rt <= 2.5)

```

The analysis of accuracy is is based on `r dim(blur_acc)[1]` data points. After removing fast and slow RTs we were left with `r dim(blur_acc_new)[1]` data point (`r round(1-dim(blur_acc_new)[1]/dim(blur_acc)[1], 3)`)

```{r}

# Create custom contrast matrix manually
contrast_matrix <- matrix(
  c(-.5,  -.5,   # C
     .5,  0,   # HB
     -.5, .5),  # LB
  ncol = 2,
  byrow = TRUE
)


blur_acc_new$blur <- as.factor(blur_acc_new$blur)

# Assign to factor
contrasts(blur_acc_new$blur) <- contrast_matrix


```

```{r}
#| eval: false

#weak prior
prior_exp1 <- c(set_prior("cauchy(0,.35)", class = "b"))

#fit model
fit_acc_weak <- brm(corr ~ blur + (1+blur|participant) + (1+blur|string), data=blur_acc_new, 
warmup = 1000,
                    iter = 5000,
                    chains = 4, 
                    init=0, 
                    family = bernoulli(),
     cores = 4,
prior = prior_exp1, 
control = list(adapt_delta = 0.9), 
backend="cmdstanr", 
save_pars = save_pars(all=T),
sample_prior = T, 
threads = threading(4), 
file="fit_acc_weak_nocontext")

```

```{r}

# get file from osf
fit_acc_noc <- read_rds("https://osf.io/p9xat/download")

```

```{r}

acc_means <- emmeans(fit_acc_noc, specs="blur", type="response") 
```

```{r}
#| label: tbl-acc1B
#| tbl-cap: "Summary of posterior distributions for fixed effects in the Bayesian accuracy model (Experiment 1B)"
#| apa-note: "95% CrI for equivalency tests. Posterior probability indicates the proportion of the posterior distribution that falls on one side of zero (either positive or negative), representing the probability that the effect is greater than or less than zero."

# Run hypotheses
a <- hypothesis(fit_acc_noc, "blur1 < 0")  # High Blur < (Low Blur + Clear)/2
b <- hypothesis(fit_acc_noc, "blur2 = 0")  # Low Blur < Clear

# Round numeric outputs
round_hypothesis <- function(hyp) {
  hyp$hypothesis <- hyp$hypothesis |> mutate(across(where(is.numeric), round, 3))
  hyp
}

a <- round_hypothesis(a)
b <- round_hypothesis(b)

# Combine and format table
tab <- bind_rows(a$hypothesis, b$hypothesis) |>
  mutate(
    Hypothesis = c(
      "High Blur < (Low Blur + Clear)",
      "Low Blur = Clear"
    ),
    `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")
  ) |>
  rename(
    Mean = Estimate,
    SE = Est.Error,
    ER = Evid.Ratio,
    `Posterior Prob` = Post.Prob
  ) |>
  select(Hypothesis, Mean, SE, `90% CrI`, ER, `Posterior Prob`)

# Create the flextable
acc_summary1B <- flextable(tab) |>
  theme_apa()

acc_summary1B


```

A summary of posterior estimates are located in @tbl-acc1B. High blur words had lower accuracy compared to clear and low blurred words, $b$ = `r a$hypothesis$Estimate`, 90% CrI\[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`. However, the evidence was weak for there being no significant difference in the identification accuracy between clear and low blurred words, $b$ = `r b$hypothesis$Estimate`, 90% CrI\[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`.

### RTs: Ex-Gaussian

```{r}
#load data from osf
rts <- read_csv("https://osf.io/excgd/download")
```

```{r}

blur_rt<- rts |>
  group_by(participant) |>
   dplyr::filter(corr==1, lex=="m")#only include nonwords

blur_rt_new <- blur_rt |> 
  dplyr::filter(rt >= .2 & rt <= 2.5) |>
  mutate(rt_ms=rt*1000)

```

The analysis of RTs (correct trials and word stimuli) is based on `r dim(blur_rt_new)[1]` data points, after removing fast and slow RTs (`r round(1-dim(blur_rt_new)[1]/dim(blur_rt)[1], 3)`)

```{r}
#hypothesis
blurC <-hypr(HB~C, HB~LB, levels=c("C", "HB", "LB"))

#set contrasts in df 
blur_rt$blur <- as.factor(blur_rt$blur)

contrasts(blur_rt$blur) <-contr.hypothesis(blurC)

```

```{r}
#| eval: false

bform_exg1B <- bf(
rt ~ 0+ blur + (1 + blur |p| participant) + (1 + blur|i| string),
sigma ~ 0+ blur + (1 + blur |p|participant) + (1 + blur |i| string),
beta ~ 0 + blur + (1 + blur |p|participant) + (1 + blur |i| string))
```

```{r}
#| eval: false
#| 
prior1B <- c(prior("normal(0,10)", class = "b", coef = ""))               

fit_exg1B <- brm(
  bform_exg1B, data = blur_rt_new,
  warmup = 1000,
  iter = 5000,
  prior = prior1B,
  family = three_param_exgauss,
  stanvars = three_param_exg_stanvars,
  init = 0,
  cores = 4, 
  file="mod_exgauss_1B", 
  chains=4, 
  seed=666, 
  sample_prior = T, 
  save_pars = save_pars(all=T),
  control = list(adapt_delta = 0.8), 
  backend="cmdstanr", 
  threads = threading(2))

```

```{r}
fit_exg1B <- read_rds("https://osf.io/ajwe4/download")

```

```{r}
#| label: tbl-exgauss1B
#| tbl-cap: "Summary of posterior distributions for fixed effects in the ex-Gaussian model (Experiment 1B)."
#| apa-note: "95% CrI for equivalency tests. Posterior probability indicates the proportion of the posterior distribution that falls on one side of zero (either positive or negative), representing the probability that the effect is greater than or less than zero. Sigma and Beta parameters are on the log (logit) scale." 


# Define hypothesis tests
a <- hypothesis(fit_exg1B, "blur1 > 0")
b <- hypothesis(fit_exg1B, "blur2 > 0")

c <- hypothesis(fit_exg1B, "sigma_blur1 > 0")
d <- hypothesis(fit_exg1B, "sigma_blur2 = 0")

e <- hypothesis(fit_exg1B, "beta_blur1 > 0")
f <- hypothesis(fit_exg1B, "beta_blur2 = 0")
 
# Round all numeric columns in each result
round_hypothesis <- function(hyp) {
  hyp$hypothesis <- hyp$hypothesis |> mutate(across(where(is.numeric), round, 3))
  hyp
}

a <- round_hypothesis(a)
b <- round_hypothesis(b)
c <- round_hypothesis(c)
d <- round_hypothesis(d)
e <- round_hypothesis(e)
f <- round_hypothesis(f)

# Combine into one table
tab <- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis) |> 
  mutate(
    Parameter = c("Mu", "Mu", "Sigma", "Sigma", "Beta", "Beta"),
    Hypothesis = c(
      "High Blur > (Low Blur + Clear)",
      "Low Blur > Clear",
      "High Blur > (Low Blur + Clear)",
      "Low Blur = Clear",
      "High Blur > (Low Blur + Clear)",
      "Low Blur = Clear"
    ),
    `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")
  ) |>
  rename(
    Mean = Estimate,
    SE = Est.Error,
    ER = Evid.Ratio,
    `Posterior Prob` = Post.Prob
  ) |>
  select(Hypothesis, Parameter, Mean, SE, `90% CrI`, ER, `Posterior Prob`)

# Create flextable
flextable(tab) |>
  theme_apa()
```

A visualization of how blurring affected processing during word recognition can be seen in the quantile and delta plots in @fig-deltaquant1B. Beginning with the $\mu$ parameter, there was greater shifting for high blurred words compared to clear and low blurred words, *b* = `r round(a$hypothesis$Estimate, 3)`, 90% CrI \[`r round(a$hypothesis$CI.Lower, 3)`, `r round(a$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`. Low blurred words had greater shifting compared to clear words, *b* = `r round(b$hypothesis$Estimate, 3)`, 90% CrI \[`r round(b$hypothesis$CI.Lower, 3)`, `r round(b$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`.

Analyses of the $\sigma$ parameter yielded a similar pattern. Variance was higher for high blurred words compared to clear and low blurred words, *b* = `r round(c$hypothesis$Estimate, 3)`, 90% CrI \[`r round(c$hypothesis$CI.Lower, 3)`, `r round(c$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`. There was no evidence of a difference in variance between low blurred and clear words,*b* = `r round(d$hypothesis$Estimate, 3)`, 90% CrI \[`r round(d$hypothesis$CI.Lower, 3)`, `r round(d$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(d$hypothesis$Evid.Ratio)`

Finally, there was greater skewing for high blurred words compared to clear and low blurred words, *b* = `r round(e$hypothesis$Estimate, 3)`, 90% CrI \[`r round(e$hypothesis$CI.Lower, 3)`, `r round(e$hypothesis$CI.Upper, 3)`\], ER = `r round(e$hypothesis$Evid.Ratio, 3)`. There was no evidence of a difference in variance between low blurred and clear words,*b* = `r round(f$hypothesis$Estimate, 3)`, 90% CrI \[`r round(f$hypothesis$CI.Lower, 3)`, `r round(f$hypothesis$CI.Upper, 3)`\], ER = `r process_ER_column(f$hypothesis$Evid.Ratio)`.

### RTs: DDM 

```{r}

blur_rt_diff<- rts |>
  group_by(participant) |>
  dplyr::filter(rt >= .2 & rt <= 2.5)|>
  dplyr::filter(lex=="m")

```

```{r}
formula <- bf(rt | dec(corr) ~ 0 + blur + 
                (1 + blur|p|participant) + (1+blur|i|string),  
              ndt ~ 0 + blur + (1 + blur|p|participant) + (1+blur|i|string),
              bias =.5)

bprior <- prior(normal(0, 1), class = b) +
  prior(normal(0, 1), class = b, dpar = ndt)+
  prior(normal(0, 1), class = sd) +
  prior(normal(0, 1), class = sd, dpar = ndt) + 
  prior("normal(0, 0.3)", class = "sd", group = "participant")+ 
  prior("normal(0, 0.3)", class = "sd", group = "string")


```

```{r}
#| eval: false

make_stancode(formula, 
              family = wiener(link_bs = "identity", 
                              link_ndt = "identity",
                              link_bias = "identity"),
              data = blur_rt_diff, 
              prior = bprior)

tmp_dat <- make_standata(formula, 
                         family = wiener(link_bs = "identity", 
                              link_ndt = "identity",
                              link_bias = "identity"),
                            data = blur_rt_diff, prior = bprior)
str(tmp_dat, 1, give.attr = FALSE)

initfun <- function() {
  list(
    b = rnorm(tmp_dat$K),
    bs=.5, 
    b_ndt = runif(tmp_dat$K_ndt, 0.1, 0.15),
    sd_1 = runif(tmp_dat$M_1, 0.5, 1),
    sd_2 = runif(tmp_dat$M_2, 0.5, 1),
    z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),
                 tmp_dat$M_1, tmp_dat$N_1),
    z_2 = matrix(rnorm(tmp_dat$M_2*tmp_dat$N_2, 0, 0.01),
                 tmp_dat$M_2, tmp_dat$N_2),
    L_1 = diag(tmp_dat$M_1),
    L_2 = diag(tmp_dat$M_2)
  )
}

```

```{r}
#| eval: false
#| 

fit_wiener1 <- brm(formula, 
                  data = blur_rt_diff,
                  family = wiener(link_bs = "identity", 
                                  link_ndt = "identity",
                                  link_bias = "identity"),
                  prior = bprior, init=initfun,
                  iter = 2000, warmup = 500, 
                  chains = 4, cores = 4,
                  file="weiner_diff_1", 
                  backend = "cmdstanr", threads = threading(4), 
                  control = list(max_treedepth = 15))


```

```{r}

fit_wiener <- read_rds("https://osf.io/gy5ab/download")

```

```{r}
#| label: tbl-diffexpt1B
#| tbl-cap: "Summary of posterior distributions for fixed effects in the DDM (Experiment 1B)"
#| apa-note:  "95% CrI for equivalency tests. Posterior probability indicates the proportion of the posterior distribution that falls on one side of zero (either positive or negative), representing the probability that the effect is greater than or less than zero."


# Define contrasts using averages and differences
a <- hypothesis(fit_wiener, "blurHB - ((blurLB + blurC)/2) < 0")
b <- hypothesis(fit_wiener, "blurLB - blurC = 0")

c <- hypothesis(fit_wiener, "ndt_blurHB - ((ndt_blurLB + ndt_blurC)/2) > 0", dpar = "ndt")
d <- hypothesis(fit_wiener, "ndt_blurLB - ndt_blurC > 0", dpar = "ndt")

# Round numeric columns
round_hypothesis <- function(hyp) {
  hyp$hypothesis <- hyp$hypothesis |> mutate(across(where(is.numeric), round, 3))
  hyp
}

a <- round_hypothesis(a)
b <- round_hypothesis(b)
c <- round_hypothesis(c)
d <- round_hypothesis(d)

# Combine into a single tidy table
tab <- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis) |>
  mutate(
    Parameter = c("v", "v", "T_er", "T_er"),
    Hypothesis = c(
      "High Blur < (Low Blur + Clear)",
      "Low Blur = Clear",
      "High Blur > (Low Blur + Clear)",
      "Low Blur = Clear"
    ),
    `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")
  ) |>
  rename(
    Mean = Estimate,
    SE = Est.Error,
    ER = Evid.Ratio,
    `Posterior Prob` = Post.Prob
  ) |>
  select(Hypothesis, Parameter, Mean, SE, `90% CrI`, ER, `Posterior Prob`)

# Generate flextable
flextable(tab) |>
  theme_apa()



```

A summary of the diffusion model results is presented in @tbl-diffexpt1B. There was strong evidence that high blurred words were associated with a lower drift rate than both clear and low blurred words, *b* = `r a$hypothesis$Estimate`, 90% CrI \[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`. In contrast, the evidence supported the absence of a drift rate difference between low blurred and clear words, *b* = `r b$hypothesis$Estimate`, 90% CrI \[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`.

There was also substantial evidence that non-decision time was greater for high blurred words compared to both clear and low blurred words, *b* = `r c$hypothesis$Estimate`, 90% CrI \[`r c$hypothesis$CI.Lower`, `r c$hypothesis$CI.Upper`\], ER = `r process_ER_column(c$hypothesis$Evid.Ratio)`. Additionally, the posterior distribution indicated that low blurred words had a longer non-decision time than clear words, *b* = `r d$hypothesis$Estimate`, 90% CrI \[`r d$hypothesis$CI.Lower`, `r d$hypothesis$CI.Upper`\], ER = `r process_ER_column(d$hypothesis$Evid.Ratio)`.

```{r}

#Delta plots (one per subject) 
quibble <- function(x, q = seq(.1, .9, .2)) {
  tibble(x = quantile(x, q), q = q)
}

data.quantiles <- rts |>
  dplyr::filter(rt >= .2 | rt <= 2.5) |> 
  dplyr::group_by(participant,blur,corr) |>
  dplyr::filter(lex=="m")|>
  dplyr::summarise(RT = list(quibble(rt, seq(.1, .9, .2)))) |> 
  tidyr::unnest(RT)


data.delta <- data.quantiles |>
  dplyr::filter(corr==1) |>
  dplyr::select(-corr) |>
  dplyr::group_by(participant, blur, q) |>
  dplyr::summarize(RT=mean(x))


#Delta plots (based on vincentiles)
vincentiles <- data.quantiles |>
  dplyr::filter(corr==1) |>
  dplyr::select(-corr) |>
  dplyr::group_by(blur,q) |>
  dplyr::summarize(RT=mean(x)) 

v=vincentiles |>
  dplyr::group_by(blur,q) |>
  dplyr::summarise(MRT=mean(RT))

v <- v |>
  mutate(blur=ifelse(blur=="HB", "High blur", ifelse(blur=="LB", "Low blur", "Clear")))


v$blur<- factor(v$blur, level=c("High blur", "Low blur", "Clear"))



p <- ggplot(v, aes(x = q, y = MRT * 1000, colour = blur, group = blur)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  scale_y_continuous(breaks = seq(500, 1600, 100)) +
  theme(legend.title = element_blank()) +
  coord_cartesian(ylim = c(500, 1600)) +
  scale_x_continuous(breaks = seq(.1, .9, .2)) +
  geom_label_repel(data = v, aes(x = q, y = MRT * 1000, label = round(MRT * 1000, 0)), color = "black", min.segment.length = 0, seed = 42, box.padding = 0.5, size = 10) +
  labs(x = "Quantiles", y = "Response latencies in ms", tag = "A", colour='Blur Condition') + # Optional tag for labeling
  theme_minimal(base_size = 16) +
  ggokabeito::scale_colour_okabe_ito()
```

```{r}

 v_chb <- v |>
    dplyr::filter(blur=="Clear" | blur=="High blur") |>
    dplyr::group_by(q)|>
     mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> select(-q) |>
   tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
    mutate(diff=`High blur`*1000-Clear*1000)
 
 
p1 <- ggplot(v_chb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme(legend.position = "none") +
    theme_minimal(base_size=16)+
scale_y_continuous(breaks=seq(10,440,50)) +
    coord_cartesian(xlim = c(600, 1300), ylim = c(10, 440)) +
  scale_x_continuous(breaks=seq(600,1300, 100))+
   geom_label_repel(data=v_chb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10)+
  labs( title = "Delta Plots: Clear - High Blur", x = "", y = "Group differences", tag = "B")

```

```{r}


 v_clb <- v |>
    dplyr::filter(blur=="Clear" | blur=="Low blur") |>
    dplyr::group_by(q)|>
     mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> 
   select(-q) |>
   tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
    mutate(diff=`Low blur`*1000-Clear*1000)
 


p2 <- ggplot(v_clb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme_minimal(base_size=16) + 
  theme(legend.position = "none") + 
scale_y_continuous(breaks=seq(10,440,50)) +
  scale_x_continuous(breaks=seq(600,1300, 100))+
      coord_cartesian(xlim = c(600, 1300), ylim = c(10, 440)) +
    geom_label_repel(data=v_clb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10) + 
  labs( title = "Delta Plots: Low Blur - Clear", x = "Mean RT per quantile", y = "")


```

```{r}

v_hlb <- v |>
  dplyr::filter(blur=="High blur" | blur=="Low blur") |>
  dplyr::group_by(q)|>
  mutate(mean_rt = mean(MRT)*1000) |>
     ungroup() |> 
   select(-q) |>
  tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
  mutate(diff=`High blur`*1000-`Low blur`*1000)


p3 <- ggplot(v_hlb, aes(x = mean_rt, y = diff)) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme_minimal(base_size = 16) + 
  theme(legend.position = "none") + 
  scale_y_continuous(breaks=seq(10,440,50)) +
  scale_x_continuous(breaks=seq(600,1300, 100))+
  coord_cartesian(xlim = c(600, 1300), ylim = c(10, 440)) +
    geom_label_repel(data=v_hlb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5, size=10)+ 
  labs( title = "Delta Plots: High Blur - Low Blur", x = "", y = "")

```

```{r}
#| label: fig-deltaquant1B
#| fig-cap: "Quantile plots for each condition (A) and Delta plots (B) depicting the magnitude of the effect for hypotheses of interest over time in Experiment 1B. Each dot represents the mean RT at the .1, .3, .5, .7 and .9 quantiles."
#| fig-width: 15
#| fig-height: 10

delta_comb = (p1+p2+p3)

combined_plot <- p / delta_comb


ggsave(filename='./figures/figure_kde2.png',width=12,height=12)

combined_plot
 # Automatically labels rows as A, B
```

### Recognition Memory

```{r}

mem_nc <- read_csv("https://osf.io/jw2gx/download")

```

```{r}
## Contrasts
#hypothesis

contrast_matrix <- matrix(
  c(-0.5,  -.5,    # C
     0.5,  0,  # HB
     -0.5,  0.5), # LB
  ncol = 2,
  byrow = TRUE
)

# Apply to factor
mem_nc$blur <- factor(mem_nc$blur, levels = c("C", "HB", "LB"))
contrasts(mem_nc$blur) <- contrast_matrix


mem_nc$isold <- factor(mem_nc$isold, levels = c(0, 1))  # ensure proper level order
contrasts(mem_nc$isold) <- matrix(c(-0.5, 0.5), ncol = 1)

# Apply to factor
mem_nc$blur <- factor(mem_nc$blur, levels = c("C", "HB", "LB"))
contrasts(mem_nc$blur) <- contrast_matrix


mem_nc$isold <- factor(mem_nc$isold, levels = c(0, 1))  # ensure proper level order
contrasts(mem_nc$isold) <- matrix(c(-0.5, 0.5), ncol = 1)
```

```{r}
#| eval: false
#| 

# weak prior
sdt_priors <- c(set_prior("cauchy(0,.35)", class = "b"))

fit_mem_nc <- brm(sayold ~ isold*blur + (1+isold*blur|participant) + (1+isold*blur|string), data=mem_nc, 
warmup = 1000,
                    iter = 5000,
                    chains = 4, 
                    init=0, 
                    family = bernoulli(link = "probit"),
                    cores = 4, 
control = list(adapt_delta = 0.9),
prior=sdt_priors, 
seed=666, 
sample_prior = T, 
file="blmm_sdt_noc",
save_pars = save_pars(all=T),
backend="cmdstanr",
threads = threading(2))
```

```{r}

fit_mem_noc <- read_rds("https://osf.io/yfzke/download")

```

```{r}
#HB > C
a <- hypothesis(fit_mem_noc , "isold1:blur1 > 0")
b <- hypothesis(fit_mem_noc , "isold1:blur2 = 0")

a$hypothesis <- a$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
b$hypothesis <- b$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
c$hypothesis <- c$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))

```

#### Sensitivity

@fig-dprimeexp1B highlights $D'$ and $c$ means and comparisons across all groups. high blurred words were better remembered than clear and low blurred words words, $\beta$ = `r a$hypothesis$Estimate`, 90% CrI \[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`. The evidence for no difference in sensitivity between clear words and low blurred words was strong, $\beta$ = `r b$hypothesis$Estimate`, 90% CrI \[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`.

```{r}
# (Negative) criteria
emm_m1_c1 <- emmeans(fit_mem_noc, ~blur) |> 
  parameters::parameters(centrality = "mean")
# Differences in (negative) criteria
emm_m1_c2 <- emmeans(fit_mem_noc, ~blur) |> 
  contrast("pairwise") |> 
  parameters::parameters(centrality = "mean")

# Dprimes for three groups
emm_m1_d1 <- emmeans(fit_mem_noc, ~isold + blur) |> 
  contrast("revpairwise", by = "blur") |> 
  parameters::parameters(centrality = "mean")
# Differences between groups
emm_m1_d2 <- emmeans(fit_mem_noc, ~isold + blur) |> 
  contrast(interaction = c("revpairwise", "pairwise")) |> 
  parameters::parameters(centrality = "mean")

```

```{r}
#| label: fig-dprimeexp1B
#| fig-cap: Estimated posterior distributions (mean) for d-prime and criterion, and differences, with 95%  CrIs (thin lines)
#| fig-width: 12
#| fig-height: 8


emm_m1_c1 <- emmeans(fit_mem_noc, ~blur) 

  
emm_m1_c2 <- emmeans(fit_mem_noc, ~blur) |> 
  contrast("pairwise")

# Dprimes for three groups
emm_m1_d1 <- emmeans(fit_mem_noc, ~isold + blur) |> 
  contrast("revpairwise", by = "blur")
# Differences between groups
emm_m1_d2 <- emmeans(fit_mem_noc, ~isold + blur) |> 
  contrast(interaction = c("revpairwise", "pairwise")) 


tmp <- bind_rows(
  bind_rows(
    gather_emmeans_draws(emm_m1_d1) |> 
      group_by(blur) |> 
      select(-contrast),
    gather_emmeans_draws(emm_m1_d2) |> 
      rename(
        blur = blur_pairwise
      ) |> 
      group_by(blur) |> 
      select(-isold_revpairwise)
  ),
  bind_rows(
    gather_emmeans_draws(emm_m1_c1),
    gather_emmeans_draws(emm_m1_c2) |> 
      rename(
        blur = contrast
      )
  ),
  .id = "Parameter"
) |> 
  mutate(Parameter = factor(Parameter, labels = c("d-prime", "Criterion"))) |> 
  mutate(
    t = if_else(str_detect(blur, " - "), "Differences", "Group means") |> 
      fct_inorder(),
    blur = fct_inorder(blur)
  )

tmp |>   
  ungroup() |> 
  mutate(.value = if_else(Parameter == "Criterion", .value * -1, .value)) |> 
  mutate(Parameter = fct_rev(Parameter)) |> 
  mutate(blur=case_when(
  blur=="C"~ "Clear", 
  blur=="C - HB" ~ "Clear - High blur", 
  blur=="HB" ~ "High blur", 
  blur=="LB" ~ "Low blur", 
  blur=="C - LB" ~ "Clear - Low blur", 
  TRUE ~ "High blur - Low blur"
  )) |> 
  ggplot(aes(blur, .value)) +
  labs(
    x = "Blurring Level (or difference)",
    y = "Parameter value", 
  ) +
  stat_halfeye(colour="black", .width = c(0.95), point_interval = "mean_qi") + 
    facet_grid(Parameter~t, scales = "free") + 
    geom_hline(yintercept = 0, linewidth = .25) + 
  theme_minimal(base_size=16) + 
   theme(
    strip.background = element_rect(fill = "black", color = "black"),
    strip.text = element_text(color = "white", face = "bold")
  )

ggsave("expt1B-dprime.png", width=19, height=8, dpi=300)

```

## Discussion

We replicated all the findings of Experiment 1A, with no context reinstatement.

# Experiment 2

In Experiments 1A and 1B, we applied mathematical modeling and distributional analyses to investigate how visual blurring influences encoding and recognition memory. high blurred words affected both early and late processing stages, as evidenced by reduced drift rates ($v$), elevated non-decision times ($T_{er}$), and changes to the shape of the response time distribution (i.e., both shifts and increased skew). In contrast, low blurred words—when compared to clear words—appeared to influence only early-stage processing, primarily through increased $T_{er}$ and distributional shifts, without affecting the drift rate.

Recognition memory also reflected this dissociation: sensitivity was greater for high blurred words than for either clear or low blurred words. Taken together, these findings support the stage-specific account, wherein the disfluency effect comprises an early, automatic or non-analytic process, and a later, analytic component. The precise locus of the late-stage benefit remains unclear, however. To explore whether this effect arises from increased attentional engagement or from control processes that operate in parallel with word recognition mechanisms, we further tested the conditions under which this memory enhancement emerges.

One way to more directly test the stage-specific account is to identify a higher-level linguistic variable known to influence word recognition and assess its interaction with blurring. Word frequency is an ideal candidate. Numerous models of word recognition propose that the speed and ease of lexical access varies systematically with word frequency [@coltheart2001; @mcclelland1981]. Analyses of response time distributions have shown that word frequency affects both early and late stages of processing, with low-frequency words producing larger shifts and greater skew in the RT distribution compared to high-frequency words [@andrews2001; @balota1999; @plourde1997; @staub2010; @yap2007; but see @gomez2014 for a DDM perspective].

In recognition memory, low-frequency words are generally remembered better than high-frequency words [@glanzer1985]. This advantage has been attributed to the increased cognitive effort or attentional resources required to encode low-frequency items [@diana2006], a view referred to as the elevated attention hypothesis [@malmberg2003; but see @pazzaglia2014 for alternative perspectives].

Critically, in tasks such as semantic categorization and pronunciation, word frequency has been shown to interact with stimulus degradation, yielding over-additive effects [@yap2007]. According to the logic of additive factors [@sternberg1969], such interactions suggest that the manipulated variables impact a shared processing stage. This interaction likely arises because perceptual disfluency disrupts early visual processing and lexical identification, thereby amplifying the frequency effect. Indeed, prior work has documented magnified word frequency effects under perceptual disfluency, including with handwritten cursive text [@barnhart2010; @perea2016] and rotated letterforms [@fernández-lópez2022].

In Experiment 2, we manipulated word frequency (high vs. low) and visual blur (clear, low blur, high blur) within a semantic categorization task, followed by a surprise recognition memory test.

The stage-specific account allows for the possibility that perceptual disfluency and lexical difficulty may interact to engage greater extra- or post-lexical processing, particularly when both factors are present—such as in the case of blurred, low-frequency words. While each manipulation alone may be resolved through more effortful lexical access (reflected in changes to $\mu$), their combination might exceed the limits of lexical-level compensation, thereby recruiting additional control or decision-related processes.

Accordingly, this view predicts a potential interaction between blurring and word frequency on $\beta$, reflecting increased engagement of late-stage, resource-limited mechanisms. If blurred, low-frequency stimuli are associated with especially long response time tails, this may indicate that processing demands extend beyond initial encoding and into later stages of cognitive control or response selection. However, this increased engagement may not always lead to a memory advantage. When both perceptual and lexical demands are high, limited cognitive resources could become overtaxed, potentially attenuating or eliminating any downstream mnemonic effects.

With respect to memory, the stage-specific account assumes that not all items benefit equally from perceptual disfluency. Low-frequency words—like highly blurred words—already tend to attract increased attention during encoding [see pupillometry evidence; @kuchinke2007]. As a result, the attentional boost provided by disfluency may be redundant for these items. In contrast, high-frequency words—which are typically processed more automatically—may benefit more from disfluency manipulations that increase attention and depth of encoding.

Consistent with this view, @ptok2019 argued that memory benefits from disfluency or conflict arise primarily under conditions that are otherwise fluent and encoding-poor. When tasks already require sustained attention or effort, these benefits are likely to diminish. For instance, @ptok2020 found that stabilizing participants' gaze with a chinrest—thus increasing endogenous attention—disrupted memory gains from semantic interference. Similarly, @geller2021 showed that informing participants of an upcoming memory test, thereby prompting deliberate encoding, reduced the disfluency effect. In a related study, @westerman1997 (Experiment 3) found that when encoding effort was uniformly high—such as when participants had to spell rather than simply read words—the memory advantage for disfluent stimuli disappeared.

Taken together, the stage-specific account suggests that disfluency effects on memory may depend on the degree to which processing demands extend beyond lexical access and engage post-lexical control mechanisms. If blurring and lexical difficulty interact to increase these demands—as reflected in longer response time tails (i.e., elevated $\beta$)—we may observe downstream consequences for memory. However, any mnemonic benefit is expected to emerge primarily for stimuli that are otherwise processed fluently, such as high-frequency words, and only when cognitive resources remain available to support deeper encoding. The present experiment examines these possibilities by testing whether blurring and word frequency interact to influence early and late components of processing (as indexed by ex-Gaussian parameters) and whether these effects are associated with differences in subsequent memory performance.

In Experiment 2, we opted not to use the DDM due to its substantial computational demands, particularly given the size and complexity of our dataset. Instead, we focused on the ex-Gaussian decomposition of RTs during encoding. Findings from Experiments 1A and 1B revealed systematic associations between ex-Gaussian and DDM parameters: specifically, a positive relationship between $\mu$ and non-decision time ($T_{er}$), and a negative relationship between $\beta$ and drift rate ($v$). Based on these observed correspondences, we interpret the ex-Gaussian parameters as indirect markers of early (encoding-related) and late (decision-related) processing stages.

## Method

### Transparency and Openness

This study was preregistered https://osf.io/kjq3t. All raw and summary data, materials, and R scripts for pre-processing, analysis, and plotting for Experiment 2 can be found at our OSF page: https://osf.io/6sy7k/.

### Participants

We preregistered a sample size of 432, which is twice the size of Experiments 1A and 1B. This sample size was chosen based on the interaction effect we aimed to test. All participants were recruited through SONA and Prolific. Participants recruited via Prolific were compensated \$12 per hour. On Prolific, we applied built-in filters to include only monolingual, native English-speaking Americans currently residing in the United States, with normal or corrected-to-normal vision. Participants recruited via SONA were given course credit for their participation.

### Materials

One hundred and eighty words (half low-frequency and half high-frequency) were adapted from @fernández-lópez2022. We further selected an additional 45 animal words from their stimuli. To make the experiment more feasible for online participants and to balance our conditions, we split the remaining non-animal words and presented 90 (half high-frequency, half low-frequency) non-animal words along with the 45 animal words during the study phase. This maintained the 2:1 ratio of non-animal to animal words used in previous experiments (e.g., @fernández-lópez2022; @perea2018).

At test, an additional 90 non-animal words not shown during the study phase served as “new” items in the recognition task. We created six counterbalanced lists to ensure that each word appeared as both “old” and “new,” and under each of the three blurring conditions (clear, high blur, low blur) across participants. Similar to nonwords in Experiments 1A and 1B, animal words were excluded from the final analysis.

The animal word stimuli used by @fernández-lópez2022 varied in length (M = 5.3 letters; range: 3–9), but their average length closely matched that of the non-animal words (high-frequency: M = 5.3, range: 3–8; low-frequency: M = 5.3, range: 3–9). Animal words also covered a wide range of frequencies in the SUBTLEX database (M = 11.84 per million; range: 0.61–192.84).

### Procedure

We used the same procedure as Experiments 1B. The main difference is that instead of making a word/non-word decision, participants made a semantic categorization judgement (i.e., "is an animal" or "is not an animal"). You can view the task here: https://run.pavlovia.org/Jgeller112/hf_lf_sem_1.

## Results

### Accuracy

```{r}
rts_wf <- read_csv("https://osf.io/29hnd/download")
```

```{r}

rts_dim <- rts_wf |>
  filter(category=="NONAN")

blur_acc_wf<- rts_wf |>
  group_by(participant) |>
  dplyr::filter(rt >= .2 & rt <= 2.5)|>
  dplyr::filter(category=="NONAN")

```

We started with `r dim(rts_dim)[1]`. After we removed RTs below .2 and above 2.5 (`r round(1-dim(blur_acc_wf)[1]/dim(rts_dim)[1], 3)`)cwe were left with `r dim(blur_acc_wf)[1]` data points.

```{r}
# Create custom contrast matrix manually
contrast_matrix <- matrix(
  c(-.5,  -.5,   # C
     .5,  0,   # HB
     -.5, .5),  # LB
  ncol = 2,
  byrow = TRUE
)

blur_acc_wf$blur <- as.factor(blur_acc_wf$blur)

# Assign to factor
contrasts(blur_acc_wf$blur) <- contrast_matrix


freqc <- hypr(HIGH~LOW,levels=c("HIGH", "LOW"))

blur_acc_wf$frequency<- as.factor(blur_acc_wf$frequency)

contrasts(blur_acc_wf$frequency) <-contr.hypothesis(freqc)

```

```{r}
#| eval: false
#| 
prior_expsc <- c(set_prior("cauchy(0,.35)", class = "b"))

fit_acc_wf <- brm(corr ~ blur*frequency + (1+blur*frequency|participant) + (1+blur|target), data=blur_acc_wf, 
warmup = 1000,
                    iter = 5000,
                    chains = 4, 
                    init=0, 
                    family = bernoulli(),
     cores = 4, 

prior=prior_expsc,
seed=666, 
sample_prior = T, 
save_pars = save_pars(all=T),
control = list(adapt_delta = 0.9), 
file="acc_blmm_sc", 
backend="cmdstanr", 
threads = threading(4))


```

```{r}
# get file from osf

fit_acc_sc <- read_rds("https://osf.io/2yb7g/download")


```

```{r}

# get mean accuracy per condition

emm_acc <- emmeans(fit_acc_sc, ~frequency + blur, type="response") |> 
  parameters::parameters(centrality = "mean")

```

```{r}
#| label: tbl-acc2
#| tbl-cap: "Summary of posterior distributions for fixed effects in the Bayesian accuracy model (Experiment 2)"
#| apa-note: "95% CrI for equivalency tests"


# Run hypothesis tests
a <- hypothesis(fit_acc_sc, "blur1 < 0")  # High Blur < (Low Blur + Clear)/2
b <- hypothesis(fit_acc_sc, "blur2 = 0")  # Low Blur = Clear
c <- hypothesis(fit_acc_sc, "frequency1 = 0")  # No frequency effect
d <- hypothesis(fit_acc_sc, "blur1:frequency1 = 0")  # No interaction: HB vs. (LB + C)/2 × Freq
e <- hypothesis(fit_acc_sc, "blur2:frequency1 = 0")  # No interaction: LB vs. C × Freq

# Helper to round hypothesis results
round_hypothesis <- function(hyp) {
  hyp$hypothesis <- hyp$hypothesis |> mutate(across(where(is.numeric), round, 3))
  hyp
}

# Apply rounding
a <- round_hypothesis(a)
b <- round_hypothesis(b)
c <- round_hypothesis(c)
d <- round_hypothesis(d)
e <- round_hypothesis(e)

# Combine and format table
tab <- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis) |>
  mutate(
    Hypothesis = c(
      "High Blur < (Low Blur + Clear)",
      "Low Blur = Clear",
      "High Freq = Low Freq",
      "Blur × Frequency (High vs. Low/Clear) = 0",
      "Blur × Frequency (Low vs. Clear) = 0"
    ),
    `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]")
  ) |>
  rename(
    Mean = Estimate,
    SE = Est.Error,
    ER = Evid.Ratio,
    `Posterior Prob` = Post.Prob
  ) |>
  select(Hypothesis, Mean, SE, `90% CrI`, ER, `Posterior Prob`)

# Create flextable
acc_exp2 <- flextable(tab) |>
  theme_apa()

acc_exp2


```

The full model summary for accuracy is presented in @tbl-acc2. There was strong evidence that high blurred words were associated with lower accuracy compared to low blurred and clear words, $b$ = `r a$hypothesis$Estimate`, 90% CrI \[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r process_ER_column(a$hypothesis$Evid.Ratio)`.

The evidence for an accuracy difference between low blurred and clear words was weak, $b$ = `r b$hypothesis$Estimate`, 90% CrI \[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r process_ER_column(b$hypothesis$Evid.Ratio)`. The credible interval spanned zero, and the evidence ratio suggested only weak support for either hypothesis.

The evidence for a frequency effect was similary weak , $b$ = `r c$hypothesis$Estimate`, 95% CrI \[`r c$hypothesis$CI.Lower`, `r c$hypothesis$CI.Upper`\], ER = `r c$hypothesis$Evid.Ratio`.

Finally, interaction terms between blurring and word frequency yielded posterior distributions centered near zero, with 95% CrIs that included zero. Evidence ratios for these terms were close to 1, indicating substantial uncertainty and no clear preference for either the null or alternative hypothesis.

### RTs: Ex-Gaussian

Given the complexity of the model, we employed stronger priors to facilitate convergence. For the $\mu$ parameter, we specified the same prior used in Experiments 1A and 1B. For the $\beta$ parameter, we applied a more constrained prior: ${Normal}(0, 0.25)$. Default priors were retained for all remaining model parameters.

```{r}
#| label: tbl-freqeffect
#| tbl-cap: "Frequency effect by blurring level"

p_rt_filter <- rts_wf |>
  filter(corr==1, category=="NONAN")


p_rt_out <- p_rt_filter |> 
 filter(rt >= .2 & rt <= 2.5) |>
  ungroup()

p_rt <- p_rt_filter |>
   filter(rt >= .2 & rt <= 2.5) |>
  group_by(frequency, blur) |>
  dplyr::summarise(rt=mean(rt)) |>
  dplyr::mutate(rt_ms=rt*1000) |>
  select(-rt)

# table for the effect
p_rt |> group_by(blur) |>
  pivot_wider(names_from=frequency, values_from=rt_ms) |>
  mutate(`Frequency Effect`=round(LOW-HIGH)) |>
  rename("Low" ="LOW", "High" = "HIGH", "Blur" = "blur") |>
mutate(Blur = case_when(
  Blur == "C"  ~ "Clear",
  Blur == "LB" ~ "Low Blur",
  TRUE         ~ "High Blur"
)) |> 
   flextable() |>
  autofit() |> 
  theme_apa()

```

We had `r dim(p_rt_filter)[1]` correct RT trials for non-animal responses. After removing RTs below .2 and above 2.5 we were left with `r dim(p_rt_out)[1]` trials.

```{r}
## Contrasts
#hypothesis
contrast_matrix <- matrix(
  c(-0.5,  -0.5,    # C
     0.5,  0,  # HB
     -0.5,  0.5), # LB
  ncol = 2,
  byrow = TRUE
)

# Apply to factor
p_rt_out$blur <- factor(p_rt_out$blur, levels = c("C", "HB", "LB"))
contrasts(p_rt_out$blur) <- contrast_matrix

freqc <- hypr(HIGH ~ LOW, levels = c("HIGH", "LOW"))
p_rt_out$frequency <- as.factor(p_rt_out$frequency)
contrasts(p_rt_out$frequency) <- contr.hypothesis(freqc)

```

```{r}
#| eval: false
# Model formula
bform_ex2 <- bf(
  rt ~ blur*frequency + (1 + blur * frequency |p| participant) + (1 + blur |i| target),
  sigma ~  blur*frequency  + (1 + blur * frequency |p| participant) + (1 + blur |i| target),
  beta ~ blur*frequency + (1 + blur * frequency |p| participant) + (1 + blur |i| target)
)
```

```{r}
#| eval: false
#|

prior <- c(
  prior(normal(0, 10), class = "b"), 
  prior(normal(0,1), dpar="sigma"), 
  prior(normal(0, 0.25), class = "b", dpar = "beta")
)

fit_exg1 <- brm(
bform_exg1, data = p_rt_out,
warmup = 1000,
                    iter = 5000,
                    chains = 4,
                    family = exgaussian(),
                    init = 0,
                    prior=prior, 
                    cores = 4,
control = list(adapt_delta = 0.8), 
sample_prior = T, 
seed=666, 
save_pars = save_pars(all=T), 
backend="cmdstanr", 
threads = threading(2))


```

```{r}
#| eval: false
#| 

#highblurvs clear highblur vslowblur
#long time to load 
fit_sc <- read_rds("http://osf.io/uwcjf/download")

```

```{r}
#| label: tbl-expt2summary
#| tbl-cap: "Summary of posterior distributions for fixed effects in the ex-Gaussian model (Experiment 2)."
#| apa-note: "95% CrI for equivalency tests.Posterior probability indicates the proportion of the posterior distribution that falls on one side of zero (either positive or negative), representing the probability that the effect is greater than or less than zero. Sigma and Beta parameters are on the log (logit) scale."

# Read CSVs from the folder and assign to objects
a <- read_csv("expt2_hypothesis/hypothesis_mu_blur1.csv")
b <- read_csv("expt2_hypothesis/hypothesis_mu_blur2.csv")
c <- read_csv("expt2_hypothesis/hypothesis_mu_frequency1.csv")
d <- read_csv("expt2_hypothesis/hypothesis_mu_blur1_frequency1.csv")
e <- read_csv("expt2_hypothesis/hypothesis_mu_blur2_frequency1.csv")

f <- read_csv("expt2_hypothesis/hypothesis_sigma_blur1.csv")
g <- read_csv("expt2_hypothesis/hypothesis_sigma_blur2.csv")
h <- read_csv("expt2_hypothesis/hypothesis_sigma_frequency1.csv")
i <- read_csv("expt2_hypothesis/hypothesis_sigma_blur1_frequency1.csv")
j <- read_csv("expt2_hypothesis/hypothesis_sigma_blur2_frequency1.csv")

k <- read_csv("expt2_hypothesis/hypothesis_beta_blur1.csv")
l <- read_csv("expt2_hypothesis/hypothesis_beta_blur2.csv")
m <- read_csv("expt2_hypothesis/hypothesis_beta_frequency1.csv")
n <- read_csv("expt2_hypothesis/hypothesis_beta_blur1_frequency1.csv")
o <- read_csv("expt2_hypothesis/hypothesis_beta_blur2_frequency1.csv")

# Combine and label all results
tab <- bind_rows(a, b, c, d, e, f, g, h, i, j, k, l, m, n, o) |>
  mutate(
    ER = as.numeric(Evid.Ratio),
    `90% CrI` = str_glue("[{CI.Lower}, {CI.Upper}]"),
    Hypothesis = c(
      "High Blur  > (vs. Clear/Low Blur) ",
      "Low Blur >  Clear",
      "High Frequency <  Low Frequency ",
      "High Blur (vs. Low Blur/Clear) × Frequency",
      "Low Blur (vs. Clear) × Frequency",
      
      "High Blur  > (vs. Clear/Low Blur)",
      "Low Blur <  Clear",
      "High Frequency < Low Frequency",
      "High Blur (vs. Low Blur/Clear) x Frequency",
      "Low Blur (vs. Clear) × Frequency",
      
      "High Blur >  (vs. Clear/Low Blur)",
      "Low Blur >  (vs. Clear)",
      "High Frequency < Low Frequency",
      "High Blur (vs. Low Blur/Clear) × Frequency",
      "Low Blur (vs. Clear) × Frequency "
    )
  ) |>
  select(Hypothesis, Parameter, Estimate, Est.Error, `90% CrI`, ER, Post.Prob) |>
  rename(
    "Mean" = "Estimate",
    "SE" = "Est.Error",
    "Posterior Prob" = "Post.Prob"
  ) |>
  flextable() |>
  theme_apa()

tab

```

@tbl-expt2summary provides a model summary. @fig-quantiledeltaexp2 visualizes RTs as quantile and delta plots highlighting how blurring and word frequency affected processing during word recognition. High blurred words showed greater central tendency shifts ($\mu$) compared to the average of clear and low blurred words, *b* = `r a$Estimate`, 90% CrI \[`r a$CI.Lower`, `r a$CI.Upper`\], ER = `r process_ER_column(a$Evid.Ratio)`. Low blurred words also showed greater shifting compared to clear words,*b* = `r b$Estimate`, 90% CrI \[`r b$CI.Lower`, `r b$CI.Upper`\], ER = `r process_ER_column(b$Evid.Ratio)`.

There was a consistent effect of word frequency: high-frequency words exhibited less shifting than low-frequency words, *b* = `r c$Estimate`, 90% CrI \[`r c$CI.Lower`, `r c$CI.Upper`\], ER = `r process_ER_column(c$Evid.Ratio)`.

The posterior distributions for blur × frequency interactions were centered near zero, providing little support for interaction effects. Specifically, the High Blur (vs. Clear/Low Blur) × Frequency interaction was, *b* = `r d$Estimate`, 90% CrI \[`r d$CI.Lower`, `r d$CI.Upper`\], ER = `r d$Evid.Ratio`, and the Low Blur (vs. Clear) × Frequency interaction was *b* = `r e$Estimate`, 90% CrI \[`r e$CI.Lower`, `r e$CI.Upper`\], ER = `r e$Evid.Ratio`.

There was evidence for greater variability ($\sigma$) in response times for high blurred words relative to the average of clear and low blurred words, *b* = `r f$Estimate`, 90% CrI \[`r f$CI.Lower`, `r f$CI.Upper`\], ER = `r process_ER_column(f$Evid.Ratio)`. The evidence for no difference between low blurred and clear words was weak, *b* = `r g$Estimate`, 90% CrI \[`r g$CI.Lower`, `r g$CI.Upper`\], ER = `r g$Evid.Ratio`. 

There was modest support for a frequency effect on variability: high-frequency words showed less variability than low-frequency words, *b* = `r h$Estimate`, 90% CrI \[`r h$CI.Lower`, `r h$CI.Upper`\], ER = `r h$Evid.Ratio`.

Blur × Frequency interactions on variability were mixed. For the High Blur (vs. Clear/Low Blur) × Frequency interaction: *b* = `r i$Estimate`, 90% CrI \[`r i$CI.Lower`, `r i$CI.Upper`\], ER = `r i$Evid.Ratio`, and for the low blur (vs. clear) × grequency interaction: *b* = `r j$Estimate`, 90% CrI \[`r j$CI.Lower`, `r j$CI.Upper`\], ER = `r j$Evid.Ratio`.

Posterior estimates indicated greater skew ($\beta$) for high blurred words compared to the average of clear and low blurred words, *b* = `r k$Estimate`, 90% CrI \[`r k$CI.Lower`, `r k$CI.Upper`\], ER = `r process_ER_column(k$Evid.Ratio)`. The difference in skew between low blurred and clear words was negligible, *b* = `r l$Estimate`, 90% CrI \[`r l$CI.Lower`, `r l$CI.Upper`\], ER = `r l$Evid.Ratio`.

There was strong evidence that high-frequency words were associated with less skew than low-frequency words, *b* = `r m$Estimate`, 90% CrI \[`r m$CI.Lower`, `r m$CI.Upper`\], ER = `r m$Evid.Ratio`.

The interaction between high blur (vs. clear/low blur) and frequency on skew was supported: *b* = `r n$Estimate`, 90% CrI \[`r n$CI.Lower`, `r n$CI.Upper`\], ER = `r n$Evid.Ratio`. In contrast, the low blur (vs. clear) × frequency interaction showed strong evidence for greater skewing for high frequency words (even though CrI included 0), *b* = `r o$Estimate`, 90% CrI \[`r o$CI.Lower`, `r o$CI.Upper`], ER = `r o$Evid.Ratio`.

```{r}

#Delta plots (one per subject)
quibble <- function(x, q = seq(.1, .9, .2)) {
  tibble(x = quantile(x, q), q = q)
}

data.quantiles <- p_rt_out |>
  dplyr::group_by(participant,blur,frequency, corr) |>
  dplyr::mutate(rt_ms = rt*1000) |> 
  dplyr::summarise(RT = list(quibble(rt_ms, seq(.1, .9, .2)))) |> 
  tidyr::unnest(RT) |>
  ungroup()


data.delta <- data.quantiles |>
  dplyr::group_by(participant, blur,frequency,  q) |>
  dplyr::summarize(RT=mean(x)) |>
  ungroup()
```

```{r}
#Delta plots (based on vincentiles)
vincentiles <- data.quantiles |>
  dplyr::group_by(blur,frequency, q) |>
  dplyr::summarize(RT=mean(x)) |>
  ungroup()

v=vincentiles |>
  dplyr::group_by(blur,frequency, q) |>
  dplyr::summarise(MRT=mean(RT)) |>
  ungroup()


v$blur<- factor(v$blur, level=c("HB", "LB", "C"))


p <- ggplot(v, aes(x = q, y = MRT, colour = blur)) +
  facet_grid(~frequency, labeller = labeller(frequency = c(
    "HIGH" = "High Frequency",
    "LOW" = "Low Frequency"))) + 
  geom_line(size = 1) +
  geom_point(size = 3) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 16, face = "bold"),
    axis.text = element_text(size = 16),
    plot.title = element_text(face = "bold", size = 20)
  ) +
  scale_y_continuous(breaks = seq(500, 1600, 100)) +
  coord_cartesian(ylim = c(500, 1600)) +
  scale_x_continuous(breaks = seq(.1, .9, .2)) +
  geom_label_repel(data = v, aes(x = q, y = MRT, label = round(MRT, 0)), color = "black", min.segment.length = 0, seed = 42, box.padding = 0.5) +
  labs(x = "Quantiles", y = "Response latencies in ms", tag = "A") +
  ggokabeito::scale_colour_okabe_ito(
    name = "Blur Condition",
    labels = c("C" = "Clear", "HB" = "High Blur", "LB" = "Low Blur")
  )

```

```{r}

p2 <- ggplot(data=v,aes(y=MRT, x=frequency, color=q)) + facet_grid(~blur)+
  geom_line()+
  geom_point(size=4) 

```

```{r}
#| label: tbl-freq
#| tbl-cap: "Word frequency effect across .1, .3, .5, .7, .9 quantiles as a function of blurring"
#diff
v_wf <- v |>
  dplyr::group_by(blur, q)|>
  tidyr::pivot_wider(names_from = "frequency", values_from = "MRT") |>
  mutate(Diff=LOW-HIGH) |>
  rename("Blur" = "blur") |>
  mutate(Blur = ifelse(Blur=="C", "Clear", ifelse(Blur=="HB", "High Blur", "Low Blur")))|> 
  ungroup()

v_wf |> select(Blur, q, Diff) |> 
  pivot_wider(names_from="q", values_from="Diff") |> 
  mutate(across(where(is.numeric), ~ round(.x, 3))) |>
   flextable() |>
  autofit() |> 
  theme_apa()

```

```{r}
v_chb <- v |>
  dplyr::filter(blur=="C" | blur=="HB") |>
  dplyr::group_by(frequency, q)|>
  mutate(mean_rt = mean(MRT)) |> 
  ungroup()|> 
  tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
  mutate(diff=HB-C)


v_chb_freq <- v |>
  dplyr::group_by(blur, q)|>
  mutate(mean_rt = mean(MRT)) |> 
  ungroup()|> 
  tidyr::pivot_wider(names_from = "frequency", values_from = "MRT") |>
  mutate(diff=LOW-HIGH)


p1 <- ggplot(v_chb, aes(x = mean_rt, y = diff)) + facet_grid(~frequency, labeller = labeller(frequency = c(
    "HIGH" = "High Frequency",
    "LOW" = "Low Frequency"))) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme_minimal(base_size=16) + 
  theme(legend.position = "none") + 
scale_y_continuous(breaks=seq(10,500,50)) +
  scale_x_continuous(breaks=seq(600,1100, 100)) +
  coord_cartesian(xlim=c(600,1100), ylim = c(10, 500)) +
geom_label_repel(data=v_chb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5)  +
  labs(title= "Delta Plots: High Blur vs. Clear", x = "", y = "Group Differences", tag = "B")


```

```{r}
v_clb <- v |>
  dplyr::group_by(frequency,q)|>
   mutate(mean_rt = mean(MRT)) |> 
  ungroup() |> 
  tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
  mutate(diff=LB-C)


p2 <- ggplot(v_clb, aes(x = mean_rt, y = diff)) + facet_grid(~frequency, labeller = labeller(frequency = c(
    "HIGH" = "High Frequency",
    "LOW" = "Low Frequency"))) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme_minimal(base_size=16) + 
  theme(legend.position = "none") + 
scale_y_continuous(breaks=seq(10,500,50)) +
  scale_x_continuous(breaks=seq(600,1100, 100))+
    coord_cartesian(xlim=c(600,1100), ylim = c(10, 500)) +
  geom_label_repel(data=v_clb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5) + 
  labs( title = "Delta Plots: Clear - Low Blur", x = "Mean RT per quantile", y = "")
```

```{r}

v_hlb <- v |>
  dplyr::filter(blur=="HB" | blur=="LB") |>
  dplyr::group_by(frequency,q)|>
   mutate(mean_rt = mean(MRT)) |> 
  ungroup() |> 
  tidyr::pivot_wider(names_from = "blur", values_from = "MRT") |>
  mutate(diff=HB-LB)


p3 <- ggplot(v_hlb, aes(x = mean_rt, y = diff)) + 
 facet_grid(~frequency, labeller = labeller(frequency = c(
    "HIGH" = "High Frequency",
    "LOW" = "Low Frequency"))) + 
  geom_abline(intercept = 0, slope = 0) +
  geom_line(size = 1, colour = "black") +
  geom_point(size = 3, colour = "black") +
  theme_minimal(base_size=16) + 
  theme(legend.position = "none") +
  scale_y_continuous(breaks=seq(10,500,50)) +
    coord_cartesian(ylim = c(10, 500)) + 
  scale_x_continuous(breaks=seq(600,1100, 100))+
    coord_cartesian(xlim=c(600,1100), ylim = c(10, 500)) +
  geom_label_repel(data=v_hlb, aes(y=diff, label=round(diff,0)), color="black", min.segment.length = 0, seed = 42, box.padding = 0.5) + 
  labs( title = "Delta Plots: High Blur - Low Blur", x = "", y = "")

```

```{r}
#| label: fig-quantiledeltaexp2
#| fig-cap: "Group RT distributions in the blurring and word frequency manipulations in word stimuli. A. Quantile plots with each point represents the average RT quantiles (.1, .3, .5, .7,and .9) in each condition. B. Delta plots obatined by computing the quantiles for each participant and subsequently averaging the obtained values for each quantile over the participants and subtracting the values from each condition."
#| fig-width: 20
#| fig-height: 12

# Create bottom row with p1, p2, p3
bottom_row <- plot_grid(p1, p2, p3, ncol = 3, label_size = 16)

# Stack p on top of the bottom_row
final_plot <- plot_grid(p, bottom_row, ncol = 1)


final_plot 

ggsave(filename='./figures/figure_kde3.png',width=18,height=12, dpi=500)

```

### Recognition Memory

To aid in model convergence, we used a binomial (probit) family rather than a Bernoulli family when modeling our results. While both approaches are appropriate for binary outcomes, the binomial model allowed us to aggregate responses within each condition, reducing the number of observations and easing computational demands. This aggregation improved sampling efficiency and stability, particularly given the complexity of the full factorial structure. However, as a trade-off, we were unable to model random intercepts and slopes for individual items, since aggregation collapses trial-level variability.

```{r}

mem_sc <- read_csv("https://osf.io/eapu5/download")

```

```{r}
dat_binom <- mem_sc %>%
  count(participant, isold, blur, frequency, sayold) %>%
  tidyr::pivot_wider(
    names_from = sayold,
    values_from = n,
    values_fill = 0,
    names_prefix = "sayold_"
  ) %>%
  rename(
    success = sayold_1,
    failure = sayold_0
  ) %>%
  mutate(total = success + failure)

```

```{r}
## Contrasts
#hypothesis
contrast_matrix <- matrix(
  c(-0.5,  -0.5,    # C
     0.5,  0,  # HB
    -0.5,  0.5), # LB
  ncol = 2,
  byrow = TRUE
)

# Apply to factor
dat_binom$blur <- factor(dat_binom$blur, levels = c("C", "HB", "LB"))
contrasts(dat_binom$blur) <- contrast_matrix


HF_cont <- hypr(HIGH~LOW,levels=c("HIGH", "LOW"))

dat_binom$frequency<-as.factor(dat_binom$frequency)

contrasts(dat_binom$frequency) <-contr.hypothesis(HF_cont)

```

```{r}
#| eval: false
#| 
prior_exp <- c(set_prior("cauchy(0,.35)", class = "b"))

fit_sc_sdt <- brm(
  formula = success |trials(total) ~ isold * blur * frequency + (1 + isold * blur * frequency | participant),
  data = dat_binom,
  family = binomial(link = "probit"),
  prior = prior_exp, 
  chains = 4,
  cores=4, 
  iter = 5000,
  sample_prior=TRUE, 
  backend="cmdstanr", 
  threads = threading(2), 
  file="blmm_sdt_sc_hblbc", 
  control = list(adapt_delta = 0.95))

```

```{r}

# read in file from osf 
fit_sc_mem <- read_rds("https://osf.io/kh43z/download")
```

```{r}
emm_m2_d1 <- emmeans(fit_sc_mem, ~isold | blur * frequency) |> 
  contrast("revpairwise")

emm_m2_d2 <- emmeans(fit_sc_mem, ~isold + blur * frequency) |> 
  contrast(interaction = c("revpairwise", "pairwise"), by = "frequency")

# (Negative) criteria
emm_m2_c1 <- emmeans(fit_sc_mem, ~blur * frequency)
emm_m2_c2 <- emmeans(fit_sc_mem, ~blur | frequency) |> 
  contrast("pairwise")
```

```{r}
#| label: fig-m2-emmeans
#| fig-cap: Estimated posterior distributions for d-prime and criterion, and differences between all conditions with 95% CrIs (thin lines)
#| fig-width: 12
#| fig-height: 10


blur_level <- c("Clear", "High Blur", "Low Blur", "Clear-High Blur", "Clear-Low Blur", "High Blur-Low Blur")

tmp <- bind_rows(
  bind_rows(
    gather_emmeans_draws(emm_m2_d1) |> 
      group_by(blur, frequency) |> 
      select(-contrast),
    gather_emmeans_draws(emm_m2_d2) |> 
      rename(
        blur = blur_pairwise
      ) |> 
      group_by(blur, frequency) |> 
      select(-isold_revpairwise)
  ),
  bind_rows(
    gather_emmeans_draws(emm_m2_c1),
    gather_emmeans_draws(emm_m2_c2) |> 
      rename(
        blur = contrast
      )
  ),
  .id = "Parameter"
) |> 
  ungroup() |> 
  mutate(Parameter = factor(Parameter, labels = c("dprime", "Criterion"))) |> 
   mutate(
    t = if_else(str_detect(blur, " - "), "Differences", "Group means") |> 
      fct_inorder(),
    blur = fct_inorder(blur), 
    frequency=ifelse(frequency=="HIGH", "High", "Low")
  ) 
# Convert blur to an ordered factor to control spacing
# Convert blur to a factor with predefined order
tmp |>   
  mutate(
    .value = if_else(Parameter == "Criterion", .value * -1, .value),
    Parameter = fct_rev(Parameter)
  ) |> 
  ggplot(aes(x = as.numeric(blur), y = .value, fill = frequency)) +  
  labs(
    x = "Blurring Level (or difference)",
    y = "Parameter value", 
    fill = "Frequency"
  ) +
  geom_hline(yintercept = 0, linewidth = .25) +
  
  # Replace scale_x_discrete with scale_x_continuous
  scale_x_continuous(
    breaks = 1:6,  # Assuming you have 6 unique blur levels
    labels = blur_level  # Your predefined labels
  ) +
  
  scale_slab_alpha_discrete(range = c(1, .5)) +
  
  stat_halfeye(
    normalize = "xy",
    width = 0.44,
    slab_color = "black", 
   point_interval = "mean_qi",
    .width = c(.95),
    aes(
      side = ifelse(frequency == "High", "left", "right"),
      x = ifelse(frequency == "High", as.numeric(blur) - .08, as.numeric(blur) + .08)
    )
  ) +
  
  guides(slab_alpha = "none") +
  facet_grid(Parameter ~ t, scales = "free") +
  ggokabeito::scale_fill_okabe_ito() + 
    theme_minimal(base_size=16) + 
   theme(
    strip.background = element_rect(fill = "black", color = "black"),
    strip.text = element_text(color = "white", face = "bold")
  )

ggsave("dprime_expt2.png", width=14, height=8, dpi=500)

```

```{r}
#| echo: false
# Run hypotheses
a <- hypothesis(fit_sc_mem, "isold:blur1 > 0")
b <- hypothesis(fit_sc_mem, "isold:blur2 = 0")
c <- hypothesis(fit_sc_mem, "isold:frequency1 > 0")
d <- hypothesis(fit_sc_mem, "isold:blur1:frequency1 > 0")
e <- hypothesis(fit_sc_mem, "isold:blur2:frequency1 > 0")

# Extract and round each hypothesis table
a$hypothesis <- a$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
b$hypothesis <- b$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
c$hypothesis <- c$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
d$hypothesis <- d$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))
e$hypothesis <- e$hypothesis |> mutate(across(where(is.numeric), ~ round(.x, 3)))

# Combine into one table
tab <- bind_rows(
  a$hypothesis,
  b$hypothesis,
  c$hypothesis,
  d$hypothesis,
  e$hypothesis
) |>
  mutate(Evid.Ratio = as.numeric(Evid.Ratio)) |>
  select(-Star)

# Optionally ensure everything except the first column is rounded again (belt-and-suspenders)
tab[, -1] <- t(apply(tab[, -1], 1, round, digits = 3))

```

#### Sensitivity

Consistent with Experiments 1A and 1B, sensitivity in recognition memory was higher for high blurred words compared to both clear and low blurred words (see @fig-m2-emmeans), $\beta$ = `r a$hypothesis$Estimate`, 90% CrI \[`r a$hypothesis$CI.Lower`, `r a$hypothesis$CI.Upper`\], ER = `r a$hypothesis$Evid.Ratio`. The difference in recognition between clear and low blurred words was negligible, with strong evidence in favor of the null hypothesis: $\beta$ = `r b$hypothesis$Estimate`, 95% CrI \[`r b$hypothesis$CI.Lower`, `r b$hypothesis$CI.Upper`\], ER = `r b$hypothesis$Evid.Ratio`.

Low-frequency words were more likely to be recognized than high-frequency words, $\beta$ = `r c$hypothesis$Estimate`, 90% CrI \[`r c$hypothesis$CI.Lower`, `r c$hypothesis$CI.Upper`\], ER = `r c$hypothesis$Evid.Ratio`. Crucially, there was strong evidence for an interaction between blur and frequency, such that high-frequency words showed a selective memory benefit under high blur conditions: $\beta$ = `r d$hypothesis$Estimate`, 95% CrI \[`r d$hypothesis$CI.Lower`, `r d$hypothesis$CI.Upper`\], ER = `r d$hypothesis$Evid.Ratio`. A second interaction emerged for the comparison between low blur and clear words: $\beta$ = `r e$hypothesis$Estimate`, 90% CrI \[`r e$hypothesis$CI.Lower`, `r e$hypothesis$CI.Upper`\], ER = `r e$hypothesis$Evid.Ratio`. In this case, recognition was also better for high-frequency words when stimuli were presented with low blur. 

## Discussion

Experiment 2 probed how late‐stage processing underlies the mnemonic boost that disfluent stimuli sometimes confer—and how that boost interacts with word frequency. Looking at accuracy, we did not see any differences between the levels of blurring, or any interactions with frequency. We did see an accuracy difference between high frequency and low frequency words. This largely corresponds with what @fernández-lópez2022 found.

In line with Experiments 1A and 1B, we observed systematic effects of visual blurring on the response time distribution. High blur led to greater shifting and skewing of the distribution compared to both low blur and clear conditions, while low blur produced more shifting than clear text. Additionally, word frequency significantly affected both the location ($\mu$) and skewness ($\beta$) parameters: low-frequency words were associated with slower responses and increased positive skew.

Critically, we observed a significant interaction between blurring and word frequency on the $\beta$ parameter. Under high blur, the frequency effect was magnified, with low-frequency words exhibiting a substantially longer tail compared to both low and no blur conditions—indicating more pronounced delays in late-stage processing.

Turning to memory, both blurred conditions elicited a perceptual disfluency effect relative to their respective baselines. High-frequency words showed a clear memory benefit under both high and low blur, whereas low-frequency words did not. This pattern is particularly striking when considered alongside the response time data. High blur produced a significant interaction on the $\beta$ parameter, with low-frequency words showing a disproportionately longer tail—indicative of greater late-stage processing difficulty. Yet, despite this increased effort, low-frequency words did not exhibit a corresponding memory advantage. In contrast, high-frequency words, which showed relatively smaller $\beta$ increases, did benefit from the disfluency.

The present findings suggest that late-stage processing may be necessary but not sufficient for disfluency-based memory benefits. In our study, high blurring magnified the word frequency effect on $\beta$, indicating more prolonged or effortful late-stage processing—particularly for low-frequency words. However, memory benefits were observed only for high-frequency words, implying that this added processing must be paired with robust lexical access to translate into improved encoding.

Interestingly, low blur words also interacted with frequency, producing a memory advantage for high-frequency words. While this may initially seem counterintuitive—given that low blur did not significantly increase $\beta$—a closer look reveals a more nuanced picture. Although the 95% credible interval for the blur–frequency interaction on $\beta$ included zero, the evidence ratio (ER = 19.10) provided substantial evidence for a positive effect. Intriguingly, the direction of the effect was reversed from the typical pattern: under low blur, high-frequency words exhibited greater late-stage processing than low-frequency words.

This suggests that high-frequency words may have been more impaired by low blurring, possibly because the disfluency disrupted the otherwise automatic processing of these familiar items, prompting additional extra-lexical processing. Yet, rather than hindering performance, this modest disruption may have facilitated deeper encoding, yielding a selective memory benefit. In contrast, low-frequency words may not have been processed deeply enough to benefit from the added perceptual difficulty. This finding highlights that memory benefits from disfluency are not solely a function of increased processing effort, but rather emerge when perceptual challenge intersects with lexical accessibility in a way that supports enriched encoding.

This asymmetry suggests that modest perceptual disfluency can amplify processing of high-frequency words, possibly by prompting additional elaboration or semantic integration, while not overburdening cognitive resources. In contrast, high blur induces excessive late-stage processing, particularly for low-frequency words, as reflected in increased $\beta$ values—without producing corresponding memory benefits. Together, these findings support the idea that additional processing is beneficial only when the perceptual challenge is manageable and the lexical stimulus is sufficiently accessible. When disfluency becomes too demanding, as under high blur, it may exceed processing capacity—especially for low-frequency words—ultimately hindering memory encoding rather than helping it.

# General Discussion

Interfering with stimulus perception during encoding can sometimes improve later explicit memory. The mixed data on perceptual disfluency has called into question the utility of such manipulations in the learning domain. One of the main aims of the current set of experiments was to examine the underlying mechanisms of the perceptual disfluency effect to better understand when perceptual disfluency aids memory and when it does not. To this end, our study delved into the impact of one type of perceptual disfluency--blurring (i.e., low blurring and high blurring)--on the process of encoding, as assessed through a LDT (Experiments 1A and 1B), and a semantic categorization task (Experiment 2). RT distributions were analyzed with an ex-Gaussian model (Experiments 1A, 1B, 2) and DDM (Experiments 1A and 1B) to better understand how perceptual disfluency affects encoding. These models offered a comprehensive descriptive and theoretical framework through which to examine the perceptual disfluency effect.

To recapitulate our findings, during encoding, high blurred words showed greater distributional shifting and skewing compared to clear and low blurred words. In addition, DDM fits indicated high blurred words had a higher $T_{er}$ and lower $v$. Conversely, low blurred words compared to clear words showed greater distributional shifting, but there was no difference in skewing. DDM fits showed higher $T_{er}$, but had no effect on $v$.

Turning to recognition memory, high blurred words were more likely to be recognized at test compared to clear words and low blurred words. This pattern arose regardless if context was reinstated at test (Experiment 1B). This pattern replicates the results from @rosner2015. In addition, we showed word frequency (Experiment 2) also modulates the disfluency effect. Namely, low frequency words did not show a disfluency effect.

These findings have several implications. At a theoretical level, the current data suggests that in order for perceptual disfluency to benefit memory it has to be disfluent enough to affect both early and late stages of processing. A manipulation that only produces a general slowing of responses is not sufficient to enact an mnemonic effect. However, an important caveat to this is that processes during encoding of the word itself are not enough to produce an mnemonic benefit. In Experiment 2, we did not observe better memory for low frequency-high blurred words which are the hardest and presumably receive the most top-down processing. We only observed a disfluency effect for high frequency-high blurred words. This points to the importance of control processes and processing limitations in producing the disfluency effect.

We argue the current findings align more closely with the stage-specific account proposed by @ptok2019. While the account was proposed to explain memory effects that require conflict during encoding, like the semantic interference effect, we feel that it is a useful theoretical framework to explain the current findings. In fact, @ptok2019 and @ptok2020 suggested a connection between their framework and perceptual disfluency effects and desirable difficulties, more broadly.

Within the stage-specific account, memory performance depends on the nature of processing during encoding and the utilization of cognitive control mechanisms. In our experiments, participants were tasked with determining whether letter strings represented words or non-words (Experiments 1A and 1B), or whether a word belonged to an animal category (Experiment 2). For skilled readers, these tasks are executed automatically and smoothly. Coupled with perceptual disfluency, this combination is believed to lead to memory advantages seen with perceptually disfluent stimuli.

When we manipulated word frequency, however, the process of recognizing low-frequency words demanded more effort and attentional resources in addition to the perceptual disfluency of blurring. Consequently, this lead to the task becoming more challenging and difficult. Thus, the increased processing demands from recognizing low frequency words may have countered the benefits from high blurred words, as more attentional resources were allocated to recognizing low-frequency words.

There are other examples that support this capacity-limited view of perceptual disfluency. For instance, @geller2018 showed that easy-to-read cursive words and hard-to-read cursive words are better remembered than computer print words, but the memory effect is much larger for easier to read cursive words. As another notable example, participants with low working memory capacity do not seem to benefit from perecptaul disfluency as much as those with higher working memory capacity [@lehmann2015]. At a broader level, @wenzel2019 suggested that intelligence is an important factor for when desirable difficulties are desirable for learning.

At a methodological level, our experiments demonstrate that a straightforward blurring manipulation can benefit memory, which we observed whether or not we reinstated the context during testing. However, blurring has to be sufficiently difficult do so. If the secondary task requires too much attentional control the effect might not be observed or attenuated.

More significantly, our current experiments underscore the benefits of using mathematical and computational models—such as the ex-Gaussian model and DDM to examine perceptual disfluency during encoding. In Experiments 1A and 1B, both models converged on similar findings. Specifically, response time distributions were differential affected by the degree of visual blur. Words with low levels of blur primarily influenced early or non-decision stages of processing (reflected in parameters such as $T_{er}$ and $\mu$), whereas highly blurred words impacted both early and later stages ($T_{er}$, $\tau$, and $v$). These findings suggest that both the DDM and ex-Gaussian model are sensitive to perceptual disfluency and can be used to uncover underlying cognitive mechanisms during encoding (see also gomez2014). Although the models converged on similar patterns, it remains an open question whether one should be favored over the other. However, in terms of directly linking disfluency to cognitive processes, the DDM offers a clearer theoretical framework and may be the more informative model in this context.

Furthermore, our distrbution modeling of RTs appears to be a more sensitive method. Although we found weak evidence for differences between clean and low blurred conditions in measures like accuracy, we did notice variations in non-decision time and a shift in the response time distribution for low blurred words compared to clear words. We recommend that future studies employ distribution modeling and DDM to decompose response times and directly quantify the impact of perceptual disfluency on encoding.

While we applied the DDM to inform processes during encoding, there is one case of the DDM being applied to the study of perceptual disfluency during retrieval. In one recent study, @hu2022 examined RTs during retrieval. @hu2022 were primarily interested in how perceptual disfluency manipulated with Sans Forgetica typeface influenced DDM parameters and confidence judgments during recognition. At test, they found a non-significant difference in mean RTs between Sans Forgetica typeface and Arial typeface. However, looking at the DDM parameters, Sans Forgetica and Airal typefaces differed on $T_{er}$, but not drift rate. They also looked at how parameters of the DDM were related to confidence judgments. Higher $T_{er}$ was related to lower confidence and higher drift rate was associated with higher confidence. While their focus was on retrieval and not encoding, this corresponds with what we observed for our low blurred words in our experiments--a weak manipulation affected non-decision time, but not drift rate. Overall, this further highlights the utility of the the DDM in studying perceptual disfluency (and other encoding conflict effects) during encoding and at test.

Finally, at a practical level, we do show that blurring can benefit later memory. However, caution needs to be taken here. First, the current experiments were conducted online using simple materials (i.e., list learning). It is unclear how these effects would generalize to a classroom setting or more educational realistic materials (see @geller2020). Second, participants were not told about the upcoming recognition test. @geller2021 has showed that low test expectancy is an important moderator for this effect. Third, while we did not establish a region of practical equivalence, the size of the disfluency effect appears to be be small in nature. Looking at the default region of proximal equivalence (ROPE) from the `bayestestr` package (here -0.10 -.10 in standardized units) many of the critical contrasts either were completely inside this region or overlapped this region, suggesting negligible differences. For more applied work, this might be well below the smallest effect size of interest. Now this is not to say all research on perceptual disfluency is superfluous. As noted in @geller2021, a fruitful avenue for future work might be to investigate how perceptual influenced effects processing in every day life where memory is largely incidental [@castel2015].

These results provide some context for the large number of replication failures. Many of the studies looking at disfluency do not take care in ensuring the disfluency manipulation is actually disfluent. Most times, studies use only two levels (disfluent/fluent) and perform analyses that may not be well suited for the type of data they have. As we have hopefully shown here, it is important to take into account the entire RT distribution. By examining the RT distributions of different levels of disfluency we obtained a richer better understanding of the stages or loci manipulations have during encoding. It is our hope that learning and memory researches will begin to use these tools to help understand encoding processes involved in perceptual disfluency effects but also encoding contexts where there is considerable conflict.

## Conclusion

Our paper contributes nuanced insights to the intricate relationship between perceptual disfluency and memory encoding. We have shown that perceptual disfluency can aid in memory retention, but its efficacy is contingent upon the degree of disfluency and other contextual factors such as word frequency. Our findings endorse the stage-specific account, emphasizing the role of cognitive control mechanisms in the observed memory advantages with perceptual disfluency. Furthermore, our methodological contributions, employing an ex-Gaussian model and DDM, not only validate the benefits of examining RT distributions, but also open new avenues for future research in learning and memory studies. We caution, however, that the applicability of these findings in real-world educational settings remains an open question, and the effect sizes observed were relatively small, thus warranting further investigation.

Ultimately, this work stands as a call to action for a more comprehensive, nuanced approach to studying perceptual disfluency, incorporating both advanced statistical methods and a more exhaustive range of experimental conditions to better elucidate when and how disfluency can facilitate memory.

# References

::: {#refs}
:::
