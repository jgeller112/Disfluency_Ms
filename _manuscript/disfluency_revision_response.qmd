---
title: Responses to Editor and Reviewer Concerns
format:
  hikmah-response-typst: default
bibliography: references.bib
csl: apa.csl
---

Thank you for the opportunity to revise and resubmit our manuscript, “The Perceptual Disfluency Effect: A Drift-Diffusion and Ex-Gaussian Analysis” (Manuscript ID: JOC-2025-0043). We greatly appreciate the insightful and constructive comments from the Editor and Reviewers. These suggestions were extremely helpful in guiding our revisions, and we used them to substantially improve the clarity, structure, and presentation of the manuscript.

Some of the general changes include streamlining the paper—per Reviewer 2’s recommendation—by trimming its overall length, moving most of the drift-diffusion modeling (DDM) material to the appendix, and updating the title. We also noticed that some raw data for Experiment 2 had not been uploaded to our OSF page; we have now added those data.

Below, we describe in detail how we revised the paper in response to the Editor’s and Reviewers’ comments. The Editor’s and Reviewers’ comments are cited or summarized in [red italicized text]{.memo-reviewer-inline}, and our responses are provided in standard black Roman text. We also include excerpts from the revised manuscript in [blue text]{.memo-excerpt-inline}. I believe we have addressed all the reviewers conerns adequetly.  

# Reviewer 1

## Main Concerns

::: memo-reviewer
***Reviewer 1***:  A central point I would encourage the authors to consider is how the experimental design—specifically, the intermixed vs. blocked presentation of stimuli—may influence the observed disfluency effects and consequently, the interpretation of the results.  The current design, which randomly intermixes clear, low-blur, and high-blur words, may limit the ecological validity of the findings, particularly in light of the educational implications suggested.
In real-world reading scenarios (e.g., reading a book or handwritten notes), perceptual characteristics such as font, handwriting, or blur are typically consistent across longer stretches of text. This consistency enables parafoveal processing and the generation of visual and orthographic predictions about upcoming stimuli. In contrast, an intermixed design reduces such predictability, and may even suppress predictive processing altogether.
:::

We thank the reviewer for raising this point, and we fully agree that this is a potential limitation of the study. Some of our work [@geller2018] and @rosner2015 has shown that certain types of disfluency (e.g., handwritten cursive and high blurring) elicit the effect regardless of design (mixed vs. pure/blocked). We opted for a mixed design for this reason; however, as you note, it is not ecologically valid. We have added this point to the discussion.

::: memo-excerpt
Second, all three experiments employed a mixed-list design. Although perceptual disfluency effects have been observed with both mixed and pure/blocked designs [@geller2018; @rosner2015], the mixed-list paradigm is not representative of typical classroom contexts.
:::

::: memo-reviewer
***Reviewer 1***: From a theoretical standpoint, it may be worthwhile to discuss these results in relation to the prediction error framework, and more specifically the orthographic prediction error model (e.g., Gagl et al., 2020, NeuroImage). This model suggests that reading involves generating visual-orthographic predictions, which are compared against input to update lexical representations. If prediction is impaired or suspended due to unpredictability in stimulus appearance (as in the current intermixed design), then top-down facilitation mechanisms may be attenuated—potentially altering both response time distributions and memory outcomes.
:::

We thank Reviewer 1 for suggesting Gagl et al. (2020). I was not aware of this paper before. We have now incorporated discussion of the orthographic prediction error model alongside the stage-specific account.

::: memo-excerpt
Complementing this stage-specific perspective, @gagl2020 proposed a more stage-agnostic framework: the orthographic prediction error (OPE) model. Drawing on fMRI and EEG data, this model holds that reading involves generating visual–orthographic predictions and comparing them to incoming input. Visual degradation increases OPE—the mismatch between expected and observed letter input—and when predictions are impaired or suspended due to unpredictability, the benefits of top-down facilitation are reduced. Processing then relies more heavily on bottom-up input, which can slow responses and alter encoding quality. This framework helps clarify how perceptual disfluency influences both response-time distributions and memory outcomes, while situating the stage-specific account within a broader predictive-processing view. Future research may further elucidate how these perspectives converge.
:::

::: memo-reviewer
***Reviewer 1***: Future work might consider using blocked designs to evaluate whether consistent disfluency allows for better predictive calibration and whether this modulates the interaction between perceptual encoding and memory. Clarifying these dynamics could enrich the theoretical grounding. It would also facilitate constraints for applying perceptual disfluency manipulations in educational settings.
:::

Thank you for this thoughtful suggestion. As noted above, prior research using blurred text has observed perceptual disfluency effects in blocked (pure) designs, and we now cite this work explicitly. We agree that directly manipulating blocked versus mixed designs would provide valuable insight into how consistency in disfluency influences predictive calibration and the interplay between encoding and memory. Such work would also clarify the conditions under which perceptual disfluency might be harnessed in applied or educational contexts.

## Minor Concerns

::: memo-reviewer
***Reviewer 1***:  p 25, lines 57: “A summary of the results can be found in. A summary of the results can be found in Table 3.”
:::

We have fixed this grammatical error.

::: memo-excerpt
A visualization of how blurring affected processing during word recognition can be seen in the quantile and delta plots in Fig. X. A summary of the ex-Gaussian model can be found in Table 3.
:::

::: memo-reviewer
***Reviewer 1***:  p 14, line 59: “Another scenario entails a general slow down of processing—causing distributional shifting μ, but not skewing”.  “but not skewing”.. Would it be better with “not an increase in skewing”?
:::

::: memo-excerpt
Another scenario entails a general slowing of processing, reflected in an increase in $\mu$ but not in $\tau$.
:::

::: memo-reviewer
***Reviewer 1***:  p 5 line 48: “Originally launched with the strong claim that typeface enhances memory retention due to the backward-slanting letters and gaps within each letter which forces individuals to ‘generate’ the missing parts of each word” . Please check grammar.
:::

::: memo-excerpt
It was originally launched with the strong claim that the typeface enhances memory retention because of the backward-slanting letters and the gaps within each letter, which force individuals to “generate” the missing parts of each word.
:::

# Reviewer 2

## Major Points

::: memo-reviewer
**Reviewer 2**: Perhaps my main concern is that the paper is very long and I found it somewhat difficult to read, although it seems to have a clear set of conclusions that could in my opinion be presented much more concisely. I hence strongly recommend streamlining the manuscript overall. Main targets for shortening should be the introduction and discussion, but I would also like to note that processing the 10 tables and 6 figures is also quite taxing for the reader.
:::

We agree and appreciate the suggestion. In response, we substantially streamlined the introduction and discussion to make them more concise and focused. We also reviewed all tables and figures to ensure that each directly supports the main text, simplified formatting to reduce redundancy, and reduced the overall number of tables and figures.

::: memo-reviewer
**Reviewer 2**: Relatedly, I personally did not see the benefit of reporting the results of two reaction time modeling approaches in experiments 1a and 1b, especially since one was
subsequently dropped for experiment 2. Of course, it is always nice if conclusions
from different methods converge, but in my opinion, the presentation of the two
different models, including their explanation in the introduction section, unnecessarily
complicates things and distracts from the main points of the paper. Could the
presentation in the main paper perhaps be limited to the ex-Gaussian model results
and the DDM models be moved to supplementary materials?
:::

We also wrestled with whether including both modeling approaches added clarity or complexity. To address this, we revised the manuscript to place greater emphasis on the ex-Gaussian results in the main text, briefly mention DDM in the discussions of Experiments 1A and 1B, and move the DDM result tables to the appendices. We believe it remains useful to report the DDM analyses given ongoing debate about the utility of ex-Gaussian models and their relationship to cognitive processes. Including both approaches shows that the conclusions converge while keeping the main narrative focused.

::: memo-reviewer
**Reviewer 2**: Relatedly, I do not believe that the observed correspondence between the two kinds of models in experiments 1a and 1b is sufficient for assuming equivalence of the
processes reflected by the derived parameters (p. 41). The authors should be more
careful with such statements.
:::

We appreciate this point. In the revision, we removed the claim of equivalence between the two models.

::: memo-reviewer
**Reviewer 2**: I found the “predictions” section on page 14 and onwards a bit wordy and difficult to
follow. Where does the statement “as it relates to memory performance, there should
be no difference between high and low blurred words” (p. 14) come from? Do the
authors not expect any differences in memory performance between the high and low
blurred words? Also, why should the compensatory account predict a mnemonic
benefit only for the high blurred words (p. 15)? Does it assume that there is some sort
of cutoff at which compensatory processes are needed, and if so, why do the authors
suspect that this cutoff is between the low and the high blur words? A similar question
arises for the predictions derived from the stage-specific account. In my
understanding, none of the views really strongly predict whether there should be a
pattern like clear = low blur < high blur or clear < low blur = high blur, or clear < low
blur < high blur, in terms of memory performance. This point also refers to table 1,
which I did not find very convincing.
:::

Thank you for raising these important points. As noted in the manuscript, our goal in the predictions section was to make a good-faith effort to translate largely verbal theories into more formal expectations, while recognizing that reasonable researchers may reach different conclusions—an unavoidable reality of scientific inference.

In line with this concern, we revised the text to temper the strength of our language and to present the predictions in a more neutral, descriptive manner. Specifically, we clarify that the different accounts do not provide strong or exclusive predictions about relative memory performance across clear, low-blur, and high-blur conditions. Instead, we outline how each account might lead to differences in response-time distributions, and how those distributional changes could (but need not) map onto recognition performance.

We also expand our discussion of why mnemonic benefits might emerge most clearly for high-blur words under a compensatory processing account, while acknowledging that such a cutoff is not explicitly defined in the original formulations and remains an open question. Similarly, for the stage-specific account, we highlight that our predictions represent one plausible interpretation of how early and late processes may interact, but alternative patterns are conceivable. We hope these clarifications make the predictions section clearer and more balanced.

::: memo-reviewer
**Reviewer 2**: I was confused by the description of over-sampling on p. 17. Was it not possible to
check most exclusion criteria (age, English native speaker etc.) before participation?
What percentage of participants had a LDT below 80% and were hence excluded?
How many participants took part overall? This is also an issue in experiment 1b.
:::

In Experiments 1A and 1B, we daisy-chained from Qualtrics to Pavlovia and back to SONA, and it was not technically feasible to pre-screen for exclusion criteria (e.g., age, native English speaker) prior to participation, as is possible on platforms such as Prolific (which we used in Experiment 2). To ensure sufficient power after exclusions, we therefore over-enrolled participants.

It was an oversight not to report the total number of participants recruited, the number excluded (e.g., LDT accuracy < 80%), and the final sample sizes. We have revised the manuscript for all experiments to provide these details and percentages.

::: memo-excerpt
All participants were recruited through the Rutgers University subject pool (SONA system). We preregistered a sample size of 216 participants. A design of this size provides at least 90% power to detect effect sizes of $\delta \geq 0.20$, assuming a one-sided test with $\alpha = 0.05$. A total of 263 participants completed the study. Per our exclusion criteria, 15 participants were removed for completing the experiment more than once, and 16 were removed for accuracy below 80%. No participants were excluded for being non-native English speakers or under 18 years of age. To account for oversampling and to ensure equal numbers across lists, we randomly selected 36 participants from each list, yielding a final sample of 216 participants.
:::

::: memo-excerpt
All participants were recruited through the Rutgers University subject pool (SONA system). We preregistered a sample size of 216 participants. A total of 282 participants completed the study. Per our exclusion criteria, 5 participants were removed for completing the experiment more than once, 19 were removed for accuracy below 80%, and 1 was removed for being under 18. No participants were excluded for being non-native English speakers. This resulted in 258 eligible participants. To account for oversampling and to ensure equal numbers across lists, we randomly selected 36 participants from each list, yielding a final analyzed sample of 216 participants.
:::

::: memo-excerpt
Participants were recruited via Prolific and compensated $12 per hour. Using Prolific’s built-in filters, we restricted eligibility to monolingual, native English–speaking adults residing in the United States, with normal or corrected-to-normal vision. A total of 465 participants completed the study. Three were excluded for completing the experiment more than once, and 13 were excluded for accuracy below .80; no participants were excluded for being under 18 years of age. This left 444 participants. Consistent with Experiments 1A and 1B, we randomly sampled participants to reach our preregistered sample size of 432. As noted by Brysbaert (2019), approximately 210 participants are required to detect a fully attenuated interaction with 80% power. To be conservative and ensure adequate sensitivity, we doubled the sample size of Experiments 1A and 1B and preregistered a target of 432 participants.
:::

::: memo-reviewer
**Reviewer 2**: Page 21: The authors note that they deviate from originally pre-registered contrasts
as they “believe they offer a more targeted test of our hypotheses”. This seems odd.
Did this thought come to mind only after data collection was finished? Or after seeing
the results of the originally proposed analyses?
:::

We appreciate the concern. To clarify: our preregistration specified a broad set of contrasts, which we report in full in the visualizations for transparency. After preregistration, we recognized that certain additional contrasts would provide a more direct test of our central hypotheses. These are supplements, not replacements: the preregistered analyses remain intact and visible, while the additional contrasts sharpen the theoretical tests.

::: memo-reviewer
**Reviewer 2**: The sample size justification of experiment 2 was not sufficient (“based on the
interaction effect we aimed to test”). Also, were participants again over-sampled?
Were participants with insufficient accuracy at encoding excluded?
:::

Thank you for raising this. We have clarified our sample-size justification for Experiment 2. Because we tested an attenuated interaction effect, a larger sample was needed for adequate power. Following Brysbaert (2019), detecting a fully attenuated interaction with 80% power requires approximately 210 participants. To be conservative, we doubled the sample size relative to Experiments 1A and 1B and ultimately collected 435 participants. Participants were oversampled to allow for exclusions, and—per preregistration—participants with insufficient accuracy during encoding were excluded. These details now appear in the Methods and a footnote.

::: memo-reviewer
**Reviewer 2**: I would suggest to write the general discussion in a more generally understandable
way, moving away from labels for specific parameters (or if anything in parentheses
after naming the process/construct they refer to) and towards verbal
descriptors/explanations.
:::

We addressed this by rewriting the discussion to foreground plain-language descriptions, with parameter labels, where needed, in parentheses.

::: memo-reviewer
**Reviewer 2**: I found no statement regarding ethics approval (or this research being exempt thereof).
:::

We apologize for the oversight. We have added statements in the Participants sections noting that each study received approval from the Institutional Review Board.

## Minor Points

::: memo-reviewer
***Reviewer 2***:  p 5 line 48: There seems to be an extra parenthesis on p.5, line 33.
:::

The introduction has been rewritten, and the extra parenthesis has been removed.

::: memo-reviewer
***Reviewer 2***:  P. 8 line 9 the formatting of the citation is off
:::

This has been fixed by rewriting the introduction.

::: memo-reviewer
***Reviewer 2***:  P. 9 line 22 and onwards: There is an (almost identical) duplicate sentence here.
:::

This has been fixed by rewriting the introduction.

::: memo-reviewer
***Reviewer 2***:  P. 13 line 7/8: “primarily influenced both...” would sound better without the “primarily”
:::

This has been fixed by rewriting the introduction.

::: memo-reviewer
Reviewer 2: The table captions are confusing, which is partly due to the formatting. Also, does “95% CrI for equivalence tests” mean that it is not the 90% CrI, but the 95% CrI that is shown in the column “90% CrI” in the tables? If so, I think the column label should be changed to something clearer rather than simply mentioning in the caption that it does not apply to equivalence tests.
:::

We appreciate this helpful comment and apologize for the confusion. To improve clarity, we relabeled the column simply as “CrI” and added a note in the caption distinguishing between directional (one-tailed) and bidirectional (two-tailed) hypotheses. We also corrected formatting for consistency across tables.

::: memo-excerpt
CrI: 90% for one-sided tests and 95% for two-sided tests against 0.
:::

::: memo-reviewer
***Reviewer 2***:  "This is absolutely not critical, but why are sigma and beta reported in log (logit) scale?”
:::

The reference to “logit” was a mistake; we used a log link. We reported $\sigma$ and $\beta$ on the log scale because that is the estimation scale, which keeps parameters within their allowable ranges (e.g., variance parameters strictly positive, probabilities constrained between 0 and 1). The `brms` package reports estimates on this transformed scale. If preferred, we can also report values on the response scale; all visualizations are on the response (ms) scale.

::: memo-reviewer
Reviewer 2: "Page 23, bottom, there is again a duplicate sentence"
:::

Thank you for catching this repetition. We removed the duplication and improved readability.

::: memo-excerpt
A visualization of how blurring affected processing during word recognition is shown in the quantile and delta plots (Fig. X). A summary of the ex-Gaussian model results is provided in Fig. X.
:::

::: memo-reviewer
Reviewer 2: "In my opinion, the quality of the figures could be improved."
:::

Thank you for this suggestion. We revised the figures to improve clarity and readability by increasing resolution, adjusting font sizes and labels, and standardizing formatting across figures.

::: memo-reviewer
**Reviewer 2**: "Page 38, the sentence in the discussion section is unclear (‘with no context reinstatement’). It becomes clear what is meant when thinking about it, but it would be beneficial to improve the clarity."
:::

Thank you for noting this. We clarified the sentence and titled this experiment “Experiment 1B: No Context Reinstatement.”

::: memo-excerpt
We replicated all findings of Experiment 1A in a design where levels of blurring were not reinstated at test.
:::

::: memo-reviewer
**Reviewer 2**: "Page 39, “remembered” is a bit of a tricky wording here as this could also have
implications for specific retrieval processes, which I do not believe is what the authors
mean
:::

Thank you. We agree that “remembered” could misleadingly suggest retrieval processes, which is not our intention. Our results speak to recognition sensitivity ($d'$), not retrieval mechanisms. We revised the sentence as follows:

::: memo-excerpt
Sensitivity was higher for high-blur words than for clear and low-blur words…
:::

::: memo-reviewer
**Reviewer 2**: Page 48 greater skewing for high frequency words”, do the authors mean “greater
skewing due to blurring in the high frequency than in the low frequency words”?
Overall, the clarity of these passages could be improved, especially when reporting
interactions.
:::

This section has been rewritten to improve clarity, particularly in the reporting of interactions.

::: memo-reviewer
**Reviewer 2**: For Table 10, it is unclear what the DV presented here is.
:::

Thank you. We agree that the DV was not sufficiently clear. Table 10 presents response times (ms), broken down by word frequency (high vs. low) across each quantile of the distribution, corresponding to the effects illustrated in Figure 6. We revised the caption to state the variable explicitly and added a cross-reference in the discussion.

::: memo-excerpt
Mean response time (ms) for the word-frequency effects across the .1, .3, .5, .7, and .9 quantiles of the RT distribution as a function of blurring. These values correspond to the quantile effects for Experiment 2 (see Fig. 6).
:::

::: memo-reviewer
**Reviewer 2**: Page 50 were more likely to be recognized”: this sentence sounds like the hit rate is
being referred to, which I do not think is the case.
:::

::: memo-excerpt
Sensitivity was higher for low-frequency words compared to high-frequency words.
:::

::: memo-reviewer
**Reviewer 2**: Page 50, “such that high frequency words showed a selective memory benefit”: it
would be better in my opinion to explicitly say “high frequency words, but not low
frequency words, showed...”
:::

We revised the sentence to read:

::: memo-excerpt
High-frequency words, but not low-frequency words, showed a selective memory benefit under high blurring.
:::

::: memo-reviewer
**Reviewer 2**: P. 55 the gomes reference is not formatted correctly
:::

The document was created in Quarto and did not format this citation correctly. We have corrected the reference and revised the section accordingly.



